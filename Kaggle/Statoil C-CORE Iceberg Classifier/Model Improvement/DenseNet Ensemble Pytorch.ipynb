{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__Python VERSION: 3.6.1 |Continuum Analytics, Inc.| (default, May 11 2017, 13:25:24) [MSC v.1900 64 bit (AMD64)]\n",
      "__pyTorch VERSION: 0.1.12\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "# Shlomo Kashani \n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "# from subprocess import check_output\n",
    "# print(check_output([\"ls\", \"../_RawData\"]).decode(\"utf8\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from sklearn import cross_validation\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score, log_loss, roc_auc_score, roc_curve, auc\n",
    "from sklearn.cross_validation import StratifiedKFold, ShuffleSplit, cross_val_score, train_test_split\n",
    "\n",
    "print('__Python VERSION:', sys.version)\n",
    "print('__pyTorch VERSION:', torch.__version__)\n",
    "\n",
    "import numpy\n",
    "import numpy as np\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "Tensor = FloatTensor\n",
    "\n",
    "import pandas\n",
    "import pandas as pd\n",
    "\n",
    "import logging\n",
    "handler=logging.basicConfig(level=logging.INFO)\n",
    "lgr = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# !pip install psutil\n",
    "import psutil\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:USE CUDA=True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.1 |Continuum Analytics, Inc.| (default, May 11 2017, 13:25:24) [MSC v.1900 64 bit (AMD64)]\n",
      "8.0\n",
      "svmem(total=17056886784, available=9242480640, percent=45.8, used=7814406144, free=9242480640)\n",
      "memory GB: 3.0486412048339844\n"
     ]
    }
   ],
   "source": [
    "def cpuStats():\n",
    "        print(sys.version)\n",
    "        print(psutil.cpu_percent())\n",
    "        print(psutil.virtual_memory())  # physical memory usage\n",
    "        pid = os.getpid()\n",
    "        py = psutil.Process(pid)\n",
    "        memoryUse = py.memory_info()[0] / 2. ** 30  # memory use in GB...I think\n",
    "        print('memory GB:', memoryUse)\n",
    "\n",
    "cpuStats()\n",
    "\n",
    "# use_cuda=False\n",
    "lgr.info(\"USE CUDA=\" + str (use_cuda))\n",
    "\n",
    "\n",
    "# #  Global params\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# fix seed\n",
    "seed=17*19\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1604, 5)\n"
     ]
    }
   ],
   "source": [
    "# #  View the Data\n",
    "# - Numerai provides a data set that is allready split into train, validation and test sets. \n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "# Data params\n",
    "TARGET_VAR= 'target'\n",
    "BASE_FOLDER = '../_RawData/'\n",
    "\n",
    "\n",
    "# #  Train / Validation / Test Split\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# data = pd.read_json(BASE_FOLDER + '/train.json')\n",
    "data = pd.read_json(\"../_RawData/train.json/data/processed/train.json\")\n",
    "\n",
    "print (data.shape)\n",
    "# data['precision_4'] = data['inc_angle'].apply(lambda x: len(str(x))) <= 7\n",
    "# data = data[data['precision_4'] == True]\n",
    "# print (data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.signal\n",
    "\n",
    "def cross_image(im1, im2):\n",
    "   # get rid of the color channels by performing a grayscale transform\n",
    "   # the type cast into 'float' is to avoid overflows\n",
    "   im1_gray = np.sum(im1.astype('float'), axis=2)\n",
    "   im2_gray = np.sum(im2.astype('float'), axis=2)\n",
    "\n",
    "   # get rid of the averages, otherwise the results are not good\n",
    "   im1_gray -= np.mean(im1_gray)\n",
    "   im2_gray -= np.mean(im2_gray)\n",
    "\n",
    "   # calculate the correlation image; note the flipping of onw of the images\n",
    "   return scipy.signal.fftconvolve(im1_gray, im2_gray[::-1,::-1], mode='same')\n",
    "   \n",
    "# Suffle\n",
    "import random\n",
    "from datetime import datetime\n",
    "from scipy import signal\n",
    "random.seed(datetime.now())\n",
    "# np.random.seed(datetime.now())\n",
    "from sklearn.utils import shuffle\n",
    "data = shuffle(data) # otherwise same validation set each time!\n",
    "data= data.reindex(np.random.permutation(data.index))\n",
    "\n",
    "data = shuffle(data) # otherwise same validation set each time!\n",
    "data= data.reindex(np.random.permutation(data.index))\n",
    "\n",
    "def Zpad(A, length):\n",
    "    arr = np.zeros(length)\n",
    "    arr[:len(A)] = A\n",
    "    return arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data['band_1'] = data['band_1'].apply(lambda x: Zpad(x,6400))\n",
    "# data['band_2'] = data['band_1'].apply(lambda x: Zpad(x,6400))\n",
    "\n",
    "data['band_1'] = data['band_1'].apply(lambda x: np.array(x).reshape(75, 75))\n",
    "data['band_2'] = data['band_2'].apply(lambda x: np.array(x).reshape(75, 75))\n",
    "\n",
    "data['inc_angle'] = pd.to_numeric(data['inc_angle'], errors='coerce')\n",
    "\n",
    "import scipy\n",
    "band_1 = np.concatenate([im for im in data['band_1']]).reshape(-1, 75, 75)\n",
    "band_2 = np.concatenate([im for im in data['band_2']]).reshape(-1, 75, 75)\n",
    "# band_3=(band_1+band_2)/2\n",
    "# band_3=signal.fftconvolve(band_1, band_1, mode = 'same')\n",
    "\n",
    "full_img = np.stack([band_1, band_2], axis=1)\n",
    "\n",
    "# https://github.com/bermanmaxim/jaccardSegment/blob/master/compose.py\n",
    "\n",
    "# #  From Numpy to PyTorch GPU tensors\n",
    "\n",
    "# In[14]:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert the np arrays into the correct dimention and type\n",
    "# Note that BCEloss requires Float in X as well as in y\n",
    "def XnumpyToTensor(x_data_np):\n",
    "    x_data_np = np.array(x_data_np, dtype=np.float32)        \n",
    "#     print(x_data_np.shape)\n",
    "    print(type(x_data_np))\n",
    "\n",
    "    if use_cuda:\n",
    "        lgr.info (\"Using the GPU\")    \n",
    "        X_tensor = (torch.from_numpy(x_data_np).cuda()) # Note the conversion for pytorch    \n",
    "    else:\n",
    "        lgr.info (\"Using the CPU\")\n",
    "        X_tensor = (torch.from_numpy(x_data_np)) # Note the conversion for pytorch\n",
    "        \n",
    "#     print((X_tensor.shape)) # torch.Size([108405, 29])\n",
    "    return X_tensor\n",
    "\n",
    "\n",
    "# Convert the np arrays into the correct dimention and type\n",
    "# Note that BCEloss requires Float in X as well as in y\n",
    "def YnumpyToTensor(y_data_np):    \n",
    "    y_data_np=y_data_np.reshape((y_data_np.shape[0],1)) # Must be reshaped for PyTorch!\n",
    "    print(y_data_np.shape)\n",
    "    print(type(y_data_np))\n",
    "\n",
    "    if use_cuda:\n",
    "        lgr.info (\"Using the GPU\")            \n",
    "    #     Y = Variable(torch.from_numpy(y_data_np).type(torch.LongTensor).cuda())\n",
    "        Y_tensor = (torch.from_numpy(y_data_np)).type(torch.FloatTensor).cuda()  # BCEloss requires Float        \n",
    "    else:\n",
    "        lgr.info (\"Using the CPU\")        \n",
    "    #     Y = Variable(torch.squeeze (torch.from_numpy(y_data_np).type(torch.LongTensor)))  #         \n",
    "        Y_tensor = (torch.from_numpy(y_data_np)).type(torch.FloatTensor)  # BCEloss requires Float        \n",
    "\n",
    "    print(type(Y_tensor)) # should be 'torch.cuda.FloatTensor'\n",
    "    print(y_data_np.shape)\n",
    "    print(type(y_data_np))    \n",
    "    return Y_tensor\n",
    "\n",
    "# #  Custom data loader\n",
    "\n",
    "# In[17]:\n",
    "\n",
    "# transformations = transforms.Compose([transforms.Scale(32),transforms.ToTensor()])\n",
    "# preprocess = transforms.Compose([\n",
    "#   transforms.Scale(75),\n",
    "#   transforms.CenterCrop(224),\n",
    "#   transforms.ToTensor(),\n",
    "#   normalize\n",
    "# ])\n",
    "\n",
    "class FullTrainningDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, full_ds, offset, length):\n",
    "        self.full_ds = full_ds\n",
    "        self.offset = offset\n",
    "        self.length = length\n",
    "        assert len(full_ds)>=offset+length, Exception(\"Parent Dataset not long enough\")\n",
    "        super(FullTrainningDataset, self).__init__()\n",
    "        \n",
    "    def __len__(self):        \n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        # label = torch.from_numpy(self.y_train[index])\n",
    "        return self.full_ds[i+self.offset]\n",
    "    \n",
    "validationRatio=0.11    \n",
    "\n",
    "def trainTestSplit(dataset, val_share=validationRatio):\n",
    "    val_offset = int(len(dataset)*(1-val_share))\n",
    "    print (\"Offest:\" + str(val_offset))\n",
    "    return FullTrainningDataset(dataset, 0, val_offset), FullTrainningDataset(dataset, \n",
    "                                                                              val_offset, len(dataset)-val_offset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Using the GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Using the GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1604, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'torch.cuda.FloatTensor'>\n",
      "(1604, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "Offest:1427\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x00000219CFA60160>\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x000002199D9294E0>\n"
     ]
    }
   ],
   "source": [
    "# In[25]:\n",
    "\n",
    "\n",
    "batch_size=64\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# train_imgs = torch.from_numpy(full_img_tr).float()\n",
    "train_imgs=XnumpyToTensor (full_img)\n",
    "train_targets = YnumpyToTensor(data['is_iceberg'].values)\n",
    "dset_train = TensorDataset(train_imgs, train_targets)\n",
    "\n",
    "\n",
    "train_ds, val_ds = trainTestSplit(dset_train)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "val_loader = torch.utils.data.DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "print (train_loader)\n",
    "print (val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epoches = 50\n",
    "import math\n",
    "\n",
    "# import attr\n",
    "import torch\n",
    "import torch.cuda\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "n_channels = 2  # max 20\n",
    "total_classes = 1\n",
    "    \n",
    "# https://github.com/Lextal/pspnet-pytorch/blob/master/train.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Funct\n",
    "\n",
    "from collections import OrderedDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# class SegNet(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(SegNet, self).__init__()\n",
    "\n",
    "#         self.encoder_1 = nn.Sequential(\n",
    "#             nn.Conv2d(2, 64, 7, padding=3),\n",
    "#             nn.BatchNorm2d(64),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d((2, 2), stride=(2, 2), return_indices=True)\n",
    "#         )  # first group\n",
    "\n",
    "#         self.encoder_2 = nn.Sequential(\n",
    "#             nn.Conv2d(64, 64, 7, padding=3),\n",
    "#             nn.BatchNorm2d(64),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d((2, 2), stride=(2, 2), return_indices=True)\n",
    "#         )  # second group\n",
    "\n",
    "#         self.encoder_3 = nn.Sequential(\n",
    "#             nn.Conv2d(64, 64, 7, padding=3),\n",
    "#             nn.BatchNorm2d(64),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d((2, 2), stride=(2, 2), return_indices=True)\n",
    "#         )  # third group\n",
    "\n",
    "#         self.encoder_4 = nn.Sequential(\n",
    "#             nn.Conv2d(64, 64, 7, padding=3),\n",
    "#             nn.BatchNorm2d(64),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d((2, 2), stride=(2, 2), return_indices=True)\n",
    "#         )  # fourth group\n",
    "\n",
    "#         self.unpool_1 = nn.MaxUnpool2d(2, stride=2)  # get masks\n",
    "#         self.unpool_2 = nn.MaxUnpool2d(2, stride=2)\n",
    "#         self.unpool_3 = nn.MaxUnpool2d(2, stride=2)\n",
    "#         self.unpool_4 = nn.MaxUnpool2d(2, stride=2)\n",
    "\n",
    "#         self.decoder_1 = nn.Sequential(\n",
    "#             nn.Conv2d(64, 64, 7, padding=3),\n",
    "#             nn.BatchNorm2d(64)\n",
    "#         )  # first group\n",
    "\n",
    "#         self.decoder_2 = nn.Sequential(\n",
    "#             nn.Conv2d(64, 64, 7, padding=3),\n",
    "#             nn.BatchNorm2d(64)\n",
    "#         )  # second group\n",
    "\n",
    "#         self.decoder_3 = nn.Sequential(\n",
    "#             nn.Conv2d(64, 64, 7, padding=3),\n",
    "#             nn.BatchNorm2d(64)\n",
    "#         )  # third group\n",
    "\n",
    "#         self.decoder_4 = nn.Sequential(\n",
    "#             nn.Conv2d(64, 3, 7, padding=3),\n",
    "#             nn.BatchNorm2d(3)\n",
    "#         )  # fourth group\n",
    "\n",
    "#         # self.conv_classifier = nn.Conv2d(128, 5, 1)\n",
    "        \n",
    "#         self.classifier = torch.nn.Sequential(\n",
    "#             nn.Linear(972, 1),             \n",
    "#         )\n",
    "        \n",
    "#         self.mp = nn.MaxPool2d(4, 4)\n",
    "        \n",
    "#         self.sig = nn.Sigmoid()   \n",
    "\n",
    "#     def forward(self, x):\n",
    "#         size_1 = x.size()\n",
    "#         x, indices_1 = self.encoder_1(x)\n",
    "\n",
    "#         size_2 = x.size()\n",
    "#         x, indices_2 = self.encoder_2(x)\n",
    "\n",
    "#         size_3 = x.size()\n",
    "#         x, indices_3 = self.encoder_3(x)\n",
    "\n",
    "#         size_4 = x.size()\n",
    "#         x, indices_4 = self.encoder_4(x)\n",
    "\n",
    "#         x = self.unpool_1(x, indices_4, output_size=size_4)\n",
    "#         x = self.decoder_1(x)\n",
    "\n",
    "#         x = self.unpool_2(x, indices_3, output_size=size_3)\n",
    "#         x = self.decoder_2(x)\n",
    "\n",
    "#         x = self.unpool_3(x, indices_2, output_size=size_2)\n",
    "#         x = self.decoder_3(x)\n",
    "\n",
    "#         x = self.unpool_4(x, indices_1, output_size=size_1)\n",
    "#         x = self.decoder_4(x)\n",
    "        \n",
    "#         x = self.mp(x)\n",
    "#         x = x.view(x.size(0), -1)    \n",
    "#         print(\"shape:\" + str(x.data.shape))\n",
    "#         x = self.classifier(x)\n",
    "#         # print(\"shape:\" + str(x.data.shape))\n",
    "\n",
    "#         x = self.sig(x)\n",
    "\n",
    "#         return x\n",
    "\n",
    "\n",
    "# model=SegNet()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision.models as models\n",
    "\n",
    "import sys\n",
    "import math\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, nChannels, growthRate):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        interChannels = 4*growthRate\n",
    "        self.bn1 = nn.BatchNorm2d(nChannels)\n",
    "        self.conv1 = nn.Conv2d(nChannels, interChannels, kernel_size=1,\n",
    "                               bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(interChannels)\n",
    "        self.conv2 = nn.Conv2d(interChannels, growthRate, kernel_size=3,\n",
    "                               padding=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(F.relu(self.bn1(x)))\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        out = torch.cat((x, out), 1)\n",
    "        return out\n",
    "\n",
    "class SingleLayer(nn.Module):\n",
    "    def __init__(self, nChannels, growthRate):\n",
    "        super(SingleLayer, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(nChannels)\n",
    "        self.conv1 = nn.Conv2d(nChannels, growthRate, kernel_size=3,\n",
    "                               padding=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(F.relu(self.bn1(x)))\n",
    "        out = torch.cat((x, out), 1)\n",
    "        return out\n",
    "\n",
    "class Transition(nn.Module):\n",
    "    def __init__(self, nChannels, nOutChannels):\n",
    "        super(Transition, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(nChannels)\n",
    "        self.conv1 = nn.Conv2d(nChannels, nOutChannels, kernel_size=1,\n",
    "                               bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(F.relu(self.bn1(x)))\n",
    "        out = F.avg_pool2d(out, 2)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# class DenseNet(nn.Module):\n",
    "#     def __init__(self, growthRate, depth, reduction, nClasses, bottleneck):\n",
    "#         super(DenseNet, self).__init__()\n",
    "\n",
    "#         nDenseBlocks = (depth-4) // 3\n",
    "#         if bottleneck:\n",
    "#             nDenseBlocks //= 2\n",
    "\n",
    "#         nChannels = 2*growthRate\n",
    "#         self.conv1 = nn.Conv2d(2, nChannels, kernel_size=3, padding=1,\n",
    "#                                bias=False)\n",
    "#         self.dense1 = self._make_dense(nChannels, growthRate, nDenseBlocks, bottleneck)\n",
    "#         nChannels += nDenseBlocks*growthRate\n",
    "#         nOutChannels = int(math.floor(nChannels*reduction))\n",
    "#         self.trans1 = Transition(nChannels, nOutChannels)\n",
    "\n",
    "#         nChannels = nOutChannels\n",
    "#         self.dense2 = self._make_dense(nChannels, growthRate, nDenseBlocks, bottleneck)\n",
    "#         nChannels += nDenseBlocks*growthRate\n",
    "#         nOutChannels = int(math.floor(nChannels*reduction))\n",
    "#         self.trans2 = Transition(nChannels, nOutChannels)\n",
    "\n",
    "#         nChannels = nOutChannels\n",
    "#         self.dense3 = self._make_dense(nChannels, growthRate, nDenseBlocks, bottleneck)\n",
    "#         nChannels += nDenseBlocks*growthRate\n",
    "\n",
    "#         self.bn1 = nn.BatchNorm2d(nChannels)\n",
    "#         self.fc = nn.Linear(128, nClasses)\n",
    "\n",
    "#         for m in self.modules():\n",
    "#             if isinstance(m, nn.Conv2d):\n",
    "#                 n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "#                 m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "#             elif isinstance(m, nn.BatchNorm2d):\n",
    "#                 m.weight.data.fill_(1)\n",
    "#                 m.bias.data.zero_()\n",
    "#             elif isinstance(m, nn.Linear):\n",
    "#                 m.bias.data.zero_()\n",
    "\n",
    "#     def _make_dense(self, nChannels, growthRate, nDenseBlocks, bottleneck):\n",
    "#         layers = []\n",
    "#         for i in range(int(nDenseBlocks)):\n",
    "#             if bottleneck:\n",
    "#                 layers.append(Bottleneck(nChannels, growthRate))\n",
    "#             else:\n",
    "#                 layers.append(SingleLayer(nChannels, growthRate))\n",
    "#             nChannels += growthRate\n",
    "#         return nn.Sequential(*layers)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         out = self.conv1(x)\n",
    "#         out = self.trans1(self.dense1(out))\n",
    "#         out = self.trans2(self.dense2(out))\n",
    "#         out = self.dense3(out)\n",
    "#         # print(out.data.shape)\n",
    "#         out = F.avg_pool2d(F.relu(self.bn1(out)), 8)\n",
    "#         out = out.view(out.size(0), -1)\n",
    "#         # print(out.data.shape)\n",
    "#         out = F.sigmoid(self.fc(out))\n",
    "#         return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'growthRate'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-2242bc0fe398>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m model = DenseNet(growthRate=8, depth=20, reduction=0.5,\n\u001b[1;32m----> 2\u001b[1;33m                             bottleneck=True, nClasses=1)\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'  + Number of params: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnelement\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'growthRate'"
     ]
    }
   ],
   "source": [
    "model = DenseNet(growthRate=8, depth=20, reduction=0.5,\n",
    "                            bottleneck=True, nClasses=1)\n",
    "\n",
    "print('  + Number of params: {}'.format(sum([p.data.nelement() for p in model.parameters()])))\n",
    "        \n",
    "print(model)\n",
    "# https://github.com/ZijunDeng/pytorch-semantic-segmentation/tree/master/models\n",
    "# https://github.com/andreasveit/densenet-pytorch/blob/master/densenet.py\n",
    "# https://github.com/meliketoy/wide-resnet.pytorch/blob/master/networks/wide_resnet.py\n",
    "\n",
    "# # Loss and optimizer\n",
    "\n",
    "# In[28]:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''DenseNet in PyTorch.'''\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, in_planes, growth_rate):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv1 = nn.Conv2d(in_planes, 4 * growth_rate, kernel_size=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(4 * growth_rate)\n",
    "        self.conv2 = nn.Conv2d(4 * growth_rate, growth_rate, kernel_size=3, padding=1, bias=False)\n",
    "\n",
    "        self.mp = torch.nn.MaxPool2d(1, 1)\n",
    "        # self.avgpool = torch.nn.AvgPool2d(2,2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(F.relu(self.bn1(x)))\n",
    "\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        # out = self.mp(out)\n",
    "        # out = self.avgpool(out)\n",
    "\n",
    "        # print (x.data.shape)\n",
    "\n",
    "        out = torch.cat([out, x], 1)\n",
    "        out = self.mp(out)\n",
    "        # out = self.avgpool(out)\n",
    "        # print(out.data.shape)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Transition(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes):\n",
    "        super(Transition, self).__init__()\n",
    "        self.bn = nn.BatchNorm2d(in_planes)\n",
    "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(F.relu(self.bn(x)))\n",
    "        out = F.avg_pool2d(out, 2)\n",
    "        return out\n",
    "\n",
    "\n",
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, block, nblocks, growth_rate=12, reduction=0.5, num_classes=1):\n",
    "        super(DenseNet, self).__init__()\n",
    "        self.growth_rate = growth_rate\n",
    "\n",
    "        num_planes = 2 * growth_rate\n",
    "        self.conv1 = nn.Conv2d(3, num_planes, kernel_size=3, padding=1, bias=False)\n",
    "\n",
    "        self.dense1 = self._make_dense_layers(block, num_planes, nblocks[0])\n",
    "        num_planes += nblocks[0] * growth_rate\n",
    "        out_planes = int(math.floor(num_planes * reduction))\n",
    "        self.trans1 = Transition(num_planes, out_planes)\n",
    "        num_planes = out_planes\n",
    "\n",
    "        self.dense2 = self._make_dense_layers(block, num_planes, nblocks[1])\n",
    "        num_planes += nblocks[1] * growth_rate\n",
    "        out_planes = int(math.floor(num_planes * reduction))\n",
    "        self.trans2 = Transition(num_planes, out_planes)\n",
    "        num_planes = out_planes\n",
    "\n",
    "        self.dense3 = self._make_dense_layers(block, num_planes, nblocks[2])\n",
    "        num_planes += nblocks[2] * growth_rate\n",
    "        out_planes = int(math.floor(num_planes * reduction))\n",
    "        self.trans3 = Transition(num_planes, out_planes)\n",
    "        num_planes = out_planes\n",
    "\n",
    "        self.dense4 = self._make_dense_layers(block, num_planes, nblocks[3])\n",
    "        num_planes += nblocks[3] * growth_rate\n",
    "\n",
    "        self.bn = nn.BatchNorm2d(num_planes)\n",
    "\n",
    "        self.linear = nn.Linear(3328, num_classes)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def _make_dense_layers(self, block, in_planes, nblock):\n",
    "        layers = []\n",
    "        for i in range(nblock):\n",
    "            layers.append(block(in_planes, self.growth_rate))\n",
    "            in_planes += self.growth_rate\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.trans1(self.dense1(out))\n",
    "        out = self.trans2(self.dense2(out))\n",
    "        out = self.trans3(self.dense3(out))\n",
    "        out = self.dense4(out)\n",
    "        out = F.avg_pool2d(F.relu(self.bn(out)), 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        # print (out.data.shape)\n",
    "        out = self.linear(out)\n",
    "        out = self.sig(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "def DenseNet121():\n",
    "    return DenseNet(Bottleneck, [6, 12, 24, 16], growth_rate=12)\n",
    "\n",
    "\n",
    "def DenseNet169():\n",
    "    return DenseNet(Bottleneck, [6, 12, 32, 32], growth_rate=16)\n",
    "\n",
    "\n",
    "def DenseNet201():\n",
    "    return DenseNet(Bottleneck, [6, 12, 48, 32], growth_rate=32)\n",
    "\n",
    "\n",
    "def DenseNet161():\n",
    "    return DenseNet(Bottleneck, [6, 12, 36, 24], growth_rate=48)\n",
    "\n",
    "\n",
    "def densenet_cifar():\n",
    "    return DenseNet(Bottleneck, [6, 12, 24, 16], growth_rate=12)\n",
    "\n",
    "# test_densenet()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Using the GPU\n",
      "INFO:__main__:<torch.optim.adam.Adam object at 0x00000219800B66A0>\n",
      "INFO:__main__:BCELoss (\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "loss_func=torch.nn.BCELoss() # Binary cross entropy: http://pytorch.org/docs/nn.html#bceloss\n",
    "\n",
    "# NN params\n",
    "LR = 0.0005\n",
    "MOMENTUM= 0.95\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR,weight_decay=5e-5) #  L2 regularization\n",
    "if use_cuda:\n",
    "    lgr.info (\"Using the GPU\")    \n",
    "    model.cuda()\n",
    "    loss_func.cuda()\n",
    "\n",
    "lgr.info (optimizer)\n",
    "lgr.info (loss_func)\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "criterion = loss_func\n",
    "all_losses = []\n",
    "val_losses = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "*****:\n",
      "[1/50] Loss: 0.247776\n",
      "[1/50] Loss: 0.243548\n",
      "Finish 1 epoch, Loss: 0.245574\n",
      "VALIDATION Loss: 0.278557\n",
      "\n",
      "Epoch 2/50\n",
      "*****:\n",
      "[2/50] Loss: 0.228234\n",
      "[2/50] Loss: 0.228031\n",
      "Finish 2 epoch, Loss: 0.229874\n",
      "VALIDATION Loss: 0.283419\n",
      "\n",
      "Epoch 3/50\n",
      "*****:\n",
      "[3/50] Loss: 0.224728\n",
      "[3/50] Loss: 0.222588\n",
      "Finish 3 epoch, Loss: 0.225731\n",
      "VALIDATION Loss: 0.286275\n",
      "\n",
      "Epoch 4/50\n",
      "*****:\n",
      "[4/50] Loss: 0.220328\n",
      "[4/50] Loss: 0.216918\n",
      "Finish 4 epoch, Loss: 0.219013\n",
      "VALIDATION Loss: 0.283483\n",
      "\n",
      "Epoch 5/50\n",
      "*****:\n",
      "[5/50] Loss: 0.216967\n",
      "[5/50] Loss: 0.213124\n",
      "Finish 5 epoch, Loss: 0.214816\n",
      "VALIDATION Loss: 0.288411\n",
      "\n",
      "Epoch 6/50\n",
      "*****:\n",
      "[6/50] Loss: 0.213047\n",
      "[6/50] Loss: 0.209786\n",
      "Finish 6 epoch, Loss: 0.211250\n",
      "VALIDATION Loss: 0.299051\n",
      "\n",
      "Epoch 7/50\n",
      "*****:\n",
      "[7/50] Loss: 0.211646\n",
      "[7/50] Loss: 0.206860\n",
      "Finish 7 epoch, Loss: 0.208247\n",
      "VALIDATION Loss: 0.310461\n",
      "\n",
      "Epoch 8/50\n",
      "*****:\n",
      "[8/50] Loss: 0.209601\n",
      "[8/50] Loss: 0.204358\n",
      "Finish 8 epoch, Loss: 0.205917\n",
      "VALIDATION Loss: 0.327847\n",
      "\n",
      "Epoch 9/50\n",
      "*****:\n",
      "[9/50] Loss: 0.210945\n",
      "[9/50] Loss: 0.202544\n",
      "Finish 9 epoch, Loss: 0.203662\n",
      "VALIDATION Loss: 0.324565\n",
      "\n",
      "Epoch 10/50\n",
      "*****:\n",
      "[10/50] Loss: 0.209842\n",
      "[10/50] Loss: 0.202521\n",
      "Finish 10 epoch, Loss: 0.203130\n",
      "VALIDATION Loss: 0.321305\n",
      "\n",
      "Epoch 11/50\n",
      "*****:\n",
      "[11/50] Loss: 0.208555\n",
      "[11/50] Loss: 0.200139\n",
      "Finish 11 epoch, Loss: 0.201494\n",
      "VALIDATION Loss: 0.311635\n",
      "\n",
      "Epoch 12/50\n",
      "*****:\n",
      "[12/50] Loss: 0.211639\n",
      "[12/50] Loss: 0.206834\n",
      "Finish 12 epoch, Loss: 0.209334\n",
      "VALIDATION Loss: 0.317954\n",
      "\n",
      "Epoch 13/50\n",
      "*****:\n",
      "[13/50] Loss: 0.208137\n",
      "[13/50] Loss: 0.195739\n",
      "Finish 13 epoch, Loss: 0.196159\n",
      "VALIDATION Loss: 0.345160\n",
      "\n",
      "Epoch 14/50\n",
      "*****:\n",
      "[14/50] Loss: 0.205135\n",
      "[14/50] Loss: 0.194662\n",
      "Finish 14 epoch, Loss: 0.195433\n",
      "VALIDATION Loss: 0.319600\n",
      "\n",
      "Epoch 15/50\n",
      "*****:\n",
      "[15/50] Loss: 0.196278\n",
      "[15/50] Loss: 0.188412\n",
      "Finish 15 epoch, Loss: 0.188440\n",
      "VALIDATION Loss: 0.336990\n",
      "\n",
      "Epoch 16/50\n",
      "*****:\n",
      "[16/50] Loss: 0.194682\n",
      "[16/50] Loss: 0.184846\n",
      "Finish 16 epoch, Loss: 0.185267\n",
      "VALIDATION Loss: 0.343214\n",
      "\n",
      "Epoch 17/50\n",
      "*****:\n",
      "[17/50] Loss: 0.192891\n",
      "[17/50] Loss: 0.181782\n",
      "Finish 17 epoch, Loss: 0.182026\n",
      "VALIDATION Loss: 0.351974\n",
      "\n",
      "Epoch 18/50\n",
      "*****:\n",
      "[18/50] Loss: 0.183418\n",
      "[18/50] Loss: 0.176283\n",
      "Finish 18 epoch, Loss: 0.180622\n",
      "VALIDATION Loss: 0.452957\n",
      "\n",
      "Epoch 19/50\n",
      "*****:\n",
      "[19/50] Loss: 0.186082\n",
      "[19/50] Loss: 0.188467\n",
      "Finish 19 epoch, Loss: 0.194089\n",
      "VALIDATION Loss: 0.506256\n",
      "\n",
      "Epoch 20/50\n",
      "*****:\n",
      "[20/50] Loss: 0.203925\n",
      "[20/50] Loss: 0.191918\n",
      "Finish 20 epoch, Loss: 0.192281\n",
      "VALIDATION Loss: 0.360349\n",
      "\n",
      "Epoch 21/50\n",
      "*****:\n",
      "[21/50] Loss: 0.189752\n",
      "[21/50] Loss: 0.179170\n",
      "Finish 21 epoch, Loss: 0.181273\n",
      "VALIDATION Loss: 0.359162\n",
      "\n",
      "Epoch 22/50\n",
      "*****:\n",
      "[22/50] Loss: 0.178039\n",
      "[22/50] Loss: 0.180007\n",
      "Finish 22 epoch, Loss: 0.186376\n",
      "VALIDATION Loss: 0.368114\n",
      "\n",
      "Epoch 23/50\n",
      "*****:\n",
      "[23/50] Loss: 0.183883\n",
      "[23/50] Loss: 0.176437\n",
      "Finish 23 epoch, Loss: 0.176863\n",
      "VALIDATION Loss: 0.300443\n",
      "\n",
      "Epoch 24/50\n",
      "*****:\n",
      "[24/50] Loss: 0.175234\n",
      "[24/50] Loss: 0.165651\n",
      "Finish 24 epoch, Loss: 0.167764\n",
      "VALIDATION Loss: 0.303325\n",
      "\n",
      "Epoch 25/50\n",
      "*****:\n",
      "[25/50] Loss: 0.161722\n",
      "[25/50] Loss: 0.155236\n",
      "Finish 25 epoch, Loss: 0.156008\n",
      "VALIDATION Loss: 0.306312\n",
      "\n",
      "Epoch 26/50\n",
      "*****:\n",
      "[26/50] Loss: 0.160516\n",
      "[26/50] Loss: 0.152383\n",
      "Finish 26 epoch, Loss: 0.153131\n",
      "VALIDATION Loss: 0.333592\n",
      "\n",
      "Epoch 27/50\n",
      "*****:\n",
      "[27/50] Loss: 0.151269\n",
      "[27/50] Loss: 0.147451\n",
      "Finish 27 epoch, Loss: 0.148237\n",
      "VALIDATION Loss: 0.336687\n",
      "\n",
      "Epoch 28/50\n",
      "*****:\n",
      "[28/50] Loss: 0.155373\n",
      "[28/50] Loss: 0.146552\n",
      "Finish 28 epoch, Loss: 0.147711\n",
      "VALIDATION Loss: 0.345102\n",
      "\n",
      "Epoch 29/50\n",
      "*****:\n",
      "[29/50] Loss: 0.156239\n",
      "[29/50] Loss: 0.143427\n",
      "Finish 29 epoch, Loss: 0.144772\n",
      "VALIDATION Loss: 0.360944\n",
      "\n",
      "Epoch 30/50\n",
      "*****:\n",
      "[30/50] Loss: 0.141434\n",
      "[30/50] Loss: 0.135222\n",
      "Finish 30 epoch, Loss: 0.139146\n",
      "VALIDATION Loss: 0.383432\n",
      "\n",
      "Epoch 31/50\n",
      "*****:\n",
      "[31/50] Loss: 0.140244\n",
      "[31/50] Loss: 0.133158\n",
      "Finish 31 epoch, Loss: 0.138229\n",
      "VALIDATION Loss: 0.520132\n",
      "\n",
      "Epoch 32/50\n",
      "*****:\n",
      "[32/50] Loss: 0.165267\n",
      "[32/50] Loss: 0.159130\n",
      "Finish 32 epoch, Loss: 0.163997\n",
      "VALIDATION Loss: 0.476591\n",
      "\n",
      "Epoch 33/50\n",
      "*****:\n",
      "[33/50] Loss: 0.165408\n",
      "[33/50] Loss: 0.157786\n",
      "Finish 33 epoch, Loss: 0.158401\n",
      "VALIDATION Loss: 0.354927\n",
      "\n",
      "Epoch 34/50\n",
      "*****:\n",
      "[34/50] Loss: 0.153542\n",
      "[34/50] Loss: 0.138397\n",
      "Finish 34 epoch, Loss: 0.138606\n",
      "VALIDATION Loss: 0.395490\n",
      "\n",
      "Epoch 35/50\n",
      "*****:\n",
      "[35/50] Loss: 0.130755\n",
      "[35/50] Loss: 0.121446\n",
      "Finish 35 epoch, Loss: 0.123389\n",
      "VALIDATION Loss: 0.398479\n",
      "\n",
      "Epoch 36/50\n",
      "*****:\n",
      "[36/50] Loss: 0.122952\n",
      "[36/50] Loss: 0.115252\n",
      "Finish 36 epoch, Loss: 0.118953\n",
      "VALIDATION Loss: 0.358367\n",
      "\n",
      "Epoch 37/50\n",
      "*****:\n",
      "[37/50] Loss: 0.121303\n",
      "[37/50] Loss: 0.117557\n",
      "Finish 37 epoch, Loss: 0.122339\n",
      "VALIDATION Loss: 0.444572\n",
      "\n",
      "Epoch 38/50\n",
      "*****:\n",
      "[38/50] Loss: 0.117783\n",
      "[38/50] Loss: 0.121545\n",
      "Finish 38 epoch, Loss: 0.124807\n",
      "VALIDATION Loss: 0.386815\n",
      "\n",
      "Epoch 39/50\n",
      "*****:\n",
      "[39/50] Loss: 0.136955\n",
      "[39/50] Loss: 0.133762\n",
      "Finish 39 epoch, Loss: 0.135047\n",
      "VALIDATION Loss: 0.373004\n",
      "\n",
      "Epoch 40/50\n",
      "*****:\n",
      "[40/50] Loss: 0.189768\n",
      "[40/50] Loss: 0.178554\n",
      "Finish 40 epoch, Loss: 0.175950\n",
      "VALIDATION Loss: 0.332302\n",
      "\n",
      "Epoch 41/50\n",
      "*****:\n",
      "[41/50] Loss: 0.145765\n",
      "[41/50] Loss: 0.133700\n",
      "Finish 41 epoch, Loss: 0.134257\n",
      "VALIDATION Loss: 0.346194\n",
      "\n",
      "Epoch 42/50\n",
      "*****:\n",
      "[42/50] Loss: 0.130814\n",
      "[42/50] Loss: 0.121743\n",
      "Finish 42 epoch, Loss: 0.122391\n",
      "VALIDATION Loss: 0.369486\n",
      "\n",
      "Epoch 43/50\n",
      "*****:\n",
      "[43/50] Loss: 0.120912\n",
      "[43/50] Loss: 0.110713\n",
      "Finish 43 epoch, Loss: 0.111594\n",
      "VALIDATION Loss: 0.355859\n",
      "\n",
      "Epoch 44/50\n",
      "*****:\n",
      "[44/50] Loss: 0.111961\n",
      "[44/50] Loss: 0.105553\n",
      "Finish 44 epoch, Loss: 0.107602\n",
      "VALIDATION Loss: 0.338155\n",
      "\n",
      "Epoch 45/50\n",
      "*****:\n",
      "[45/50] Loss: 0.112752\n",
      "[45/50] Loss: 0.108852\n",
      "Finish 45 epoch, Loss: 0.115765\n",
      "VALIDATION Loss: 0.314800\n",
      "\n",
      "Epoch 46/50\n",
      "*****:\n",
      "[46/50] Loss: 0.121336\n",
      "[46/50] Loss: 0.118043\n",
      "Finish 46 epoch, Loss: 0.117464\n",
      "VALIDATION Loss: 0.363114\n",
      "\n",
      "Epoch 47/50\n",
      "*****:\n",
      "[47/50] Loss: 0.131148\n",
      "[47/50] Loss: 0.118990\n",
      "Finish 47 epoch, Loss: 0.117020\n",
      "VALIDATION Loss: 0.432228\n",
      "\n",
      "Epoch 48/50\n",
      "*****:\n",
      "[48/50] Loss: 0.120868\n",
      "[48/50] Loss: 0.110471\n",
      "Finish 48 epoch, Loss: 0.111395\n",
      "VALIDATION Loss: 0.427951\n",
      "\n",
      "Epoch 49/50\n",
      "*****:\n",
      "[49/50] Loss: 0.112461\n",
      "[49/50] Loss: 0.109618\n",
      "Finish 49 epoch, Loss: 0.114016\n",
      "VALIDATION Loss: 0.502091\n",
      "\n",
      "Epoch 50/50\n",
      "*****:\n",
      "[50/50] Loss: 0.110053\n",
      "[50/50] Loss: 0.105154\n",
      "Finish 50 epoch, Loss: 0.109549\n",
      "VALIDATION Loss: 0.447942\n",
      "\n",
      "(8424, 4)\n",
      "submission1.csv\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    for epoch in range(num_epoches):\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epoches))\n",
    "        print('*' * 5 + ':')\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        for i, data in enumerate(train_loader, 1):\n",
    "    \n",
    "            img, label = data\n",
    "            if use_cuda:\n",
    "                img, label = Variable(img.cuda(async=True)), Variable(label.cuda(async=True))  # On GPU\n",
    "            else:\n",
    "                img, label = Variable(img), Variable(\n",
    "                    label)  # RuntimeError: expected CPU tensor (got CUDA tensor)\n",
    "    \n",
    "            out = model(img)\n",
    "            loss = criterion(out, label)\n",
    "            running_loss += loss.data[0] * label.size(0)\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            if i % 10 == 0:\n",
    "                all_losses.append(running_loss / (batch_size * i))\n",
    "                print('[{}/{}] Loss: {:.6f}'.format(\n",
    "                    epoch + 1, num_epoches, running_loss / (batch_size * i),\n",
    "                    running_acc / (batch_size * i)))\n",
    "    \n",
    "        print('Finish {} epoch, Loss: {:.6f}'.format(epoch + 1, running_loss / (len(train_ds))))\n",
    "    \n",
    "        model.eval()\n",
    "        eval_loss = 0\n",
    "        eval_acc = 0\n",
    "        for data in val_loader:\n",
    "            img, label = data\n",
    "    \n",
    "            if use_cuda:\n",
    "                img = Variable(img.cuda(async=True), volatile=True)\n",
    "                label = Variable(label.cuda(async=True), volatile=True)  # On GPU\n",
    "            else:\n",
    "                img = Variable(img, volatile=True)\n",
    "                label = Variable(label, volatile=True)\n",
    "    \n",
    "            out = model(img)\n",
    "            loss = criterion(out, label)\n",
    "            eval_loss += loss.data[0] * label.size(0)\n",
    "    \n",
    "        print('VALIDATION Loss: {:.6f}'.format(eval_loss / (len(val_ds))))\n",
    "        val_losses.append(eval_loss / (len(val_ds)))\n",
    "        print()\n",
    "    \n",
    "    torch.save(model.state_dict(), './cnn.pth')\n",
    "    \n",
    "    \n",
    "    df_test_set = pd.read_json('../_RawData/test.json/data/processed/test.json')\n",
    "    \n",
    "    df_test_set['band_1'] = df_test_set['band_1'].apply(lambda x: np.array(x).reshape(75, 75))\n",
    "    df_test_set['band_2'] = df_test_set['band_2'].apply(lambda x: np.array(x).reshape(75, 75))\n",
    "    df_test_set['inc_angle'] = pd.to_numeric(df_test_set['inc_angle'], errors='coerce')\n",
    "    \n",
    "    df_test_set.head(3)\n",
    "    \n",
    "    \n",
    "    print (df_test_set.shape)\n",
    "    columns = ['id', 'is_iceberg']\n",
    "    df_pred=pd.DataFrame(data=np.zeros((0,len(columns))), columns=columns)\n",
    "    # df_pred.id.astype(int)\n",
    "    \n",
    "    for index, row in df_test_set.iterrows():\n",
    "        rwo_no_id=row.drop('id')    \n",
    "        band_1_test = (rwo_no_id['band_1']).reshape(-1, 75, 75)\n",
    "        band_2_test = (rwo_no_id['band_2']).reshape(-1, 75, 75)\n",
    "        full_img_test = np.stack([band_1_test, band_2_test], axis=1)\n",
    "    \n",
    "        x_data_np = np.array(full_img_test, dtype=np.float32)        \n",
    "        if use_cuda:\n",
    "            X_tensor_test = Variable(torch.from_numpy(x_data_np).cuda()) # Note the conversion for pytorch    \n",
    "        else:\n",
    "            X_tensor_test = Variable(torch.from_numpy(x_data_np)) # Note the conversion for pytorch\n",
    "                        \n",
    "    #     X_tensor_test=X_tensor_test.view(1, trainX.shape[1]) # does not work with 1d tensors            \n",
    "        predicted_val = (model(X_tensor_test).data).float() # probabilities     \n",
    "        p_test =   predicted_val.cpu().numpy().item() # otherwise we get an array, we need a single float\n",
    "        \n",
    "        df_pred = df_pred.append({'id':row['id'], 'is_iceberg':p_test},ignore_index=True)\n",
    "    #     df_pred = df_pred.append({'id':row['id'].astype(int), 'probability':p_test},ignore_index=True)\n",
    "    \n",
    "    df_pred.head(5)\n",
    "    \n",
    "    \n",
    "    def savePred(df_pred):\n",
    "    #     csv_path = 'pred/p_{}_{}_{}.csv'.format(loss, name, (str(time.time())))\n",
    "    #     csv_path = 'pred_{}_{}.csv'.format(loss, (str(time.time())))\n",
    "        csv_path='submission1.csv'\n",
    "        df_pred.to_csv(csv_path, columns=('id', 'is_iceberg'), index=None)\n",
    "        print (csv_path)\n",
    "        \n",
    "    savePred (df_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
