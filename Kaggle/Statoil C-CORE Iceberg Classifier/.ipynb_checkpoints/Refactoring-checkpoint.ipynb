{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import pdb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras import backend as K\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import scipy\n",
    "from scipy import misc, ndimage\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "from scipy.ndimage import imread\n",
    "import helpers\n",
    "from models import DaveModel, DaveVGG, DaveVGG19, SimpleModel, LeNetModel, BestModel\n",
    "from trainer import Trainer\n",
    "from labeller import PseudoLabeller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_json(\"_RawData/train.json/data/processed/train.json\")\n",
    "test = pd.read_json(\"_RawData/test.json/data/processed/test.json\")\n",
    "\n",
    "X = helpers.get_images(train)\n",
    "X_test = helpers.get_images(test)\n",
    "\n",
    "y = to_categorical(train.is_iceberg.values,num_classes=2)\n",
    "\n",
    "ids = test[\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainRunner = Trainer(ids, X, y, X_test, [\n",
    "    DaveModel(ids),\n",
    "    DaveVGG(ids),\n",
    "    DaveVGG19(ids),\n",
    "    LeNetModel(ids),\n",
    "    BestModel(ids)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "================================================\n",
      "Model: davemodel\n",
      "Batch Size: 32\n",
      "Epochs: 50\n",
      "Epoch 1/50\n",
      "41/40 [==============================] - 4s - loss: 0.5788 - acc: 0.7038 - val_loss: 0.6754 - val_acc: 0.6573\n",
      "Epoch 2/50\n",
      "41/40 [==============================] - 1s - loss: 0.5039 - acc: 0.7393 - val_loss: 0.6410 - val_acc: 0.6573\n",
      "Epoch 3/50\n",
      "41/40 [==============================] - 1s - loss: 0.4743 - acc: 0.7644 - val_loss: 0.6493 - val_acc: 0.6573\n",
      "Epoch 4/50\n",
      "41/40 [==============================] - 1s - loss: 0.4491 - acc: 0.7709 - val_loss: 0.7121 - val_acc: 0.6573\n",
      "Epoch 5/50\n",
      "41/40 [==============================] - 1s - loss: 0.4021 - acc: 0.8032 - val_loss: 0.7779 - val_acc: 0.6573\n",
      "Epoch 6/50\n",
      "41/40 [==============================] - 1s - loss: 0.3898 - acc: 0.8124 - val_loss: 0.7852 - val_acc: 0.6573\n",
      "Epoch 7/50\n",
      "41/40 [==============================] - 1s - loss: 0.3661 - acc: 0.8197 - val_loss: 1.0326 - val_acc: 0.6573\n",
      "Epoch 8/50\n",
      "41/40 [==============================] - 2s - loss: 0.3586 - acc: 0.8308 - val_loss: 1.1484 - val_acc: 0.6573\n",
      "Epoch 9/50\n",
      "41/40 [==============================] - 2s - loss: 0.3436 - acc: 0.8426 - val_loss: 1.0779 - val_acc: 0.6573\n",
      "Epoch 10/50\n",
      "41/40 [==============================] - 2s - loss: 0.3435 - acc: 0.8368 - val_loss: 0.9181 - val_acc: 0.6480\n",
      "Epoch 11/50\n",
      "41/40 [==============================] - 2s - loss: 0.3320 - acc: 0.8464 - val_loss: 0.7674 - val_acc: 0.6667\n",
      "Epoch 12/50\n",
      "41/40 [==============================] - 2s - loss: 0.3253 - acc: 0.8444 - val_loss: 0.4942 - val_acc: 0.7726\n",
      "Epoch 13/50\n",
      "41/40 [==============================] - 2s - loss: 0.3222 - acc: 0.8574 - val_loss: 0.3994 - val_acc: 0.8162\n",
      "Epoch 14/50\n",
      "41/40 [==============================] - 2s - loss: 0.3018 - acc: 0.8566 - val_loss: 0.3540 - val_acc: 0.8162\n",
      "Epoch 15/50\n",
      "41/40 [==============================] - 2s - loss: 0.3084 - acc: 0.8491 - val_loss: 0.4141 - val_acc: 0.8380\n",
      "Epoch 16/50\n",
      "41/40 [==============================] - 2s - loss: 0.3264 - acc: 0.8342 - val_loss: 0.4207 - val_acc: 0.7695\n",
      "Epoch 17/50\n",
      "41/40 [==============================] - 2s - loss: 0.3109 - acc: 0.8521 - val_loss: 0.3476 - val_acc: 0.8224\n",
      "Epoch 18/50\n",
      "41/40 [==============================] - 2s - loss: 0.3141 - acc: 0.8673 - val_loss: 0.3699 - val_acc: 0.7975\n",
      "Epoch 19/50\n",
      "41/40 [==============================] - 1s - loss: 0.2894 - acc: 0.8589 - val_loss: 0.3120 - val_acc: 0.8629\n",
      "Epoch 20/50\n",
      "41/40 [==============================] - 1s - loss: 0.3078 - acc: 0.8616 - val_loss: 0.3020 - val_acc: 0.8505\n",
      "Epoch 21/50\n",
      "41/40 [==============================] - 1s - loss: 0.2987 - acc: 0.8475 - val_loss: 0.3205 - val_acc: 0.8567\n",
      "Epoch 22/50\n",
      "41/40 [==============================] - 1s - loss: 0.2846 - acc: 0.8643 - val_loss: 0.3282 - val_acc: 0.8224\n",
      "Epoch 23/50\n",
      "41/40 [==============================] - 1s - loss: 0.2825 - acc: 0.8635 - val_loss: 0.4037 - val_acc: 0.7726\n",
      "Epoch 24/50\n",
      "41/40 [==============================] - 1s - loss: 0.2861 - acc: 0.8670 - val_loss: 0.2954 - val_acc: 0.8505\n",
      "Epoch 25/50\n",
      "41/40 [==============================] - 2s - loss: 0.3008 - acc: 0.8571 - val_loss: 0.2942 - val_acc: 0.8474\n",
      "Epoch 26/50\n",
      "41/40 [==============================] - 1s - loss: 0.2768 - acc: 0.8734 - val_loss: 0.2948 - val_acc: 0.8536\n",
      "Epoch 27/50\n",
      "41/40 [==============================] - 1s - loss: 0.2816 - acc: 0.8681 - val_loss: 0.3020 - val_acc: 0.8505\n",
      "Epoch 28/50\n",
      "41/40 [==============================] - 1s - loss: 0.2813 - acc: 0.8650 - val_loss: 0.3041 - val_acc: 0.8255\n",
      "Epoch 29/50\n",
      "41/40 [==============================] - 1s - loss: 0.2943 - acc: 0.8624 - val_loss: 0.2916 - val_acc: 0.8474\n",
      "Epoch 30/50\n",
      "41/40 [==============================] - 1s - loss: 0.2892 - acc: 0.8681 - val_loss: 0.3074 - val_acc: 0.8131\n",
      "Epoch 31/50\n",
      "41/40 [==============================] - 1s - loss: 0.2715 - acc: 0.8787 - val_loss: 0.3114 - val_acc: 0.8474\n",
      "Epoch 32/50\n",
      "41/40 [==============================] - 1s - loss: 0.2834 - acc: 0.8719 - val_loss: 0.3145 - val_acc: 0.8318\n",
      "Epoch 33/50\n",
      "41/40 [==============================] - 1s - loss: 0.2801 - acc: 0.8636 - val_loss: 0.2958 - val_acc: 0.8598\n",
      "Epoch 34/50\n",
      "41/40 [==============================] - 1s - loss: 0.2770 - acc: 0.8605 - val_loss: 0.2990 - val_acc: 0.8474\n",
      "Epoch 35/50\n",
      "41/40 [==============================] - 1s - loss: 0.2628 - acc: 0.8864 - val_loss: 0.3970 - val_acc: 0.7850\n",
      "Epoch 36/50\n",
      "41/40 [==============================] - 1s - loss: 0.2733 - acc: 0.8731 - val_loss: 0.2886 - val_acc: 0.8754\n",
      "Epoch 37/50\n",
      "41/40 [==============================] - 1s - loss: 0.2748 - acc: 0.8738 - val_loss: 0.2848 - val_acc: 0.8536\n",
      "Epoch 38/50\n",
      "41/40 [==============================] - 1s - loss: 0.2761 - acc: 0.8830 - val_loss: 0.3304 - val_acc: 0.8162\n",
      "Epoch 39/50\n",
      "41/40 [==============================] - 1s - loss: 0.2623 - acc: 0.8662 - val_loss: 0.2995 - val_acc: 0.8723\n",
      "Epoch 40/50\n",
      "41/40 [==============================] - 1s - loss: 0.2591 - acc: 0.8803 - val_loss: 0.3016 - val_acc: 0.8442\n",
      "Epoch 41/50\n",
      "41/40 [==============================] - 1s - loss: 0.2484 - acc: 0.8818 - val_loss: 0.2919 - val_acc: 0.8879\n",
      "Epoch 42/50\n",
      "41/40 [==============================] - 1s - loss: 0.2565 - acc: 0.8864 - val_loss: 0.2972 - val_acc: 0.8411\n",
      "Epoch 43/50\n",
      "41/40 [==============================] - 1s - loss: 0.2484 - acc: 0.8864 - val_loss: 0.2777 - val_acc: 0.8660\n",
      "Epoch 44/50\n",
      "41/40 [==============================] - 1s - loss: 0.2456 - acc: 0.8872 - val_loss: 0.2841 - val_acc: 0.9003\n",
      "Epoch 45/50\n",
      "41/40 [==============================] - 1s - loss: 0.2521 - acc: 0.8894 - val_loss: 0.2751 - val_acc: 0.8847\n",
      "Epoch 46/50\n",
      "41/40 [==============================] - 2s - loss: 0.2568 - acc: 0.8754 - val_loss: 0.2882 - val_acc: 0.8941\n",
      "Epoch 47/50\n",
      "41/40 [==============================] - 1s - loss: 0.2443 - acc: 0.8970 - val_loss: 0.2976 - val_acc: 0.8411\n",
      "Epoch 48/50\n",
      "41/40 [==============================] - 1s - loss: 0.2410 - acc: 0.8803 - val_loss: 0.3063 - val_acc: 0.8224\n",
      "Epoch 49/50\n",
      "41/40 [==============================] - 1s - loss: 0.2395 - acc: 0.8970 - val_loss: 0.3200 - val_acc: 0.8162\n",
      "Epoch 50/50\n",
      "41/40 [==============================] - 1s - loss: 0.2483 - acc: 0.8940 - val_loss: 0.2861 - val_acc: 0.8442\n",
      "\n",
      "\n",
      "================================================\n",
      "Model: vgg\n",
      "Batch Size: 32\n",
      "Epochs: 50\n",
      "Epoch 1/50\n",
      "41/40 [==============================] - 7s - loss: 0.6842 - acc: 0.5470 - val_loss: 0.7437 - val_acc: 0.3707\n",
      "Epoch 2/50\n",
      "41/40 [==============================] - 5s - loss: 0.6719 - acc: 0.5842 - val_loss: 0.7078 - val_acc: 0.3832\n",
      "Epoch 3/50\n",
      "41/40 [==============================] - 5s - loss: 0.6928 - acc: 0.5070 - val_loss: 0.6943 - val_acc: 0.3427\n",
      "Epoch 4/50\n",
      "41/40 [==============================] - 5s - loss: 0.6922 - acc: 0.5353 - val_loss: 0.6953 - val_acc: 0.4393\n",
      "Epoch 5/50\n",
      "41/40 [==============================] - 5s - loss: 0.6711 - acc: 0.5798 - val_loss: 0.6441 - val_acc: 0.5732\n",
      "Epoch 6/50\n",
      "41/40 [==============================] - 5s - loss: 0.6505 - acc: 0.5825 - val_loss: 0.6788 - val_acc: 0.4611\n",
      "Epoch 7/50\n",
      "41/40 [==============================] - 5s - loss: 0.6772 - acc: 0.5349 - val_loss: 0.5993 - val_acc: 0.5639\n",
      "Epoch 8/50\n",
      "41/40 [==============================] - 5s - loss: 0.6865 - acc: 0.5776 - val_loss: 0.6460 - val_acc: 0.6573\n",
      "Epoch 9/50\n",
      "41/40 [==============================] - 5s - loss: 0.6828 - acc: 0.5784 - val_loss: 0.7331 - val_acc: 0.5109\n",
      "Epoch 10/50\n",
      "41/40 [==============================] - 5s - loss: 0.5960 - acc: 0.6600 - val_loss: 0.6464 - val_acc: 0.4860\n",
      "Epoch 11/50\n",
      "41/40 [==============================] - 5s - loss: 0.5991 - acc: 0.6595 - val_loss: 0.5910 - val_acc: 0.5794\n",
      "Epoch 12/50\n",
      "41/40 [==============================] - 5s - loss: 0.5672 - acc: 0.6797 - val_loss: 0.6067 - val_acc: 0.5265\n",
      "Epoch 13/50\n",
      "41/40 [==============================] - 5s - loss: 0.5613 - acc: 0.6821 - val_loss: 0.6129 - val_acc: 0.5514\n",
      "Epoch 14/50\n",
      "41/40 [==============================] - 5s - loss: 0.5403 - acc: 0.7102 - val_loss: 0.6320 - val_acc: 0.5639\n",
      "Epoch 15/50\n",
      "41/40 [==============================] - 5s - loss: 0.5326 - acc: 0.7106 - val_loss: 0.6056 - val_acc: 0.5607\n",
      "Epoch 16/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/40 [==============================] - 5s - loss: 0.5310 - acc: 0.7152 - val_loss: 0.5710 - val_acc: 0.6137\n",
      "Epoch 17/50\n",
      "41/40 [==============================] - 5s - loss: 0.5413 - acc: 0.7171 - val_loss: 0.5565 - val_acc: 0.6075\n",
      "Epoch 18/50\n",
      "41/40 [==============================] - 5s - loss: 0.5096 - acc: 0.7579 - val_loss: 0.6273 - val_acc: 0.6355\n",
      "Epoch 19/50\n",
      "41/40 [==============================] - 5s - loss: 0.4870 - acc: 0.7598 - val_loss: 0.4743 - val_acc: 0.7352\n",
      "Epoch 20/50\n",
      "41/40 [==============================] - 5s - loss: 0.4379 - acc: 0.7953 - val_loss: 0.4762 - val_acc: 0.7601\n",
      "Epoch 21/50\n",
      "41/40 [==============================] - 5s - loss: 0.4305 - acc: 0.7930 - val_loss: 0.3714 - val_acc: 0.8100\n",
      "Epoch 22/50\n",
      "41/40 [==============================] - 5s - loss: 0.4126 - acc: 0.8022 - val_loss: 0.4117 - val_acc: 0.8193\n",
      "Epoch 23/50\n",
      "41/40 [==============================] - 5s - loss: 0.4450 - acc: 0.7930 - val_loss: 0.4201 - val_acc: 0.8069\n",
      "Epoch 24/50\n",
      "41/40 [==============================] - 5s - loss: 0.4110 - acc: 0.8177 - val_loss: 0.3454 - val_acc: 0.8287\n",
      "Epoch 25/50\n",
      "41/40 [==============================] - 5s - loss: 0.4046 - acc: 0.8171 - val_loss: 0.3348 - val_acc: 0.8474\n",
      "Epoch 26/50\n",
      "41/40 [==============================] - 5s - loss: 0.3803 - acc: 0.8315 - val_loss: 0.3422 - val_acc: 0.8380\n",
      "Epoch 27/50\n",
      "41/40 [==============================] - 5s - loss: 0.3624 - acc: 0.8471 - val_loss: 0.4144 - val_acc: 0.7975\n",
      "Epoch 28/50\n",
      "41/40 [==============================] - 5s - loss: 0.3566 - acc: 0.8414 - val_loss: 0.3860 - val_acc: 0.8131\n",
      "Epoch 29/50\n",
      "41/40 [==============================] - 5s - loss: 0.3847 - acc: 0.8331 - val_loss: 0.3825 - val_acc: 0.8131\n",
      "Epoch 30/50\n",
      "41/40 [==============================] - 5s - loss: 0.3769 - acc: 0.8360 - val_loss: 0.3453 - val_acc: 0.8411\n",
      "Epoch 31/50\n",
      "41/40 [==============================] - 5s - loss: 0.3611 - acc: 0.8467 - val_loss: 0.3752 - val_acc: 0.8287\n",
      "Epoch 32/50\n",
      "41/40 [==============================] - 5s - loss: 0.3546 - acc: 0.8444 - val_loss: 0.3869 - val_acc: 0.8069\n",
      "Epoch 33/50\n",
      "41/40 [==============================] - 5s - loss: 0.3383 - acc: 0.8543 - val_loss: 0.3171 - val_acc: 0.8380\n",
      "Epoch 34/50\n",
      "41/40 [==============================] - 5s - loss: 0.3388 - acc: 0.8536 - val_loss: 0.3338 - val_acc: 0.8349\n",
      "Epoch 35/50\n",
      "41/40 [==============================] - 5s - loss: 0.3167 - acc: 0.8566 - val_loss: 0.3095 - val_acc: 0.8474\n",
      "Epoch 36/50\n",
      "41/40 [==============================] - 5s - loss: 0.3344 - acc: 0.8597 - val_loss: 0.3177 - val_acc: 0.8349\n",
      "Epoch 37/50\n",
      "41/40 [==============================] - 5s - loss: 0.3597 - acc: 0.8407 - val_loss: 0.3931 - val_acc: 0.8255\n",
      "Epoch 38/50\n",
      "41/40 [==============================] - 5s - loss: 0.3886 - acc: 0.8304 - val_loss: 0.3159 - val_acc: 0.8474\n",
      "Epoch 39/50\n",
      "41/40 [==============================] - 5s - loss: 0.3243 - acc: 0.8604 - val_loss: 0.3078 - val_acc: 0.8567\n",
      "Epoch 40/50\n",
      "41/40 [==============================] - 5s - loss: 0.3293 - acc: 0.8482 - val_loss: 0.3185 - val_acc: 0.8349\n",
      "Epoch 41/50\n",
      "41/40 [==============================] - 5s - loss: 0.3169 - acc: 0.8650 - val_loss: 0.3216 - val_acc: 0.8411\n",
      "Epoch 42/50\n",
      "41/40 [==============================] - 5s - loss: 0.3205 - acc: 0.8574 - val_loss: 0.3124 - val_acc: 0.8474\n",
      "Epoch 43/50\n",
      "41/40 [==============================] - 5s - loss: 0.2961 - acc: 0.8688 - val_loss: 0.3596 - val_acc: 0.8442\n",
      "Epoch 44/50\n",
      "41/40 [==============================] - 5s - loss: 0.3393 - acc: 0.8604 - val_loss: 0.3989 - val_acc: 0.8224\n",
      "Epoch 45/50\n",
      "41/40 [==============================] - 5s - loss: 0.3297 - acc: 0.8589 - val_loss: 0.3884 - val_acc: 0.7913\n",
      "Epoch 46/50\n",
      "41/40 [==============================] - 5s - loss: 0.3159 - acc: 0.8548 - val_loss: 0.3300 - val_acc: 0.8442\n",
      "Epoch 47/50\n",
      "41/40 [==============================] - 5s - loss: 0.3083 - acc: 0.8612 - val_loss: 0.3278 - val_acc: 0.8287\n",
      "Epoch 48/50\n",
      "41/40 [==============================] - 5s - loss: 0.3000 - acc: 0.8688 - val_loss: 0.3154 - val_acc: 0.8474\n",
      "Epoch 49/50\n",
      "41/40 [==============================] - 5s - loss: 0.3205 - acc: 0.8502 - val_loss: 0.4003 - val_acc: 0.7757\n",
      "Epoch 50/50\n",
      "41/40 [==============================] - 5s - loss: 0.3383 - acc: 0.8482 - val_loss: 0.5589 - val_acc: 0.6417\n",
      "\n",
      "\n",
      "================================================\n",
      "Model: vgg19\n",
      "Batch Size: 32\n",
      "Epochs: 50\n",
      "Epoch 1/50\n",
      "41/40 [==============================] - 7s - loss: 0.6945 - acc: 0.4971 - val_loss: 0.6937 - val_acc: 0.3427\n",
      "Epoch 2/50\n",
      "41/40 [==============================] - 6s - loss: 0.6923 - acc: 0.4975 - val_loss: 0.6935 - val_acc: 0.3427\n",
      "Epoch 3/50\n",
      "41/40 [==============================] - 6s - loss: 0.6856 - acc: 0.5539 - val_loss: 0.7161 - val_acc: 0.5016\n",
      "Epoch 4/50\n",
      "41/40 [==============================] - 6s - loss: 0.6588 - acc: 0.5833 - val_loss: 0.7944 - val_acc: 0.4330\n",
      "Epoch 5/50\n",
      "41/40 [==============================] - 6s - loss: 0.6568 - acc: 0.6196 - val_loss: 0.7280 - val_acc: 0.3614\n",
      "Epoch 6/50\n",
      "41/40 [==============================] - 6s - loss: 0.6562 - acc: 0.6576 - val_loss: 0.6023 - val_acc: 0.5857\n",
      "Epoch 7/50\n",
      "41/40 [==============================] - 6s - loss: 0.6158 - acc: 0.6584 - val_loss: 0.6443 - val_acc: 0.6106\n",
      "Epoch 8/50\n",
      "41/40 [==============================] - 6s - loss: 0.5977 - acc: 0.7018 - val_loss: 0.7234 - val_acc: 0.5265\n",
      "Epoch 9/50\n",
      "41/40 [==============================] - 6s - loss: 0.6086 - acc: 0.6844 - val_loss: 0.6464 - val_acc: 0.6075\n",
      "Epoch 10/50\n",
      "41/40 [==============================] - 6s - loss: 0.5625 - acc: 0.7155 - val_loss: 0.6573 - val_acc: 0.5670\n",
      "Epoch 11/50\n",
      "41/40 [==============================] - 6s - loss: 0.6068 - acc: 0.6736 - val_loss: 0.6160 - val_acc: 0.6386\n",
      "Epoch 12/50\n",
      "41/40 [==============================] - 6s - loss: 0.5603 - acc: 0.7183 - val_loss: 0.5902 - val_acc: 0.6511\n",
      "Epoch 13/50\n",
      "41/40 [==============================] - 6s - loss: 0.5370 - acc: 0.7339 - val_loss: 0.6249 - val_acc: 0.6542\n",
      "Epoch 14/50\n",
      "41/40 [==============================] - 6s - loss: 0.5358 - acc: 0.7308 - val_loss: 0.6734 - val_acc: 0.5732\n",
      "Epoch 15/50\n",
      "41/40 [==============================] - 6s - loss: 0.5227 - acc: 0.7392 - val_loss: 0.5310 - val_acc: 0.6885\n",
      "Epoch 16/50\n",
      "41/40 [==============================] - 6s - loss: 0.5144 - acc: 0.7541 - val_loss: 0.5133 - val_acc: 0.7570\n",
      "Epoch 17/50\n",
      "41/40 [==============================] - 6s - loss: 0.5104 - acc: 0.7389 - val_loss: 0.5337 - val_acc: 0.6947\n",
      "Epoch 18/50\n",
      "41/40 [==============================] - 6s - loss: 0.4641 - acc: 0.7839 - val_loss: 0.4731 - val_acc: 0.7913\n",
      "Epoch 19/50\n",
      "41/40 [==============================] - 6s - loss: 0.5130 - acc: 0.7484 - val_loss: 0.5981 - val_acc: 0.7508\n",
      "Epoch 20/50\n",
      "41/40 [==============================] - 6s - loss: 0.4955 - acc: 0.7720 - val_loss: 0.6980 - val_acc: 0.5794\n",
      "Epoch 21/50\n",
      "41/40 [==============================] - 6s - loss: 0.4818 - acc: 0.7808 - val_loss: 0.7633 - val_acc: 0.5358\n",
      "Epoch 22/50\n",
      "41/40 [==============================] - 6s - loss: 0.4841 - acc: 0.7712 - val_loss: 0.6006 - val_acc: 0.6822\n",
      "Epoch 23/50\n",
      "41/40 [==============================] - 6s - loss: 0.4229 - acc: 0.7938 - val_loss: 0.4096 - val_acc: 0.8380\n",
      "Epoch 24/50\n",
      "41/40 [==============================] - 6s - loss: 0.4170 - acc: 0.8098 - val_loss: 0.4402 - val_acc: 0.7882\n",
      "Epoch 25/50\n",
      "41/40 [==============================] - 6s - loss: 0.4190 - acc: 0.7930 - val_loss: 0.5181 - val_acc: 0.6760\n",
      "Epoch 26/50\n",
      "41/40 [==============================] - 6s - loss: 0.4160 - acc: 0.8044 - val_loss: 0.5223 - val_acc: 0.6791\n",
      "Epoch 27/50\n",
      "41/40 [==============================] - 6s - loss: 0.3656 - acc: 0.8391 - val_loss: 0.3546 - val_acc: 0.8536\n",
      "Epoch 28/50\n",
      "41/40 [==============================] - 6s - loss: 0.4028 - acc: 0.8113 - val_loss: 0.4587 - val_acc: 0.7321\n",
      "Epoch 29/50\n",
      "41/40 [==============================] - 6s - loss: 0.3696 - acc: 0.8238 - val_loss: 0.4646 - val_acc: 0.7445\n",
      "Epoch 30/50\n",
      "41/40 [==============================] - 6s - loss: 0.3517 - acc: 0.8421 - val_loss: 0.3944 - val_acc: 0.8162\n",
      "Epoch 31/50\n",
      "41/40 [==============================] - 6s - loss: 0.3409 - acc: 0.8498 - val_loss: 0.3382 - val_acc: 0.8754\n",
      "Epoch 32/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/40 [==============================] - 6s - loss: 0.3397 - acc: 0.8444 - val_loss: 0.4166 - val_acc: 0.7882\n",
      "Epoch 33/50\n",
      "41/40 [==============================] - 6s - loss: 0.3456 - acc: 0.8254 - val_loss: 0.3853 - val_acc: 0.8629\n",
      "Epoch 34/50\n",
      "41/40 [==============================] - 6s - loss: 0.3285 - acc: 0.8551 - val_loss: 0.3366 - val_acc: 0.8536\n",
      "Epoch 35/50\n",
      "41/40 [==============================] - 6s - loss: 0.3205 - acc: 0.8426 - val_loss: 0.3942 - val_acc: 0.8037\n",
      "Epoch 36/50\n",
      "41/40 [==============================] - 6s - loss: 0.3335 - acc: 0.8452 - val_loss: 0.4288 - val_acc: 0.7819\n",
      "Epoch 37/50\n",
      "41/40 [==============================] - 6s - loss: 0.3487 - acc: 0.8655 - val_loss: 0.3919 - val_acc: 0.8006\n",
      "Epoch 38/50\n",
      "41/40 [==============================] - 6s - loss: 0.4630 - acc: 0.7999 - val_loss: 0.3879 - val_acc: 0.8287\n",
      "Epoch 39/50\n",
      "41/40 [==============================] - 6s - loss: 0.3245 - acc: 0.8391 - val_loss: 0.3344 - val_acc: 0.9097\n",
      "Epoch 40/50\n",
      "41/40 [==============================] - 6s - loss: 0.3035 - acc: 0.8719 - val_loss: 0.3546 - val_acc: 0.8847\n",
      "Epoch 41/50\n",
      "41/40 [==============================] - 6s - loss: 0.3044 - acc: 0.8627 - val_loss: 0.3373 - val_acc: 0.8660\n",
      "Epoch 42/50\n",
      "41/40 [==============================] - 6s - loss: 0.3055 - acc: 0.8571 - val_loss: 0.3455 - val_acc: 0.8629\n",
      "Epoch 43/50\n",
      "41/40 [==============================] - 6s - loss: 0.2963 - acc: 0.8513 - val_loss: 0.4198 - val_acc: 0.7850\n",
      "Epoch 44/50\n",
      "41/40 [==============================] - 6s - loss: 0.3328 - acc: 0.8444 - val_loss: 0.3223 - val_acc: 0.8785\n",
      "Epoch 45/50\n",
      "41/40 [==============================] - 6s - loss: 0.2887 - acc: 0.8650 - val_loss: 0.4105 - val_acc: 0.7788\n",
      "Epoch 46/50\n",
      "41/40 [==============================] - 6s - loss: 0.2927 - acc: 0.8620 - val_loss: 0.3675 - val_acc: 0.8318\n",
      "Epoch 47/50\n",
      "41/40 [==============================] - 6s - loss: 0.2811 - acc: 0.8601 - val_loss: 0.3472 - val_acc: 0.8847\n",
      "Epoch 48/50\n",
      "41/40 [==============================] - 6s - loss: 0.2979 - acc: 0.8453 - val_loss: 0.3643 - val_acc: 0.8380\n",
      "Epoch 49/50\n",
      "41/40 [==============================] - 6s - loss: 0.2774 - acc: 0.8723 - val_loss: 0.3457 - val_acc: 0.8879\n",
      "Epoch 50/50\n",
      "41/40 [==============================] - 6s - loss: 0.3139 - acc: 0.8528 - val_loss: 0.3927 - val_acc: 0.8442\n",
      "\n",
      "\n",
      "================================================\n",
      "Model: lenet\n",
      "Batch Size: 32\n",
      "Epochs: 50\n",
      "Epoch 1/50\n",
      "41/40 [==============================] - 2s - loss: 0.6766 - acc: 0.6141 - val_loss: 0.6831 - val_acc: 0.5763\n",
      "Epoch 2/50\n",
      "41/40 [==============================] - 2s - loss: 0.6190 - acc: 0.6721 - val_loss: 0.6942 - val_acc: 0.5670\n",
      "Epoch 3/50\n",
      "41/40 [==============================] - 1s - loss: 0.6075 - acc: 0.6744 - val_loss: 0.6584 - val_acc: 0.5763\n",
      "Epoch 4/50\n",
      "41/40 [==============================] - 1s - loss: 0.5874 - acc: 0.7011 - val_loss: 0.6370 - val_acc: 0.5794\n",
      "Epoch 5/50\n",
      "41/40 [==============================] - 1s - loss: 0.5637 - acc: 0.7079 - val_loss: 0.5895 - val_acc: 0.6262\n",
      "Epoch 6/50\n",
      "41/40 [==============================] - 1s - loss: 0.5500 - acc: 0.6955 - val_loss: 0.6545 - val_acc: 0.5701\n",
      "Epoch 7/50\n",
      "41/40 [==============================] - 1s - loss: 0.5488 - acc: 0.7233 - val_loss: 0.5861 - val_acc: 0.6355\n",
      "Epoch 8/50\n",
      "41/40 [==============================] - 1s - loss: 0.5405 - acc: 0.7262 - val_loss: 0.5977 - val_acc: 0.6199\n",
      "Epoch 9/50\n",
      "41/40 [==============================] - 1s - loss: 0.5148 - acc: 0.7366 - val_loss: 0.5254 - val_acc: 0.7196\n",
      "Epoch 10/50\n",
      "41/40 [==============================] - 1s - loss: 0.4993 - acc: 0.7514 - val_loss: 0.5435 - val_acc: 0.6885\n",
      "Epoch 11/50\n",
      "41/40 [==============================] - 1s - loss: 0.4888 - acc: 0.7595 - val_loss: 0.5446 - val_acc: 0.6822\n",
      "Epoch 12/50\n",
      "41/40 [==============================] - 1s - loss: 0.4679 - acc: 0.7755 - val_loss: 0.5575 - val_acc: 0.6729\n",
      "Epoch 13/50\n",
      "41/40 [==============================] - 1s - loss: 0.4576 - acc: 0.7983 - val_loss: 0.6196 - val_acc: 0.6480\n",
      "Epoch 14/50\n",
      "41/40 [==============================] - 1s - loss: 0.4431 - acc: 0.7888 - val_loss: 0.4821 - val_acc: 0.7383\n",
      "Epoch 15/50\n",
      "41/40 [==============================] - 1s - loss: 0.4176 - acc: 0.8048 - val_loss: 0.4778 - val_acc: 0.7321\n",
      "Epoch 16/50\n",
      "41/40 [==============================] - 1s - loss: 0.4495 - acc: 0.7811 - val_loss: 0.4442 - val_acc: 0.8100\n",
      "Epoch 17/50\n",
      "41/40 [==============================] - 1s - loss: 0.4240 - acc: 0.8048 - val_loss: 0.4321 - val_acc: 0.8162\n",
      "Epoch 18/50\n",
      "41/40 [==============================] - 1s - loss: 0.4227 - acc: 0.8026 - val_loss: 0.4887 - val_acc: 0.7944\n",
      "Epoch 19/50\n",
      "41/40 [==============================] - 1s - loss: 0.4260 - acc: 0.8109 - val_loss: 0.5131 - val_acc: 0.7165\n",
      "Epoch 20/50\n",
      "41/40 [==============================] - 1s - loss: 0.4142 - acc: 0.8029 - val_loss: 0.4898 - val_acc: 0.7383\n",
      "Epoch 21/50\n",
      "41/40 [==============================] - 1s - loss: 0.4033 - acc: 0.8105 - val_loss: 0.3947 - val_acc: 0.8255\n",
      "Epoch 22/50\n",
      "41/40 [==============================] - 1s - loss: 0.3704 - acc: 0.8330 - val_loss: 0.4122 - val_acc: 0.8100\n",
      "Epoch 23/50\n",
      "41/40 [==============================] - 1s - loss: 0.3897 - acc: 0.8162 - val_loss: 0.4547 - val_acc: 0.7726\n",
      "Epoch 24/50\n",
      "41/40 [==============================] - 1s - loss: 0.3731 - acc: 0.8147 - val_loss: 0.5695 - val_acc: 0.7103\n",
      "Epoch 25/50\n",
      "41/40 [==============================] - 1s - loss: 0.3731 - acc: 0.8296 - val_loss: 0.4429 - val_acc: 0.7788\n",
      "Epoch 26/50\n",
      "41/40 [==============================] - 1s - loss: 0.3767 - acc: 0.8235 - val_loss: 0.4125 - val_acc: 0.8100\n",
      "Epoch 27/50\n",
      "41/40 [==============================] - 1s - loss: 0.3807 - acc: 0.8266 - val_loss: 0.3885 - val_acc: 0.8255\n",
      "Epoch 28/50\n",
      "41/40 [==============================] - 1s - loss: 0.3957 - acc: 0.8132 - val_loss: 0.4359 - val_acc: 0.7664\n",
      "Epoch 29/50\n",
      "41/40 [==============================] - 1s - loss: 0.3839 - acc: 0.8174 - val_loss: 0.4397 - val_acc: 0.7664\n",
      "Epoch 30/50\n",
      "41/40 [==============================] - 1s - loss: 0.3496 - acc: 0.8216 - val_loss: 0.3508 - val_acc: 0.8380\n",
      "Epoch 31/50\n",
      "41/40 [==============================] - 1s - loss: 0.3412 - acc: 0.8365 - val_loss: 0.4395 - val_acc: 0.7726\n",
      "Epoch 32/50\n",
      "41/40 [==============================] - 1s - loss: 0.3509 - acc: 0.8307 - val_loss: 0.3629 - val_acc: 0.8723\n",
      "Epoch 33/50\n",
      "41/40 [==============================] - 1s - loss: 0.3449 - acc: 0.8399 - val_loss: 0.3522 - val_acc: 0.8660\n",
      "Epoch 34/50\n",
      "41/40 [==============================] - 1s - loss: 0.3423 - acc: 0.8410 - val_loss: 0.4620 - val_acc: 0.7757\n",
      "Epoch 35/50\n",
      "41/40 [==============================] - 1s - loss: 0.3350 - acc: 0.8551 - val_loss: 0.4082 - val_acc: 0.8131\n",
      "Epoch 36/50\n",
      "41/40 [==============================] - 1s - loss: 0.3511 - acc: 0.8354 - val_loss: 0.3699 - val_acc: 0.8162\n",
      "Epoch 37/50\n",
      "41/40 [==============================] - 1s - loss: 0.3335 - acc: 0.8460 - val_loss: 0.4400 - val_acc: 0.7788\n",
      "Epoch 38/50\n",
      "41/40 [==============================] - 1s - loss: 0.3234 - acc: 0.8536 - val_loss: 0.3898 - val_acc: 0.8224\n",
      "Epoch 39/50\n",
      "41/40 [==============================] - 1s - loss: 0.3300 - acc: 0.8426 - val_loss: 0.3953 - val_acc: 0.8037\n",
      "Epoch 40/50\n",
      "41/40 [==============================] - 1s - loss: 0.3179 - acc: 0.8349 - val_loss: 0.3466 - val_acc: 0.8629\n",
      "Epoch 41/50\n",
      "41/40 [==============================] - 1s - loss: 0.3257 - acc: 0.8449 - val_loss: 0.3245 - val_acc: 0.8380\n",
      "Epoch 42/50\n",
      "41/40 [==============================] - 1s - loss: 0.2998 - acc: 0.8734 - val_loss: 0.3368 - val_acc: 0.8287\n",
      "Epoch 43/50\n",
      "41/40 [==============================] - 1s - loss: 0.3129 - acc: 0.8540 - val_loss: 0.3864 - val_acc: 0.8598\n",
      "Epoch 44/50\n",
      "41/40 [==============================] - 1s - loss: 0.3697 - acc: 0.8182 - val_loss: 0.3973 - val_acc: 0.7975\n",
      "Epoch 45/50\n",
      "41/40 [==============================] - 1s - loss: 0.3287 - acc: 0.8426 - val_loss: 0.3358 - val_acc: 0.8505\n",
      "Epoch 46/50\n",
      "41/40 [==============================] - 1s - loss: 0.3104 - acc: 0.8665 - val_loss: 0.3470 - val_acc: 0.8380\n",
      "Epoch 47/50\n",
      "41/40 [==============================] - 1s - loss: 0.3053 - acc: 0.8574 - val_loss: 0.3192 - val_acc: 0.8816\n",
      "Epoch 48/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/40 [==============================] - 1s - loss: 0.3051 - acc: 0.8589 - val_loss: 0.3144 - val_acc: 0.8692\n",
      "Epoch 49/50\n",
      "41/40 [==============================] - 1s - loss: 0.3039 - acc: 0.8582 - val_loss: 0.4072 - val_acc: 0.7913\n",
      "Epoch 50/50\n",
      "41/40 [==============================] - 1s - loss: 0.3193 - acc: 0.8471 - val_loss: 0.3633 - val_acc: 0.8318\n",
      "\n",
      "\n",
      "================================================\n",
      "Model: best\n",
      "Batch Size: 32\n",
      "Epochs: 50\n",
      "Epoch 1/50\n",
      "41/40 [==============================] - 2s - loss: 0.6883 - acc: 0.5258 - val_loss: 0.6919 - val_acc: 0.4330\n",
      "Epoch 2/50\n",
      "41/40 [==============================] - 1s - loss: 0.6708 - acc: 0.5631 - val_loss: 0.6771 - val_acc: 0.6573\n",
      "Epoch 3/50\n",
      "41/40 [==============================] - 1s - loss: 0.6738 - acc: 0.5704 - val_loss: 0.6900 - val_acc: 0.5016\n",
      "Epoch 4/50\n",
      "41/40 [==============================] - 1s - loss: 0.6213 - acc: 0.6530 - val_loss: 0.6233 - val_acc: 0.6262\n",
      "Epoch 5/50\n",
      "41/40 [==============================] - 1s - loss: 0.5953 - acc: 0.7041 - val_loss: 0.6242 - val_acc: 0.5981\n",
      "Epoch 6/50\n",
      "41/40 [==============================] - 1s - loss: 0.5805 - acc: 0.6977 - val_loss: 0.5811 - val_acc: 0.6386\n",
      "Epoch 7/50\n",
      "41/40 [==============================] - 1s - loss: 0.5583 - acc: 0.7186 - val_loss: 0.6156 - val_acc: 0.5888\n",
      "Epoch 8/50\n",
      "41/40 [==============================] - 1s - loss: 0.5232 - acc: 0.7369 - val_loss: 0.5834 - val_acc: 0.6449\n",
      "Epoch 9/50\n",
      "41/40 [==============================] - 1s - loss: 0.5093 - acc: 0.7598 - val_loss: 0.5209 - val_acc: 0.6822\n",
      "Epoch 10/50\n",
      "41/40 [==============================] - 1s - loss: 0.4768 - acc: 0.7610 - val_loss: 0.4509 - val_acc: 0.7913\n",
      "Epoch 11/50\n",
      "41/40 [==============================] - 1s - loss: 0.4614 - acc: 0.7971 - val_loss: 0.5061 - val_acc: 0.7072\n",
      "Epoch 12/50\n",
      "41/40 [==============================] - 1s - loss: 0.4643 - acc: 0.7744 - val_loss: 0.4309 - val_acc: 0.8069\n",
      "Epoch 13/50\n",
      "41/40 [==============================] - 1s - loss: 0.4728 - acc: 0.7667 - val_loss: 0.5917 - val_acc: 0.6636\n",
      "Epoch 14/50\n",
      "41/40 [==============================] - 1s - loss: 0.5604 - acc: 0.7148 - val_loss: 0.7384 - val_acc: 0.5078\n",
      "Epoch 15/50\n",
      "41/40 [==============================] - 1s - loss: 0.5203 - acc: 0.7404 - val_loss: 0.4397 - val_acc: 0.8411\n",
      "Epoch 16/50\n",
      "41/40 [==============================] - 1s - loss: 0.4772 - acc: 0.7606 - val_loss: 0.4959 - val_acc: 0.7601\n",
      "Epoch 17/50\n",
      "41/40 [==============================] - 1s - loss: 0.4362 - acc: 0.7968 - val_loss: 0.5370 - val_acc: 0.7072\n",
      "Epoch 18/50\n",
      "41/40 [==============================] - 1s - loss: 0.5192 - acc: 0.7145 - val_loss: 0.5487 - val_acc: 0.7040\n",
      "Epoch 19/50\n",
      "41/40 [==============================] - 1s - loss: 0.4510 - acc: 0.7907 - val_loss: 0.4639 - val_acc: 0.7788\n",
      "Epoch 20/50\n",
      "41/40 [==============================] - 1s - loss: 0.3885 - acc: 0.8200 - val_loss: 0.3959 - val_acc: 0.8069\n",
      "Epoch 21/50\n",
      "41/40 [==============================] - 1s - loss: 0.4554 - acc: 0.7926 - val_loss: 0.3765 - val_acc: 0.8536\n",
      "Epoch 22/50\n",
      "41/40 [==============================] - 1s - loss: 0.3727 - acc: 0.8205 - val_loss: 0.3953 - val_acc: 0.8380\n",
      "Epoch 23/50\n",
      "41/40 [==============================] - 1s - loss: 0.4386 - acc: 0.7999 - val_loss: 0.3865 - val_acc: 0.8411\n",
      "Epoch 24/50\n",
      "41/40 [==============================] - 1s - loss: 0.3835 - acc: 0.8299 - val_loss: 0.4491 - val_acc: 0.7601\n",
      "Epoch 25/50\n",
      "41/40 [==============================] - 1s - loss: 0.3792 - acc: 0.8223 - val_loss: 0.3721 - val_acc: 0.8318\n",
      "Epoch 26/50\n",
      "41/40 [==============================] - 1s - loss: 0.3858 - acc: 0.8193 - val_loss: 0.3820 - val_acc: 0.8255\n",
      "Epoch 27/50\n",
      "41/40 [==============================] - 1s - loss: 0.3790 - acc: 0.8322 - val_loss: 0.4391 - val_acc: 0.7975\n",
      "Epoch 28/50\n",
      "41/40 [==============================] - 1s - loss: 0.3918 - acc: 0.8197 - val_loss: 0.3822 - val_acc: 0.8536\n",
      "Epoch 29/50\n",
      "41/40 [==============================] - 1s - loss: 0.3832 - acc: 0.8284 - val_loss: 0.3794 - val_acc: 0.8069\n",
      "Epoch 30/50\n",
      "41/40 [==============================] - 1s - loss: 0.3762 - acc: 0.8201 - val_loss: 0.4492 - val_acc: 0.7975\n",
      "Epoch 31/50\n",
      "41/40 [==============================] - 1s - loss: 0.3965 - acc: 0.8208 - val_loss: 0.4273 - val_acc: 0.8037\n",
      "Epoch 32/50\n",
      "41/40 [==============================] - 1s - loss: 0.3798 - acc: 0.8197 - val_loss: 0.3783 - val_acc: 0.8349\n",
      "Epoch 33/50\n",
      "41/40 [==============================] - 1s - loss: 0.3761 - acc: 0.8235 - val_loss: 0.3606 - val_acc: 0.8224\n",
      "Epoch 34/50\n",
      "41/40 [==============================] - 1s - loss: 0.3682 - acc: 0.8376 - val_loss: 0.3636 - val_acc: 0.8162\n",
      "Epoch 35/50\n",
      "41/40 [==============================] - 1s - loss: 0.3727 - acc: 0.8368 - val_loss: 0.3552 - val_acc: 0.8723\n",
      "Epoch 36/50\n",
      "41/40 [==============================] - 1s - loss: 0.3815 - acc: 0.8216 - val_loss: 0.3594 - val_acc: 0.8318\n",
      "Epoch 37/50\n",
      "41/40 [==============================] - 1s - loss: 0.3712 - acc: 0.8284 - val_loss: 0.3608 - val_acc: 0.8692\n",
      "Epoch 38/50\n",
      "41/40 [==============================] - 1s - loss: 0.3632 - acc: 0.8277 - val_loss: 0.3493 - val_acc: 0.8598\n",
      "Epoch 39/50\n",
      "41/40 [==============================] - 1s - loss: 0.4507 - acc: 0.8133 - val_loss: 0.3822 - val_acc: 0.8692\n",
      "Epoch 40/50\n",
      "41/40 [==============================] - 1s - loss: 0.4489 - acc: 0.7796 - val_loss: 0.4942 - val_acc: 0.7445\n",
      "Epoch 41/50\n",
      "41/40 [==============================] - 1s - loss: 0.4076 - acc: 0.8075 - val_loss: 0.3900 - val_acc: 0.8131\n",
      "Epoch 42/50\n",
      "41/40 [==============================] - 1s - loss: 0.3476 - acc: 0.8322 - val_loss: 0.3788 - val_acc: 0.8349\n",
      "Epoch 43/50\n",
      "41/40 [==============================] - 1s - loss: 0.3463 - acc: 0.8475 - val_loss: 0.3625 - val_acc: 0.8598\n",
      "Epoch 44/50\n",
      "41/40 [==============================] - 1s - loss: 0.3237 - acc: 0.8475 - val_loss: 0.3425 - val_acc: 0.8505\n",
      "Epoch 45/50\n",
      "41/40 [==============================] - 1s - loss: 0.3690 - acc: 0.8338 - val_loss: 0.3882 - val_acc: 0.8255\n",
      "Epoch 46/50\n",
      "41/40 [==============================] - 1s - loss: 0.3322 - acc: 0.8536 - val_loss: 0.3998 - val_acc: 0.8411\n",
      "Epoch 47/50\n",
      "41/40 [==============================] - 1s - loss: 0.3919 - acc: 0.8223 - val_loss: 0.4482 - val_acc: 0.7757\n",
      "Epoch 48/50\n",
      "41/40 [==============================] - 1s - loss: 0.3524 - acc: 0.8399 - val_loss: 0.3530 - val_acc: 0.8411\n",
      "Epoch 49/50\n",
      "41/40 [==============================] - 1s - loss: 0.3218 - acc: 0.8444 - val_loss: 0.3764 - val_acc: 0.8255\n",
      "Epoch 50/50\n",
      "41/40 [==============================] - 1s - loss: 0.3579 - acc: 0.8368 - val_loss: 0.3068 - val_acc: 0.8692\n",
      "\n",
      "\n",
      "================================================\n",
      "Predict Model: davemodel\n",
      "\n",
      "\n",
      "================================================\n",
      "Predict Model: vgg\n",
      "\n",
      "\n",
      "================================================\n",
      "Predict Model: vgg19\n",
      "\n",
      "\n",
      "================================================\n",
      "Predict Model: lenet\n",
      "\n",
      "\n",
      "================================================\n",
      "Predict Model: best\n",
      "davemodel\n",
      "vgg\n",
      "vgg19\n",
      "lenet\n",
      "best\n",
      "           ids  definitive  davemodel       vgg     vgg19     lenet      best  \\\n",
      "0     5941774d       False   0.609118  0.338854  0.443578  0.642698  0.654271   \n",
      "1     4023181e       False   0.496923  0.343355  0.950608  0.552938  0.778422   \n",
      "2     b20200e4       False   0.999873  0.388912  1.000000  0.997846  0.998998   \n",
      "3     e7f018bb        True   0.001376  0.032233  0.001072  0.001706  0.027113   \n",
      "4     4371c8c3        True   0.994152  0.525234  0.965022  0.552247  0.783079   \n",
      "5     a8d9b1fd       False   0.995310  0.048648  0.985114  0.146012  0.826237   \n",
      "6     29e7727e       False   0.986685  0.322457  0.968292  0.708887  0.909460   \n",
      "7     92a51ffb        True   0.001685  0.032846  0.000037  0.002255  0.008327   \n",
      "8     c769ac97        True   0.999856  0.745149  0.999450  0.993647  0.994616   \n",
      "9     aee0547d        True   0.999515  0.998159  1.000000  0.991294  0.999799   \n",
      "10    565b28ac        True   0.999949  0.999680  1.000000  0.999990  0.999999   \n",
      "11    e04e9775       False   0.964371  0.287072  0.995542  0.277776  0.779715   \n",
      "12    8e8161d1        True   0.993352  0.657326  0.983066  0.969104  0.985952   \n",
      "13    4cf4d256       False   0.403648  0.372126  0.992729  0.764891  0.808376   \n",
      "14    139e5324       False   0.492354  0.050944  0.977176  0.150648  0.703259   \n",
      "15    f156976f       False   0.983477  0.431868  0.910294  0.803931  0.966239   \n",
      "16    68a117cc        True   0.997853  0.861544  1.000000  0.984924  0.996555   \n",
      "17    d9aa7a56        True   0.867494  0.821596  0.999779  0.914511  0.778136   \n",
      "18    9005b143       False   0.658230  0.031238  0.986679  0.404529  0.672935   \n",
      "19    5f6d3988        True   0.132316  0.360757  0.013584  0.167079  0.153910   \n",
      "20    9ad70954       False   0.991035  0.717649  0.995669  0.419828  0.915233   \n",
      "21    b9087b9e        True   0.489897  0.313490  0.204363  0.134201  0.354588   \n",
      "22    a39a1427       False   0.820007  0.143706  0.990378  0.560971  0.907472   \n",
      "23    82fbe8ed        True   0.972375  0.771514  0.933102  0.887081  0.865750   \n",
      "24    1fae4879        True   0.999916  0.798256  0.998240  0.996880  0.972549   \n",
      "25    6dd8f13d       False   0.997765  0.903152  1.000000  0.487260  0.999658   \n",
      "26    bbad5958       False   0.683384  0.368450  0.369789  0.449054  0.770665   \n",
      "27    54527583        True   0.996879  0.601036  0.999994  0.553953  0.929302   \n",
      "28    be8fa29c        True   0.875186  0.701706  0.916208  0.724572  0.894015   \n",
      "29    81a3328f        True   0.000416  0.033320  0.000471  0.004817  0.024086   \n",
      "...        ...         ...        ...       ...       ...       ...       ...   \n",
      "8394  8ae30ce6       False   0.999149  0.911384  0.998525  0.461465  0.957790   \n",
      "8395  de27ed88       False   0.996586  0.165147  0.996690  0.941622  0.993882   \n",
      "8396  66d5196f        True   0.995763  0.754591  1.000000  0.998847  0.998861   \n",
      "8397  d85f1858       False   0.397588  0.281445  0.851844  0.780694  0.726823   \n",
      "8398  16dcb33a       False   0.781446  0.436775  0.999263  0.945188  0.899647   \n",
      "8399  eca3158e       False   0.999038  0.361205  0.990438  0.296318  0.857844   \n",
      "8400  08daeee6        True   0.995004  0.974140  0.999982  0.961068  0.997002   \n",
      "8401  e9c513ee       False   0.960918  0.449240  0.984779  0.831926  0.949297   \n",
      "8402  b1519fa6       False   0.948114  0.492601  0.999826  0.985723  0.922886   \n",
      "8403  dfc89540       False   0.357959  0.550356  0.784245  0.869272  0.834634   \n",
      "8404  8fd8c0e9       False   0.211175  0.408541  0.701393  0.942698  0.872043   \n",
      "8405  45df6347        True   0.993882  0.811863  0.983729  0.883565  0.963747   \n",
      "8406  bf7928d7        True   0.999078  0.656903  0.999327  0.734876  0.878060   \n",
      "8407  7b587c05        True   0.636296  0.670383  0.859603  0.845196  0.969433   \n",
      "8408  c2834388        True   0.855365  0.607249  1.000000  0.983319  0.912686   \n",
      "8409  146143c3        True   0.010136  0.066086  0.000369  0.022108  0.031588   \n",
      "8410  d59aee00        True   0.999905  0.924436  0.999729  0.998962  0.990851   \n",
      "8411  cbc0b93b       False   0.960609  0.092900  0.998567  0.743525  0.823361   \n",
      "8412  088e2ff7        True   0.151938  0.287912  0.007384  0.146691  0.154451   \n",
      "8413  673d33cd       False   0.988495  0.496604  0.999108  0.581158  0.878671   \n",
      "8414  674b031e        True   0.993226  0.996630  1.000000  0.958483  0.998148   \n",
      "8415  43db4207       False   0.811035  0.209601  0.997121  0.917410  0.798679   \n",
      "8416  156855e1        True   0.997792  0.938494  0.998028  0.974707  0.943510   \n",
      "8417  ac96cfb0        True   0.926111  0.740011  0.999893  0.873080  0.713728   \n",
      "8418  fe45aef5        True   0.019263  0.199094  0.000988  0.043547  0.037130   \n",
      "8419  16ee9b50        True   0.999966  0.752722  0.997086  0.988722  0.974697   \n",
      "8420  5a599eb7        True   0.965782  0.548990  0.999997  0.910848  0.941990   \n",
      "8421  df30d6dd        True   0.982327  0.596211  0.939138  0.650938  0.860780   \n",
      "8422  18af95b1        True   0.003396  0.027633  0.000151  0.015467  0.020533   \n",
      "8423  27d788c8        True   0.997607  0.875797  1.000000  0.782254  0.999397   \n",
      "\n",
      "      label  \n",
      "0         1  \n",
      "1         1  \n",
      "2         1  \n",
      "3         0  \n",
      "4         1  \n",
      "5         1  \n",
      "6         1  \n",
      "7         0  \n",
      "8         1  \n",
      "9         1  \n",
      "10        1  \n",
      "11        1  \n",
      "12        1  \n",
      "13        1  \n",
      "14        0  \n",
      "15        1  \n",
      "16        1  \n",
      "17        1  \n",
      "18        1  \n",
      "19        0  \n",
      "20        1  \n",
      "21        0  \n",
      "22        1  \n",
      "23        1  \n",
      "24        1  \n",
      "25        1  \n",
      "26        0  \n",
      "27        1  \n",
      "28        1  \n",
      "29        0  \n",
      "...     ...  \n",
      "8394      1  \n",
      "8395      1  \n",
      "8396      1  \n",
      "8397      1  \n",
      "8398      1  \n",
      "8399      1  \n",
      "8400      1  \n",
      "8401      1  \n",
      "8402      1  \n",
      "8403      1  \n",
      "8404      1  \n",
      "8405      1  \n",
      "8406      1  \n",
      "8407      1  \n",
      "8408      1  \n",
      "8409      0  \n",
      "8410      1  \n",
      "8411      1  \n",
      "8412      0  \n",
      "8413      1  \n",
      "8414      1  \n",
      "8415      1  \n",
      "8416      1  \n",
      "8417      1  \n",
      "8418      0  \n",
      "8419      1  \n",
      "8420      1  \n",
      "8421      1  \n",
      "8422      0  \n",
      "8423      1  \n",
      "\n",
      "[8424 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "label_maker = PseudoLabeller(trainRunner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ids</th>\n",
       "      <th>definitive</th>\n",
       "      <th>davemodel</th>\n",
       "      <th>vgg</th>\n",
       "      <th>vgg19</th>\n",
       "      <th>lenet</th>\n",
       "      <th>best</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5941774d</td>\n",
       "      <td>False</td>\n",
       "      <td>0.609118</td>\n",
       "      <td>0.338854</td>\n",
       "      <td>0.443578</td>\n",
       "      <td>0.642698</td>\n",
       "      <td>0.654271</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4023181e</td>\n",
       "      <td>False</td>\n",
       "      <td>0.496923</td>\n",
       "      <td>0.343355</td>\n",
       "      <td>0.950608</td>\n",
       "      <td>0.552938</td>\n",
       "      <td>0.778422</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b20200e4</td>\n",
       "      <td>False</td>\n",
       "      <td>0.999873</td>\n",
       "      <td>0.388912</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997846</td>\n",
       "      <td>0.998998</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e7f018bb</td>\n",
       "      <td>True</td>\n",
       "      <td>0.001376</td>\n",
       "      <td>0.032233</td>\n",
       "      <td>0.001072</td>\n",
       "      <td>0.001706</td>\n",
       "      <td>0.027113</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4371c8c3</td>\n",
       "      <td>True</td>\n",
       "      <td>0.994152</td>\n",
       "      <td>0.525234</td>\n",
       "      <td>0.965022</td>\n",
       "      <td>0.552247</td>\n",
       "      <td>0.783079</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ids  definitive  davemodel       vgg     vgg19     lenet      best  \\\n",
       "0  5941774d       False   0.609118  0.338854  0.443578  0.642698  0.654271   \n",
       "1  4023181e       False   0.496923  0.343355  0.950608  0.552938  0.778422   \n",
       "2  b20200e4       False   0.999873  0.388912  1.000000  0.997846  0.998998   \n",
       "3  e7f018bb        True   0.001376  0.032233  0.001072  0.001706  0.027113   \n",
       "4  4371c8c3        True   0.994152  0.525234  0.965022  0.552247  0.783079   \n",
       "\n",
       "   label  \n",
       "0      1  \n",
       "1      1  \n",
       "2      1  \n",
       "3      0  \n",
       "4      1  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_maker.results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1283, 75, 75, 3)\n",
      "(1283, 2)\n",
      "(4106, 75, 75, 3)\n",
      "(4106, 2)\n",
      "(5389, 75, 75, 3)\n",
      "(5389, 2)\n"
     ]
    }
   ],
   "source": [
    "print(label_maker.trainer.X_train.shape)\n",
    "print(label_maker.trainer.y_train.shape)\n",
    "\n",
    "X_train_pseudo = label_maker.trainer.X_test[label_maker.results[\"definitive\"]]\n",
    "y_train_pseudo = label_maker.results[label_maker.results[\"definitive\"]][\"label\"]\n",
    "\n",
    "print(X_train_pseudo.shape)\n",
    "\n",
    "new_train_y = []\n",
    "\n",
    "for record in y_train_pseudo:\n",
    "    if record == 0:\n",
    "        new_train_y.append([0., 1.])\n",
    "    elif record == 1:\n",
    "        new_train_y.append([1., 0.])\n",
    "        \n",
    "y_train_pseudo = np.array(new_train_y)\n",
    "\n",
    "print(y_train_pseudo.shape)\n",
    "\n",
    "# Add definitive data to original training data\n",
    "orig_train = label_maker.trainer.X_train.copy()\n",
    "X_train_pseudo = np.append(orig_train, X_train_pseudo, axis = 0)\n",
    "\n",
    "orig_labels = label_maker.trainer.y_train.copy()\n",
    "y_train_pseudo = np.append(orig_labels, y_train_pseudo, axis = 0)\n",
    "\n",
    "print(X_train_pseudo.shape)\n",
    "print(y_train_pseudo.shape)\n",
    "\n",
    "# anotherRunner = Trainer(ids, X_train_pseudo, y_train_pseudo, X_test, [\n",
    "#     DaveModel(ids),\n",
    "#     DaveVGG(ids),\n",
    "#     DaveVGG19(ids),\n",
    "#     LeNetModel(ids),\n",
    "#     BestModel(ids)\n",
    "# ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "================================================\n",
      "Model: best\n",
      "Batch Size: 32\n",
      "Epochs: 50\n",
      "Epoch 1/50\n",
      "169/168 [==============================] - 8s - loss: 0.4164 - acc: 0.7987 - val_loss: 0.5991 - val_acc: 0.6573\n",
      "Epoch 2/50\n",
      "169/168 [==============================] - 7s - loss: 0.2711 - acc: 0.8892 - val_loss: 0.5738 - val_acc: 0.7103\n",
      "Epoch 3/50\n",
      "169/168 [==============================] - 7s - loss: 0.2044 - acc: 0.9211 - val_loss: 0.4486 - val_acc: 0.7757\n",
      "Epoch 4/50\n",
      "169/168 [==============================] - 7s - loss: 0.1791 - acc: 0.9347 - val_loss: 0.7519 - val_acc: 0.6885\n",
      "Epoch 5/50\n",
      "169/168 [==============================] - 7s - loss: 0.1675 - acc: 0.9384 - val_loss: 0.4556 - val_acc: 0.8100\n",
      "Epoch 6/50\n",
      "169/168 [==============================] - 7s - loss: 0.1880 - acc: 0.9318 - val_loss: 0.5175 - val_acc: 0.7819\n",
      "Epoch 7/50\n",
      "169/168 [==============================] - 7s - loss: 0.1584 - acc: 0.9442 - val_loss: 0.4197 - val_acc: 0.8287\n",
      "Epoch 8/50\n",
      "169/168 [==============================] - 7s - loss: 0.1533 - acc: 0.9446 - val_loss: 0.4569 - val_acc: 0.8100\n",
      "Epoch 9/50\n",
      "169/168 [==============================] - 7s - loss: 0.1521 - acc: 0.9440 - val_loss: 0.5794 - val_acc: 0.6791\n",
      "Epoch 10/50\n",
      "169/168 [==============================] - 7s - loss: 0.1467 - acc: 0.9455 - val_loss: 0.5238 - val_acc: 0.8193\n",
      "Epoch 11/50\n",
      "169/168 [==============================] - 7s - loss: 0.1532 - acc: 0.9449 - val_loss: 0.5884 - val_acc: 0.6854\n",
      "Epoch 12/50\n",
      "169/168 [==============================] - 7s - loss: 0.1473 - acc: 0.9463 - val_loss: 0.4055 - val_acc: 0.8349\n",
      "Epoch 13/50\n",
      "169/168 [==============================] - 7s - loss: 0.1540 - acc: 0.9462 - val_loss: 0.4909 - val_acc: 0.7819\n",
      "Epoch 14/50\n",
      "169/168 [==============================] - 7s - loss: 0.1361 - acc: 0.9528 - val_loss: 0.3986 - val_acc: 0.8162\n",
      "Epoch 15/50\n",
      "169/168 [==============================] - 7s - loss: 0.1271 - acc: 0.9528 - val_loss: 0.4390 - val_acc: 0.8287\n",
      "Epoch 16/50\n",
      "169/168 [==============================] - 7s - loss: 0.1489 - acc: 0.9464 - val_loss: 0.4121 - val_acc: 0.7913\n",
      "Epoch 17/50\n",
      "169/168 [==============================] - 7s - loss: 0.1491 - acc: 0.9437 - val_loss: 0.4624 - val_acc: 0.7570\n",
      "Epoch 18/50\n",
      "169/168 [==============================] - 7s - loss: 0.1429 - acc: 0.9508 - val_loss: 0.4103 - val_acc: 0.8162\n",
      "Epoch 19/50\n",
      "169/168 [==============================] - 7s - loss: 0.1301 - acc: 0.9519 - val_loss: 0.3656 - val_acc: 0.8723\n",
      "Epoch 20/50\n",
      "169/168 [==============================] - 7s - loss: 0.1404 - acc: 0.9488 - val_loss: 0.4234 - val_acc: 0.8349\n",
      "Epoch 21/50\n",
      "169/168 [==============================] - 7s - loss: 0.1385 - acc: 0.9505 - val_loss: 0.4971 - val_acc: 0.7726\n",
      "Epoch 22/50\n",
      "169/168 [==============================] - 7s - loss: 0.1420 - acc: 0.9497 - val_loss: 0.3968 - val_acc: 0.8287\n",
      "Epoch 23/50\n",
      "169/168 [==============================] - 7s - loss: 0.1418 - acc: 0.9487 - val_loss: 0.3807 - val_acc: 0.8442\n",
      "Epoch 24/50\n",
      "169/168 [==============================] - 7s - loss: 0.1511 - acc: 0.9467 - val_loss: 0.3555 - val_acc: 0.8380\n",
      "Epoch 25/50\n",
      "169/168 [==============================] - 7s - loss: 0.1309 - acc: 0.9524 - val_loss: 0.3787 - val_acc: 0.8442\n",
      "Epoch 26/50\n",
      "169/168 [==============================] - 7s - loss: 0.1423 - acc: 0.9495 - val_loss: 0.3500 - val_acc: 0.8318\n",
      "Epoch 27/50\n",
      "169/168 [==============================] - 7s - loss: 0.1338 - acc: 0.9512 - val_loss: 0.3598 - val_acc: 0.8193\n",
      "Epoch 28/50\n",
      "169/168 [==============================] - 7s - loss: 0.1301 - acc: 0.9544 - val_loss: 0.3891 - val_acc: 0.8131\n",
      "Epoch 29/50\n",
      "169/168 [==============================] - 7s - loss: 0.1263 - acc: 0.9526 - val_loss: 0.3827 - val_acc: 0.8224\n",
      "Epoch 30/50\n",
      "169/168 [==============================] - 7s - loss: 0.1228 - acc: 0.9553 - val_loss: 0.3539 - val_acc: 0.8380\n",
      "Epoch 31/50\n",
      "169/168 [==============================] - 7s - loss: 0.1191 - acc: 0.9556 - val_loss: 0.3874 - val_acc: 0.8131\n",
      "Epoch 32/50\n",
      "169/168 [==============================] - 7s - loss: 0.1185 - acc: 0.9581 - val_loss: 0.3851 - val_acc: 0.8536\n",
      "Epoch 33/50\n",
      "169/168 [==============================] - 7s - loss: 0.1301 - acc: 0.9531 - val_loss: 0.4312 - val_acc: 0.7882\n",
      "Epoch 34/50\n",
      "169/168 [==============================] - 7s - loss: 0.1352 - acc: 0.9536 - val_loss: 0.3548 - val_acc: 0.8442\n",
      "Epoch 35/50\n",
      "169/168 [==============================] - 7s - loss: 0.1278 - acc: 0.9550 - val_loss: 0.4425 - val_acc: 0.7975\n",
      "Epoch 36/50\n",
      "169/168 [==============================] - 7s - loss: 0.1207 - acc: 0.9566 - val_loss: 0.3968 - val_acc: 0.8100\n",
      "Epoch 37/50\n",
      "169/168 [==============================] - 7s - loss: 0.1277 - acc: 0.9559 - val_loss: 0.4509 - val_acc: 0.8224\n",
      "Epoch 38/50\n",
      "169/168 [==============================] - 7s - loss: 0.1213 - acc: 0.9591 - val_loss: 0.4254 - val_acc: 0.8349\n",
      "Epoch 39/50\n",
      "169/168 [==============================] - 7s - loss: 0.1254 - acc: 0.9554 - val_loss: 0.3351 - val_acc: 0.8318\n",
      "Epoch 40/50\n",
      "169/168 [==============================] - 7s - loss: 0.1274 - acc: 0.9551 - val_loss: 0.4518 - val_acc: 0.7726\n",
      "Epoch 41/50\n",
      "169/168 [==============================] - 7s - loss: 0.1211 - acc: 0.9573 - val_loss: 0.3619 - val_acc: 0.8131\n",
      "Epoch 42/50\n",
      "169/168 [==============================] - 7s - loss: 0.1327 - acc: 0.9495 - val_loss: 0.4789 - val_acc: 0.7913\n",
      "Epoch 43/50\n",
      "169/168 [==============================] - 7s - loss: 0.1394 - acc: 0.9502 - val_loss: 0.4021 - val_acc: 0.8287\n",
      "Epoch 44/50\n",
      "169/168 [==============================] - 7s - loss: 0.1285 - acc: 0.9566 - val_loss: 0.4268 - val_acc: 0.8224\n",
      "Epoch 45/50\n",
      "169/168 [==============================] - 7s - loss: 0.1137 - acc: 0.9573 - val_loss: 0.3689 - val_acc: 0.8411\n",
      "Epoch 46/50\n",
      "169/168 [==============================] - 8s - loss: 0.1198 - acc: 0.9577 - val_loss: 0.3663 - val_acc: 0.8349\n",
      "Epoch 47/50\n",
      "169/168 [==============================] - 7s - loss: 0.1223 - acc: 0.9561 - val_loss: 0.4278 - val_acc: 0.8255\n",
      "Epoch 48/50\n",
      "169/168 [==============================] - 7s - loss: 0.1136 - acc: 0.9563 - val_loss: 0.5160 - val_acc: 0.7570\n",
      "Epoch 49/50\n",
      "169/168 [==============================] - 7s - loss: 0.1260 - acc: 0.9562 - val_loss: 0.3763 - val_acc: 0.8411\n",
      "Epoch 50/50\n",
      "169/168 [==============================] - 7s - loss: 0.1162 - acc: 0.9604 - val_loss: 0.3470 - val_acc: 0.8723\n"
     ]
    }
   ],
   "source": [
    "best = BestModel(ids)\n",
    "best.train(X_train_pseudo, y_train_pseudo, label_maker.trainer.X_val, label_maker.trainer.y_val, epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_68 (Conv2D)           (None, 73, 73, 64)        1792      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_63 (MaxPooling (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_49 (Dropout)         (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_69 (Conv2D)           (None, 34, 34, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_64 (MaxPooling (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_50 (Dropout)         (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_70 (Conv2D)           (None, 15, 15, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_65 (MaxPooling (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_51 (Dropout)         (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_71 (Conv2D)           (None, 5, 5, 64)          73792     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_66 (MaxPooling (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_52 (Dropout)         (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_21 (Flatten)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_53 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_54 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 560,450\n",
      "Trainable params: 560,450\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd # Used to open CSV files \n",
    "import numpy as np # Used for matrix operations\n",
    "import cv2 # Used for image augmentation\n",
    "from matplotlib import pyplot as plt\n",
    "np.random.seed(666)\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "def get_more_images(imgs):\n",
    "    \n",
    "    more_images = []\n",
    "    vert_flip_imgs = []\n",
    "    hori_flip_imgs = []\n",
    "      \n",
    "    for i in range(0,imgs.shape[0]):\n",
    "        a=imgs[i,:,:,0]\n",
    "        b=imgs[i,:,:,1]\n",
    "        c=imgs[i,:,:,2]\n",
    "        \n",
    "        av=cv2.flip(a,1)\n",
    "        ah=cv2.flip(a,0)\n",
    "        bv=cv2.flip(b,1)\n",
    "        bh=cv2.flip(b,0)\n",
    "        cv=cv2.flip(c,1)\n",
    "        ch=cv2.flip(c,0)\n",
    "        \n",
    "        vert_flip_imgs.append(np.dstack((av, bv, cv)))\n",
    "        hori_flip_imgs.append(np.dstack((ah, bh, ch)))\n",
    "      \n",
    "    v = np.array(vert_flip_imgs)\n",
    "    h = np.array(hori_flip_imgs)\n",
    "       \n",
    "    more_images = np.concatenate((imgs,v,h))\n",
    "    \n",
    "    return more_images\n",
    "\n",
    "Xtr_more = get_more_images(X_train_pseudo) \n",
    "Ytr_more = np.concatenate((y_train_pseudo,y_train_pseudo,y_train_pseudo))\n",
    "\n",
    "def getModel():\n",
    "    #Build keras model\n",
    "    \n",
    "    model=Sequential()\n",
    "    \n",
    "    # CNN 1\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3),activation='relu', input_shape=(75, 75, 3)))\n",
    "    model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # CNN 2\n",
    "    model.add(Conv2D(128, kernel_size=(3, 3), activation='relu' ))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # CNN 3\n",
    "    model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    #CNN 4\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    # You must flatten the data for the dense layers\n",
    "    model.add(Flatten())\n",
    "\n",
    "    #Dense 1\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    #Dense 2\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # Output \n",
    "    model.add(Dense(2, activation=\"softmax\"))\n",
    "\n",
    "    optimizer = Adam(lr=0.001, decay=0.0)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = getModel()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16167, 75, 75, 3)\n",
      "(16167, 2)\n"
     ]
    }
   ],
   "source": [
    "print(Xtr_more.shape)\n",
    "print(Ytr_more.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12125 samples, validate on 4042 samples\n",
      "Epoch 1/50\n",
      "12125/12125 [==============================] - 16s - loss: 0.3137 - acc: 0.8628 - val_loss: 0.0957 - val_acc: 0.9753\n",
      "Epoch 2/50\n",
      "12125/12125 [==============================] - 11s - loss: 0.2142 - acc: 0.9169 - val_loss: 0.1274 - val_acc: 0.9854\n",
      "Epoch 3/50\n",
      "12125/12125 [==============================] - 12s - loss: 0.1715 - acc: 0.9353 - val_loss: 0.0710 - val_acc: 0.9904\n",
      "Epoch 4/50\n",
      "12125/12125 [==============================] - 12s - loss: 0.1678 - acc: 0.9372 - val_loss: 0.0551 - val_acc: 0.9916\n",
      "Epoch 5/50\n",
      "12125/12125 [==============================] - 12s - loss: 0.1540 - acc: 0.9404 - val_loss: 0.0417 - val_acc: 0.9911\n",
      "Epoch 6/50\n",
      "12125/12125 [==============================] - 11s - loss: 0.1452 - acc: 0.9461 - val_loss: 0.0308 - val_acc: 0.9970\n",
      "Epoch 7/50\n",
      "12125/12125 [==============================] - 11s - loss: 0.1428 - acc: 0.9470 - val_loss: 0.0602 - val_acc: 0.9951\n",
      "Epoch 8/50\n",
      "12125/12125 [==============================] - 11s - loss: 0.1404 - acc: 0.9463 - val_loss: 0.0428 - val_acc: 0.9904\n",
      "Epoch 9/50\n",
      "12125/12125 [==============================] - 11s - loss: 0.1300 - acc: 0.9505 - val_loss: 0.0525 - val_acc: 0.9859\n",
      "Epoch 10/50\n",
      "12125/12125 [==============================] - 11s - loss: 0.1381 - acc: 0.9475 - val_loss: 0.0381 - val_acc: 0.9931\n",
      "Epoch 11/50\n",
      "12125/12125 [==============================] - 12s - loss: 0.1260 - acc: 0.9518 - val_loss: 0.0159 - val_acc: 0.9980\n",
      "Epoch 12/50\n",
      "12125/12125 [==============================] - 12s - loss: 0.1294 - acc: 0.9502 - val_loss: 0.0221 - val_acc: 0.9983\n",
      "Epoch 13/50\n",
      "12125/12125 [==============================] - 12s - loss: 0.1257 - acc: 0.9524 - val_loss: 0.1176 - val_acc: 0.9599\n",
      "Epoch 14/50\n",
      "12125/12125 [==============================] - 12s - loss: 0.1301 - acc: 0.9494 - val_loss: 0.0324 - val_acc: 0.9921\n",
      "Epoch 15/50\n",
      "12125/12125 [==============================] - 12s - loss: 0.1215 - acc: 0.9529 - val_loss: 0.0281 - val_acc: 0.9970\n",
      "Epoch 16/50\n",
      "12125/12125 [==============================] - 12s - loss: 0.1234 - acc: 0.9535 - val_loss: 0.0358 - val_acc: 0.9953\n",
      "Epoch 17/50\n",
      "12125/12125 [==============================] - 12s - loss: 0.1247 - acc: 0.9537 - val_loss: 0.0356 - val_acc: 0.9953\n",
      "Epoch 18/50\n",
      "12125/12125 [==============================] - 12s - loss: 0.1169 - acc: 0.9546 - val_loss: 0.0491 - val_acc: 0.9896\n",
      "Epoch 19/50\n",
      "12064/12125 [============================>.] - ETA: 0s - loss: 0.1151 - acc: 0.9557\n",
      "Epoch 00018: reducing learning rate to 0.00010000000474974513.\n",
      "12125/12125 [==============================] - 12s - loss: 0.1162 - acc: 0.9555 - val_loss: 0.0319 - val_acc: 0.9933\n",
      "Epoch 20/50\n",
      "12125/12125 [==============================] - 12s - loss: 0.0938 - acc: 0.9638 - val_loss: 0.0240 - val_acc: 0.9975\n",
      "Epoch 21/50\n",
      "12125/12125 [==============================] - 11s - loss: 0.0883 - acc: 0.9645 - val_loss: 0.0227 - val_acc: 0.9965\n",
      "Epoch 22/50\n",
      "12125/12125 [==============================] - 11s - loss: 0.0872 - acc: 0.9656 - val_loss: 0.0220 - val_acc: 0.9973\n",
      "dict_keys(['val_loss', 'val_acc', 'lr', 'loss', 'acc'])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'y_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-104-8712be6c7161>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'.mdl_wts.hdf5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Train score:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Train accuracy:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_train' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKaWUR0NBKaWUR0NBKaWU5/8HXlRHR81YIoAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1eeecdda128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save = ModelCheckpoint('.mdl_wts.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=1, epsilon=1e-4, mode='min')\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# Let's view progress \n",
    "history = model.fit(Xtr_more, Ytr_more, batch_size=batch_size, epochs=50, verbose=1, callbacks=[earlyStopping, mcp_save, reduce_lr_loss], validation_split=0.25)\n",
    "\n",
    "print(history.history.keys())\n",
    "#\n",
    "fig = plt.figure()\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='lower left')\n",
    "#\n",
    "fig.savefig('performance.png')\n",
    "#---------------------------------------------------------------------------------------\n",
    "\n",
    "model.load_weights(filepath = '.mdl_wts.hdf5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1216/1283 [===========================>..] - ETA: 0sTrain score: 0.334075201719\n",
      "Train accuracy: 0.866718628215\n",
      "         id  is_iceberg\n",
      "0  5941774d      0.7115\n",
      "1  4023181e      0.0019\n",
      "2  b20200e4      0.0000\n",
      "3  e7f018bb      1.0000\n",
      "4  4371c8c3      0.0000\n",
      "5  a8d9b1fd      0.0208\n",
      "6  29e7727e      0.2366\n",
      "7  92a51ffb      1.0000\n",
      "8  c769ac97      0.0001\n",
      "9  aee0547d      0.0000\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(label_maker.trainer.X_train, label_maker.trainer.y_train, verbose=1)\n",
    "print('Train score:', score[0])\n",
    "print('Train accuracy:', score[1])\n",
    "\n",
    "# df_test = pd.read_json('_RawData/test.json/data/processed/test.json')\n",
    "# df_test.inc_angle = df_test.inc_angle.replace('na',0)\n",
    "# Xtest = (get_scaled_imgs(df_test))\n",
    "\n",
    "pred_test = model.predict(label_maker.trainer.X_test)\n",
    "answers = pred_test[:, 1].round(4)\n",
    "answers[answers == 1.] = 0.9999\n",
    "\n",
    "submission = pd.DataFrame({'id': ids, 'is_iceberg': pred_test[:, 1].round(4) })\n",
    "print(submission.head(10))\n",
    "\n",
    "\n",
    "\n",
    "submission.to_csv('submission.csv', float_format='%g', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = pred_test[:, 1].round(4)\n",
    "answers[answers == 1.] = 0.9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=float32)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers[answers == 1.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_maker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# trainRunner.train(32, 1, False)\n",
    "# trainRunner.train_full(32, 1, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainRunner.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.58547407,  0.41452599],\n",
       "        [ 0.57440114,  0.42559892],\n",
       "        [ 0.60632485,  0.39367515],\n",
       "        ..., \n",
       "        [ 0.59274668,  0.4072533 ],\n",
       "        [ 0.59482783,  0.40517214],\n",
       "        [ 0.62089276,  0.37910724]], dtype=float32),\n",
       " array([[ 0.51180393,  0.48819605],\n",
       "        [ 0.51168621,  0.48831385],\n",
       "        [ 0.51210481,  0.48789516],\n",
       "        ..., \n",
       "        [ 0.5117113 ,  0.4882887 ],\n",
       "        [ 0.51172554,  0.48827448],\n",
       "        [ 0.51198667,  0.48801333]], dtype=float32),\n",
       " array([[ 0.50135714,  0.49864292],\n",
       "        [ 0.50138915,  0.49861091],\n",
       "        [ 0.96230453,  0.03769542],\n",
       "        ..., \n",
       "        [ 0.50137621,  0.49862379],\n",
       "        [ 0.50154185,  0.49845815],\n",
       "        [ 0.50160444,  0.49839562]], dtype=float32),\n",
       " array([[ 0.24273774,  0.75726223],\n",
       "        [ 0.35269514,  0.64730477],\n",
       "        [ 0.88178951,  0.11821055],\n",
       "        ..., \n",
       "        [ 0.28946409,  0.71053594],\n",
       "        [ 0.28727075,  0.71272928],\n",
       "        [ 0.50449407,  0.4955059 ]], dtype=float32)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainRunner.predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
