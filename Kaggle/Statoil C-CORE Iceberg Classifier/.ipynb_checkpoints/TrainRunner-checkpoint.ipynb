{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pseudo labelling and splitting data by angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import pdb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras import backend as K\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import scipy\n",
    "from scipy import misc, ndimage\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "from scipy.ndimage import imread\n",
    "import helpers\n",
    "from models import DaveModel, DaveVGG, DaveVGG19, SimpleModel, LeNetModel\n",
    "from trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_json(\"_RawData/train.json/data/processed/train.json\")\n",
    "test = pd.read_json(\"_RawData/test.json/data/processed/test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = helpers.get_images(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = to_categorical(train.is_iceberg.values,num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xtr, Xv, ytr, yv = train_test_split(X, y, shuffle=False, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the model and compile\n",
    "model = DaveModel(Xtr, ytr, Xv, yv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size: 32\n",
      "Epochs: 50\n",
      "Epoch 1/50\n",
      "41/40 [==============================] - 7s - loss: 0.5864 - acc: 0.6974 - val_loss: 0.6622 - val_acc: 0.6573\n",
      "Epoch 2/50\n",
      "41/40 [==============================] - 1s - loss: 0.5113 - acc: 0.7484 - val_loss: 0.6435 - val_acc: 0.6573\n",
      "Epoch 3/50\n",
      "41/40 [==============================] - 1s - loss: 0.4713 - acc: 0.7556 - val_loss: 0.6317 - val_acc: 0.6573\n",
      "Epoch 4/50\n",
      "41/40 [==============================] - 1s - loss: 0.4273 - acc: 0.7895 - val_loss: 0.6288 - val_acc: 0.6573\n",
      "Epoch 5/50\n",
      "41/40 [==============================] - 1s - loss: 0.3972 - acc: 0.8101 - val_loss: 0.6285 - val_acc: 0.6573\n",
      "Epoch 6/50\n",
      "41/40 [==============================] - 1s - loss: 0.3921 - acc: 0.8121 - val_loss: 0.6737 - val_acc: 0.6573\n",
      "Epoch 7/50\n",
      "41/40 [==============================] - 1s - loss: 0.3898 - acc: 0.7904 - val_loss: 0.6496 - val_acc: 0.6573\n",
      "Epoch 8/50\n",
      "41/40 [==============================] - 1s - loss: 0.3572 - acc: 0.8170 - val_loss: 0.7837 - val_acc: 0.6542\n",
      "Epoch 9/50\n",
      "41/40 [==============================] - 1s - loss: 0.3371 - acc: 0.8383 - val_loss: 0.7957 - val_acc: 0.6417\n",
      "Epoch 10/50\n",
      "41/40 [==============================] - 1s - loss: 0.3310 - acc: 0.8414 - val_loss: 0.7424 - val_acc: 0.6573\n",
      "Epoch 11/50\n",
      "41/40 [==============================] - 1s - loss: 0.3460 - acc: 0.8368 - val_loss: 0.7990 - val_acc: 0.6573\n",
      "Epoch 12/50\n",
      "41/40 [==============================] - 1s - loss: 0.3252 - acc: 0.8345 - val_loss: 0.6277 - val_acc: 0.7196\n",
      "Epoch 13/50\n",
      "41/40 [==============================] - 1s - loss: 0.3297 - acc: 0.8365 - val_loss: 0.5104 - val_acc: 0.7539\n",
      "Epoch 14/50\n",
      "41/40 [==============================] - 1s - loss: 0.3225 - acc: 0.8513 - val_loss: 0.3961 - val_acc: 0.8069\n",
      "Epoch 15/50\n",
      "41/40 [==============================] - 1s - loss: 0.3138 - acc: 0.8513 - val_loss: 0.3948 - val_acc: 0.8318\n",
      "Epoch 16/50\n",
      "41/40 [==============================] - 1s - loss: 0.3076 - acc: 0.8415 - val_loss: 0.3220 - val_acc: 0.8505\n",
      "Epoch 17/50\n",
      "41/40 [==============================] - 1s - loss: 0.3307 - acc: 0.8372 - val_loss: 0.3454 - val_acc: 0.8505\n",
      "Epoch 18/50\n",
      "41/40 [==============================] - 1s - loss: 0.3134 - acc: 0.8410 - val_loss: 0.3287 - val_acc: 0.8224\n",
      "Epoch 19/50\n",
      "41/40 [==============================] - 1s - loss: 0.3050 - acc: 0.8456 - val_loss: 0.3049 - val_acc: 0.8567\n",
      "Epoch 20/50\n",
      "41/40 [==============================] - 1s - loss: 0.2989 - acc: 0.8571 - val_loss: 0.3084 - val_acc: 0.8411\n",
      "Epoch 21/50\n",
      "41/40 [==============================] - 1s - loss: 0.2820 - acc: 0.8848 - val_loss: 0.3320 - val_acc: 0.8037\n",
      "Epoch 22/50\n",
      "41/40 [==============================] - 1s - loss: 0.2905 - acc: 0.8734 - val_loss: 0.3533 - val_acc: 0.8037\n",
      "Epoch 23/50\n",
      "41/40 [==============================] - 1s - loss: 0.2950 - acc: 0.8559 - val_loss: 0.3210 - val_acc: 0.8318\n",
      "Epoch 24/50\n",
      "41/40 [==============================] - 1s - loss: 0.2688 - acc: 0.8734 - val_loss: 0.3207 - val_acc: 0.8598\n",
      "Epoch 25/50\n",
      "41/40 [==============================] - 1s - loss: 0.2743 - acc: 0.8635 - val_loss: 0.3815 - val_acc: 0.7539\n",
      "Epoch 26/50\n",
      "41/40 [==============================] - 1s - loss: 0.2950 - acc: 0.8670 - val_loss: 0.3048 - val_acc: 0.8380\n",
      "Epoch 27/50\n",
      "41/40 [==============================] - 1s - loss: 0.2798 - acc: 0.8665 - val_loss: 0.3157 - val_acc: 0.8349\n",
      "Epoch 28/50\n",
      "41/40 [==============================] - 1s - loss: 0.2781 - acc: 0.8719 - val_loss: 0.3260 - val_acc: 0.8006\n",
      "Epoch 29/50\n",
      "41/40 [==============================] - 1s - loss: 0.2884 - acc: 0.8594 - val_loss: 0.3030 - val_acc: 0.8349\n",
      "Epoch 30/50\n",
      "41/40 [==============================] - 1s - loss: 0.2815 - acc: 0.8586 - val_loss: 0.3661 - val_acc: 0.8536\n",
      "Epoch 31/50\n",
      "41/40 [==============================] - 1s - loss: 0.2892 - acc: 0.8616 - val_loss: 0.3242 - val_acc: 0.8069\n",
      "Epoch 32/50\n",
      "41/40 [==============================] - 1s - loss: 0.2862 - acc: 0.8613 - val_loss: 0.3428 - val_acc: 0.8598\n",
      "Epoch 33/50\n",
      "41/40 [==============================] - 1s - loss: 0.2958 - acc: 0.8525 - val_loss: 0.3086 - val_acc: 0.8380\n",
      "Epoch 34/50\n",
      "41/40 [==============================] - 1s - loss: 0.2682 - acc: 0.8731 - val_loss: 0.2988 - val_acc: 0.8505\n",
      "Epoch 35/50\n",
      "41/40 [==============================] - 1s - loss: 0.2740 - acc: 0.8731 - val_loss: 0.3075 - val_acc: 0.8380\n",
      "Epoch 36/50\n",
      "41/40 [==============================] - 1s - loss: 0.2910 - acc: 0.8522 - val_loss: 0.2864 - val_acc: 0.8629\n",
      "Epoch 37/50\n",
      "41/40 [==============================] - 1s - loss: 0.2705 - acc: 0.8810 - val_loss: 0.3436 - val_acc: 0.8131\n",
      "Epoch 38/50\n",
      "41/40 [==============================] - 1s - loss: 0.2675 - acc: 0.8723 - val_loss: 0.3156 - val_acc: 0.8100\n",
      "Epoch 39/50\n",
      "41/40 [==============================] - 1s - loss: 0.2793 - acc: 0.8708 - val_loss: 0.2991 - val_acc: 0.8474\n",
      "Epoch 40/50\n",
      "41/40 [==============================] - 1s - loss: 0.2528 - acc: 0.8879 - val_loss: 0.2907 - val_acc: 0.8598\n",
      "Epoch 41/50\n",
      "41/40 [==============================] - 1s - loss: 0.2536 - acc: 0.8795 - val_loss: 0.3141 - val_acc: 0.8255\n",
      "Epoch 42/50\n",
      "41/40 [==============================] - 1s - loss: 0.2557 - acc: 0.8765 - val_loss: 0.3051 - val_acc: 0.8723\n",
      "Epoch 43/50\n",
      "41/40 [==============================] - 1s - loss: 0.2706 - acc: 0.8723 - val_loss: 0.3091 - val_acc: 0.8567\n",
      "Epoch 44/50\n",
      "41/40 [==============================] - 1s - loss: 0.2651 - acc: 0.8738 - val_loss: 0.3292 - val_acc: 0.8536\n",
      "Epoch 45/50\n",
      "41/40 [==============================] - 1s - loss: 0.2487 - acc: 0.8807 - val_loss: 0.3020 - val_acc: 0.8910\n",
      "Epoch 46/50\n",
      "41/40 [==============================] - 1s - loss: 0.2528 - acc: 0.8780 - val_loss: 0.2897 - val_acc: 0.8754\n",
      "Epoch 47/50\n",
      "41/40 [==============================] - 1s - loss: 0.2419 - acc: 0.8864 - val_loss: 0.2933 - val_acc: 0.8723\n",
      "Epoch 48/50\n",
      "41/40 [==============================] - 1s - loss: 0.2478 - acc: 0.8795 - val_loss: 0.3169 - val_acc: 0.8318\n",
      "Epoch 49/50\n",
      "41/40 [==============================] - 1s - loss: 0.2578 - acc: 0.8712 - val_loss: 0.3053 - val_acc: 0.8380\n",
      "Epoch 50/50\n",
      "41/40 [==============================] - 1s - loss: 0.2479 - acc: 0.8848 - val_loss: 0.4184 - val_acc: 0.7788\n"
     ]
    }
   ],
   "source": [
    "model.train(32, 50, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg = DaveVGG(Xtr, ytr, Xv, yv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size: 32\n",
      "Epochs: 50\n",
      "Epoch 1/50\n",
      "41/40 [==============================] - 8s - loss: 0.6934 - acc: 0.4803 - val_loss: 0.6938 - val_acc: 0.4891\n",
      "Epoch 2/50\n",
      "41/40 [==============================] - 5s - loss: 0.6898 - acc: 0.5250 - val_loss: 0.6914 - val_acc: 0.6573\n",
      "Epoch 3/50\n",
      "41/40 [==============================] - 5s - loss: 0.6928 - acc: 0.5131 - val_loss: 0.6859 - val_acc: 0.6573\n",
      "Epoch 4/50\n",
      "41/40 [==============================] - 5s - loss: 0.6913 - acc: 0.5136 - val_loss: 0.6974 - val_acc: 0.3458\n",
      "Epoch 5/50\n",
      "41/40 [==============================] - 5s - loss: 0.6907 - acc: 0.5509 - val_loss: 0.6939 - val_acc: 0.4611\n",
      "Epoch 6/50\n",
      "41/40 [==============================] - 5s - loss: 0.6782 - acc: 0.5387 - val_loss: 0.6685 - val_acc: 0.6573\n",
      "Epoch 7/50\n",
      "41/40 [==============================] - 5s - loss: 0.6746 - acc: 0.5125 - val_loss: 0.7023 - val_acc: 0.3551\n",
      "Epoch 8/50\n",
      "41/40 [==============================] - 5s - loss: 0.6729 - acc: 0.5935 - val_loss: 0.7551 - val_acc: 0.3769\n",
      "Epoch 9/50\n",
      "41/40 [==============================] - 5s - loss: 0.6822 - acc: 0.5734 - val_loss: 0.7401 - val_acc: 0.3925\n",
      "Epoch 10/50\n",
      "41/40 [==============================] - 5s - loss: 0.6601 - acc: 0.6467 - val_loss: 0.6183 - val_acc: 0.6293\n",
      "Epoch 11/50\n",
      "41/40 [==============================] - 5s - loss: 0.6200 - acc: 0.6550 - val_loss: 0.6484 - val_acc: 0.5576\n",
      "Epoch 12/50\n",
      "41/40 [==============================] - 5s - loss: 0.6288 - acc: 0.6432 - val_loss: 0.6281 - val_acc: 0.6106\n",
      "Epoch 13/50\n",
      "41/40 [==============================] - 5s - loss: 0.5927 - acc: 0.6790 - val_loss: 0.6615 - val_acc: 0.5607\n",
      "Epoch 14/50\n",
      "41/40 [==============================] - 5s - loss: 0.5833 - acc: 0.6923 - val_loss: 0.7252 - val_acc: 0.5047\n",
      "Epoch 15/50\n",
      "41/40 [==============================] - 5s - loss: 0.5938 - acc: 0.6702 - val_loss: 0.6499 - val_acc: 0.5888\n",
      "Epoch 16/50\n",
      "41/40 [==============================] - 5s - loss: 0.5666 - acc: 0.7087 - val_loss: 0.6533 - val_acc: 0.6012\n",
      "Epoch 17/50\n",
      "41/40 [==============================] - 5s - loss: 0.5539 - acc: 0.7201 - val_loss: 0.7175 - val_acc: 0.5389\n",
      "Epoch 18/50\n",
      "41/40 [==============================] - 5s - loss: 0.5499 - acc: 0.7285 - val_loss: 0.5794 - val_acc: 0.6449\n",
      "Epoch 19/50\n",
      "41/40 [==============================] - 5s - loss: 0.5652 - acc: 0.7084 - val_loss: 0.5799 - val_acc: 0.6168\n",
      "Epoch 20/50\n",
      "41/40 [==============================] - 5s - loss: 0.5500 - acc: 0.6974 - val_loss: 0.6268 - val_acc: 0.6075\n",
      "Epoch 21/50\n",
      "41/40 [==============================] - 5s - loss: 0.5423 - acc: 0.7129 - val_loss: 0.6567 - val_acc: 0.5514\n",
      "Epoch 22/50\n",
      "41/40 [==============================] - 5s - loss: 0.5553 - acc: 0.7129 - val_loss: 0.6204 - val_acc: 0.6293\n",
      "Epoch 23/50\n",
      "41/40 [==============================] - 5s - loss: 0.5322 - acc: 0.7239 - val_loss: 0.6609 - val_acc: 0.5888\n",
      "Epoch 24/50\n",
      "41/40 [==============================] - 5s - loss: 0.5295 - acc: 0.7289 - val_loss: 0.6117 - val_acc: 0.6293\n",
      "Epoch 25/50\n",
      "41/40 [==============================] - 5s - loss: 0.5461 - acc: 0.7183 - val_loss: 0.6311 - val_acc: 0.5888\n",
      "Epoch 26/50\n",
      "41/40 [==============================] - 5s - loss: 0.5337 - acc: 0.7057 - val_loss: 0.5561 - val_acc: 0.6386\n",
      "Epoch 27/50\n",
      "41/40 [==============================] - 5s - loss: 0.5135 - acc: 0.7297 - val_loss: 0.5188 - val_acc: 0.7134\n",
      "Epoch 28/50\n",
      "41/40 [==============================] - 5s - loss: 0.4893 - acc: 0.7483 - val_loss: 0.6756 - val_acc: 0.5919\n",
      "Epoch 29/50\n",
      "41/40 [==============================] - 5s - loss: 0.4731 - acc: 0.7613 - val_loss: 0.4665 - val_acc: 0.7975\n",
      "Epoch 30/50\n",
      "41/40 [==============================] - 5s - loss: 0.4577 - acc: 0.7689 - val_loss: 0.5927 - val_acc: 0.6729\n",
      "Epoch 31/50\n",
      "41/40 [==============================] - 5s - loss: 0.4319 - acc: 0.7842 - val_loss: 0.4603 - val_acc: 0.7850\n",
      "Epoch 32/50\n",
      "41/40 [==============================] - 5s - loss: 0.4424 - acc: 0.7926 - val_loss: 0.4636 - val_acc: 0.7570\n",
      "Epoch 33/50\n",
      "41/40 [==============================] - 5s - loss: 0.4157 - acc: 0.7979 - val_loss: 0.5166 - val_acc: 0.7196\n",
      "Epoch 34/50\n",
      "41/40 [==============================] - 5s - loss: 0.4112 - acc: 0.8040 - val_loss: 0.4364 - val_acc: 0.7975\n",
      "Epoch 35/50\n",
      "41/40 [==============================] - 5s - loss: 0.3960 - acc: 0.8185 - val_loss: 0.4035 - val_acc: 0.8318\n",
      "Epoch 36/50\n",
      "41/40 [==============================] - 5s - loss: 0.4184 - acc: 0.7938 - val_loss: 0.4048 - val_acc: 0.8318\n",
      "Epoch 37/50\n",
      "41/40 [==============================] - 5s - loss: 0.3946 - acc: 0.8151 - val_loss: 0.3970 - val_acc: 0.8287\n",
      "Epoch 38/50\n",
      "41/40 [==============================] - 5s - loss: 0.3997 - acc: 0.8101 - val_loss: 0.4110 - val_acc: 0.8411\n",
      "Epoch 39/50\n",
      "41/40 [==============================] - 5s - loss: 0.3535 - acc: 0.8475 - val_loss: 0.4052 - val_acc: 0.8567\n",
      "Epoch 40/50\n",
      "41/40 [==============================] - 5s - loss: 0.3637 - acc: 0.8464 - val_loss: 0.3679 - val_acc: 0.8411\n",
      "Epoch 41/50\n",
      "41/40 [==============================] - 5s - loss: 0.3411 - acc: 0.8437 - val_loss: 0.4297 - val_acc: 0.8069\n",
      "Epoch 42/50\n",
      "41/40 [==============================] - 5s - loss: 0.3824 - acc: 0.8277 - val_loss: 0.4754 - val_acc: 0.7539\n",
      "Epoch 43/50\n",
      "41/40 [==============================] - 5s - loss: 0.3231 - acc: 0.8536 - val_loss: 0.3594 - val_acc: 0.8442\n",
      "Epoch 44/50\n",
      "41/40 [==============================] - 5s - loss: 0.3566 - acc: 0.8365 - val_loss: 0.3388 - val_acc: 0.8318\n",
      "Epoch 45/50\n",
      "41/40 [==============================] - 5s - loss: 0.3452 - acc: 0.8255 - val_loss: 0.3107 - val_acc: 0.8598\n",
      "Epoch 46/50\n",
      "41/40 [==============================] - 5s - loss: 0.3463 - acc: 0.8429 - val_loss: 0.3761 - val_acc: 0.8474\n",
      "Epoch 47/50\n",
      "41/40 [==============================] - 5s - loss: 0.3308 - acc: 0.8528 - val_loss: 0.3622 - val_acc: 0.8598\n",
      "Epoch 48/50\n",
      "41/40 [==============================] - 5s - loss: 0.3318 - acc: 0.8514 - val_loss: 0.3584 - val_acc: 0.8349\n",
      "Epoch 49/50\n",
      "41/40 [==============================] - 5s - loss: 0.4175 - acc: 0.7983 - val_loss: 0.3846 - val_acc: 0.8567\n",
      "Epoch 50/50\n",
      "41/40 [==============================] - 5s - loss: 0.3336 - acc: 0.8429 - val_loss: 0.4003 - val_acc: 0.8162\n"
     ]
    }
   ],
   "source": [
    "vgg.train(32, 50, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n41AwM7OMQ8HMzDL/D9m5r9mkFcuyAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1b59ad2dc88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlFKzAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1b670335eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vgg.plot_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  3.90024602e-01,   6.09975457e-01],\n",
       "       [  9.93215561e-01,   6.78448053e-03],\n",
       "       [  1.00000000e+00,   7.81740126e-27],\n",
       "       ..., \n",
       "       [  9.22401726e-01,   7.75983259e-02],\n",
       "       [  9.18363966e-03,   9.90816355e-01],\n",
       "       [  1.00000000e+00,   4.76142665e-14]], dtype=float32)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = helpers.get_images(test)\n",
    "predict = vgg.predict(X_test)\n",
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple = SimpleModel(Xtr, ytr, Xv, yv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lambda_2 (Lambda)            (None, 75, 75, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 73, 73, 512)       14336     \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_2 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 1026      \n",
      "=================================================================\n",
      "Total params: 278,018\n",
      "Trainable params: 278,018\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "simple.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size: 32\n",
      "Epochs: 50\n",
      "Epoch 1/50\n",
      "41/40 [==============================] - 2s - loss: 0.6852 - acc: 0.5642 - val_loss: 0.6950 - val_acc: 0.4829\n",
      "Epoch 2/50\n",
      "41/40 [==============================] - 1s - loss: 0.6696 - acc: 0.6283 - val_loss: 0.6732 - val_acc: 0.5639\n",
      "Epoch 3/50\n",
      "41/40 [==============================] - 1s - loss: 0.6565 - acc: 0.6344 - val_loss: 0.6783 - val_acc: 0.5202\n",
      "Epoch 4/50\n",
      "41/40 [==============================] - 1s - loss: 0.6458 - acc: 0.6493 - val_loss: 0.6668 - val_acc: 0.5639\n",
      "Epoch 5/50\n",
      "41/40 [==============================] - 1s - loss: 0.6383 - acc: 0.6658 - val_loss: 0.6581 - val_acc: 0.5826\n",
      "Epoch 6/50\n",
      "41/40 [==============================] - 1s - loss: 0.6160 - acc: 0.6774 - val_loss: 0.6355 - val_acc: 0.6386\n",
      "Epoch 7/50\n",
      "41/40 [==============================] - 1s - loss: 0.6183 - acc: 0.6688 - val_loss: 0.6280 - val_acc: 0.6106\n",
      "Epoch 8/50\n",
      "41/40 [==============================] - 1s - loss: 0.6063 - acc: 0.6969 - val_loss: 0.6754 - val_acc: 0.5794\n",
      "Epoch 9/50\n",
      "41/40 [==============================] - 1s - loss: 0.6164 - acc: 0.6711 - val_loss: 0.6851 - val_acc: 0.5732\n",
      "Epoch 10/50\n",
      "41/40 [==============================] - 1s - loss: 0.6025 - acc: 0.6867 - val_loss: 0.6417 - val_acc: 0.6012\n",
      "Epoch 11/50\n",
      "41/40 [==============================] - 1s - loss: 0.5915 - acc: 0.6995 - val_loss: 0.6432 - val_acc: 0.6012\n",
      "Epoch 12/50\n",
      "41/40 [==============================] - 1s - loss: 0.5879 - acc: 0.6911 - val_loss: 0.6289 - val_acc: 0.6262\n",
      "Epoch 13/50\n",
      "41/40 [==============================] - 1s - loss: 0.5853 - acc: 0.7056 - val_loss: 0.6075 - val_acc: 0.6231\n",
      "Epoch 14/50\n",
      "41/40 [==============================] - 1s - loss: 0.5869 - acc: 0.6911 - val_loss: 0.6374 - val_acc: 0.6075\n",
      "Epoch 15/50\n",
      "41/40 [==============================] - 1s - loss: 0.5831 - acc: 0.7045 - val_loss: 0.6419 - val_acc: 0.6262\n",
      "Epoch 16/50\n",
      "41/40 [==============================] - 1s - loss: 0.5800 - acc: 0.7129 - val_loss: 0.6288 - val_acc: 0.6137\n",
      "Epoch 17/50\n",
      "41/40 [==============================] - 1s - loss: 0.5746 - acc: 0.7171 - val_loss: 0.6093 - val_acc: 0.6231\n",
      "Epoch 18/50\n",
      "41/40 [==============================] - 1s - loss: 0.5789 - acc: 0.7152 - val_loss: 0.5977 - val_acc: 0.6386\n",
      "Epoch 19/50\n",
      "41/40 [==============================] - 1s - loss: 0.5754 - acc: 0.7175 - val_loss: 0.6506 - val_acc: 0.6355\n",
      "Epoch 20/50\n",
      "41/40 [==============================] - 1s - loss: 0.5733 - acc: 0.7064 - val_loss: 0.6399 - val_acc: 0.6262\n",
      "Epoch 21/50\n",
      "41/40 [==============================] - 1s - loss: 0.5701 - acc: 0.7102 - val_loss: 0.6156 - val_acc: 0.6199\n",
      "Epoch 22/50\n",
      "41/40 [==============================] - 1s - loss: 0.5713 - acc: 0.7137 - val_loss: 0.5864 - val_acc: 0.6324\n",
      "Epoch 23/50\n",
      "41/40 [==============================] - 1s - loss: 0.5714 - acc: 0.7106 - val_loss: 0.6056 - val_acc: 0.6262\n",
      "Epoch 24/50\n",
      "41/40 [==============================] - 1s - loss: 0.5685 - acc: 0.7198 - val_loss: 0.6177 - val_acc: 0.6262\n",
      "Epoch 25/50\n",
      "41/40 [==============================] - 1s - loss: 0.5616 - acc: 0.7206 - val_loss: 0.6165 - val_acc: 0.6480\n",
      "Epoch 26/50\n",
      "41/40 [==============================] - 1s - loss: 0.5708 - acc: 0.7099 - val_loss: 0.6285 - val_acc: 0.6199\n",
      "Epoch 27/50\n",
      "41/40 [==============================] - 1s - loss: 0.5739 - acc: 0.7221 - val_loss: 0.5916 - val_acc: 0.6480\n",
      "Epoch 28/50\n",
      "41/40 [==============================] - 1s - loss: 0.5657 - acc: 0.7206 - val_loss: 0.6052 - val_acc: 0.6262\n",
      "Epoch 29/50\n",
      "41/40 [==============================] - 1s - loss: 0.5529 - acc: 0.7270 - val_loss: 0.6215 - val_acc: 0.6355\n",
      "Epoch 30/50\n",
      "41/40 [==============================] - 1s - loss: 0.5689 - acc: 0.7149 - val_loss: 0.5925 - val_acc: 0.6324\n",
      "Epoch 31/50\n",
      "41/40 [==============================] - 1s - loss: 0.5538 - acc: 0.7137 - val_loss: 0.5691 - val_acc: 0.6573\n",
      "Epoch 32/50\n",
      "41/40 [==============================] - 1s - loss: 0.5522 - acc: 0.7293 - val_loss: 0.5848 - val_acc: 0.6324\n",
      "Epoch 33/50\n",
      "41/40 [==============================] - 1s - loss: 0.5584 - acc: 0.7213 - val_loss: 0.5737 - val_acc: 0.6449\n",
      "Epoch 34/50\n",
      "41/40 [==============================] - 1s - loss: 0.5644 - acc: 0.7149 - val_loss: 0.5741 - val_acc: 0.6262\n",
      "Epoch 35/50\n",
      "41/40 [==============================] - 1s - loss: 0.5514 - acc: 0.7259 - val_loss: 0.5867 - val_acc: 0.6293\n",
      "Epoch 36/50\n",
      "41/40 [==============================] - 1s - loss: 0.5509 - acc: 0.7247 - val_loss: 0.6637 - val_acc: 0.6199\n",
      "Epoch 37/50\n",
      "41/40 [==============================] - 1s - loss: 0.5568 - acc: 0.7248 - val_loss: 0.5987 - val_acc: 0.6449\n",
      "Epoch 38/50\n",
      "41/40 [==============================] - 1s - loss: 0.5407 - acc: 0.7468 - val_loss: 0.5843 - val_acc: 0.6511\n",
      "Epoch 39/50\n",
      "41/40 [==============================] - 1s - loss: 0.5524 - acc: 0.7092 - val_loss: 0.5744 - val_acc: 0.6449\n",
      "Epoch 40/50\n",
      "41/40 [==============================] - 1s - loss: 0.5387 - acc: 0.7361 - val_loss: 0.5953 - val_acc: 0.6511\n",
      "Epoch 41/50\n",
      "41/40 [==============================] - 1s - loss: 0.5498 - acc: 0.7244 - val_loss: 0.6045 - val_acc: 0.6417\n",
      "Epoch 42/50\n",
      "41/40 [==============================] - 1s - loss: 0.5436 - acc: 0.7331 - val_loss: 0.5754 - val_acc: 0.6542\n",
      "Epoch 43/50\n",
      "41/40 [==============================] - 1s - loss: 0.5465 - acc: 0.7312 - val_loss: 0.6710 - val_acc: 0.6262\n",
      "Epoch 44/50\n",
      "41/40 [==============================] - 1s - loss: 0.5495 - acc: 0.7167 - val_loss: 0.5894 - val_acc: 0.6480\n",
      "Epoch 45/50\n",
      "41/40 [==============================] - 1s - loss: 0.5537 - acc: 0.7225 - val_loss: 0.6340 - val_acc: 0.6324\n",
      "Epoch 46/50\n",
      "41/40 [==============================] - 1s - loss: 0.5427 - acc: 0.7244 - val_loss: 0.5792 - val_acc: 0.6604\n",
      "Epoch 47/50\n",
      "41/40 [==============================] - 1s - loss: 0.5339 - acc: 0.7476 - val_loss: 0.5725 - val_acc: 0.6417\n",
      "Epoch 48/50\n",
      "41/40 [==============================] - 1s - loss: 0.5430 - acc: 0.7228 - val_loss: 0.6119 - val_acc: 0.6417\n",
      "Epoch 49/50\n",
      "41/40 [==============================] - 1s - loss: 0.5463 - acc: 0.7199 - val_loss: 0.5713 - val_acc: 0.6636\n",
      "Epoch 50/50\n",
      "41/40 [==============================] - 1s - loss: 0.5439 - acc: 0.7282 - val_loss: 0.6075 - val_acc: 0.6449\n"
     ]
    }
   ],
   "source": [
    "simple.train(32, 50, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = helpers.get_images(test)\n",
    "predict = simple.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.34748366,  0.65251637],\n",
       "       [ 0.45657286,  0.54342711],\n",
       "       [ 0.69432497,  0.30567497],\n",
       "       ..., \n",
       "       [ 0.39580071,  0.60419929],\n",
       "       [ 0.48593286,  0.51406711],\n",
       "       [ 0.572124  ,  0.42787597]], dtype=float32)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg19 = DaveVGG19(Xtr, ytr, Xv, yv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size: 32\n",
      "Epochs: 50\n",
      "Epoch 1/50\n",
      "41/40 [==============================] - 7s - loss: 0.6942 - acc: 0.4892 - val_loss: 0.6929 - val_acc: 0.6573\n",
      "Epoch 2/50\n",
      "41/40 [==============================] - 6s - loss: 0.6920 - acc: 0.4860 - val_loss: 0.6643 - val_acc: 0.6573\n",
      "Epoch 3/50\n",
      "41/40 [==============================] - 6s - loss: 0.6935 - acc: 0.4925 - val_loss: 0.6933 - val_acc: 0.3427\n",
      "Epoch 4/50\n",
      "41/40 [==============================] - 6s - loss: 0.6932 - acc: 0.4975 - val_loss: 0.6940 - val_acc: 0.3427\n",
      "Epoch 5/50\n",
      "41/40 [==============================] - 6s - loss: 0.6937 - acc: 0.4782 - val_loss: 0.6952 - val_acc: 0.4081\n",
      "Epoch 6/50\n",
      "41/40 [==============================] - 6s - loss: 0.6932 - acc: 0.4983 - val_loss: 0.6928 - val_acc: 0.6573\n",
      "Epoch 7/50\n",
      "41/40 [==============================] - 6s - loss: 0.6889 - acc: 0.5128 - val_loss: 0.6419 - val_acc: 0.6573\n",
      "Epoch 8/50\n",
      "41/40 [==============================] - 6s - loss: 0.6679 - acc: 0.5715 - val_loss: 0.6822 - val_acc: 0.5763\n",
      "Epoch 9/50\n",
      "41/40 [==============================] - 6s - loss: 0.6925 - acc: 0.4949 - val_loss: 0.6943 - val_acc: 0.3427\n",
      "Epoch 10/50\n",
      "41/40 [==============================] - 6s - loss: 0.6942 - acc: 0.5093 - val_loss: 0.6922 - val_acc: 0.6573\n",
      "Epoch 11/50\n",
      "41/40 [==============================] - 6s - loss: 0.6932 - acc: 0.4953 - val_loss: 0.6921 - val_acc: 0.6573\n",
      "Epoch 12/50\n",
      "41/40 [==============================] - 6s - loss: 0.6932 - acc: 0.4773 - val_loss: 0.6929 - val_acc: 0.6573\n",
      "Epoch 13/50\n",
      "41/40 [==============================] - 6s - loss: 0.6932 - acc: 0.4953 - val_loss: 0.6925 - val_acc: 0.6573\n",
      "Epoch 14/50\n",
      "41/40 [==============================] - 6s - loss: 0.6924 - acc: 0.5981 - val_loss: 0.6908 - val_acc: 0.5389\n",
      "Epoch 15/50\n",
      "41/40 [==============================] - 6s - loss: 0.6401 - acc: 0.6394 - val_loss: 0.5967 - val_acc: 0.5607\n",
      "Epoch 16/50\n",
      "41/40 [==============================] - 6s - loss: 0.6480 - acc: 0.6138 - val_loss: 0.6344 - val_acc: 0.5016\n",
      "Epoch 17/50\n",
      "41/40 [==============================] - 6s - loss: 0.6291 - acc: 0.6249 - val_loss: 0.6527 - val_acc: 0.4984\n",
      "Epoch 18/50\n",
      "41/40 [==============================] - 6s - loss: 0.5999 - acc: 0.6835 - val_loss: 0.5804 - val_acc: 0.6293\n",
      "Epoch 19/50\n",
      "41/40 [==============================] - 6s - loss: 0.6041 - acc: 0.6988 - val_loss: 0.6839 - val_acc: 0.5919\n",
      "Epoch 20/50\n",
      "41/40 [==============================] - 6s - loss: 0.5969 - acc: 0.6951 - val_loss: 0.5819 - val_acc: 0.6449\n",
      "Epoch 21/50\n",
      "41/40 [==============================] - 6s - loss: 0.5826 - acc: 0.6919 - val_loss: 0.6006 - val_acc: 0.6449\n",
      "Epoch 22/50\n",
      "41/40 [==============================] - 6s - loss: 0.5742 - acc: 0.7018 - val_loss: 0.6357 - val_acc: 0.5794\n",
      "Epoch 23/50\n",
      "41/40 [==============================] - 6s - loss: 0.5533 - acc: 0.7430 - val_loss: 0.6532 - val_acc: 0.6480\n",
      "Epoch 24/50\n",
      "41/40 [==============================] - 6s - loss: 0.5312 - acc: 0.7529 - val_loss: 0.5283 - val_acc: 0.7788\n",
      "Epoch 25/50\n",
      "41/40 [==============================] - 6s - loss: 0.5449 - acc: 0.7446 - val_loss: 0.5296 - val_acc: 0.7196\n",
      "Epoch 26/50\n",
      "41/40 [==============================] - 6s - loss: 0.5182 - acc: 0.7587 - val_loss: 0.4848 - val_acc: 0.7539\n",
      "Epoch 27/50\n",
      "41/40 [==============================] - 6s - loss: 0.5142 - acc: 0.7400 - val_loss: 0.5543 - val_acc: 0.6698\n",
      "Epoch 28/50\n",
      "41/40 [==============================] - 6s - loss: 0.4767 - acc: 0.7831 - val_loss: 0.5372 - val_acc: 0.6978\n",
      "Epoch 29/50\n",
      "41/40 [==============================] - 6s - loss: 0.4508 - acc: 0.7971 - val_loss: 0.5393 - val_acc: 0.6729\n",
      "Epoch 30/50\n",
      "41/40 [==============================] - 6s - loss: 0.4586 - acc: 0.7717 - val_loss: 0.5095 - val_acc: 0.7009\n",
      "Epoch 31/50\n",
      "41/40 [==============================] - 6s - loss: 0.4204 - acc: 0.7942 - val_loss: 0.4610 - val_acc: 0.7508\n",
      "Epoch 32/50\n",
      "41/40 [==============================] - 6s - loss: 0.4040 - acc: 0.8110 - val_loss: 0.4134 - val_acc: 0.8318\n",
      "Epoch 33/50\n",
      "41/40 [==============================] - 6s - loss: 0.4039 - acc: 0.8174 - val_loss: 0.4060 - val_acc: 0.7975\n",
      "Epoch 34/50\n",
      "41/40 [==============================] - 6s - loss: 0.3763 - acc: 0.8288 - val_loss: 0.4149 - val_acc: 0.8380\n",
      "Epoch 35/50\n",
      "41/40 [==============================] - 6s - loss: 0.3513 - acc: 0.8388 - val_loss: 0.4073 - val_acc: 0.7975\n",
      "Epoch 36/50\n",
      "41/40 [==============================] - 6s - loss: 0.3874 - acc: 0.8102 - val_loss: 0.3919 - val_acc: 0.8069\n",
      "Epoch 37/50\n",
      "41/40 [==============================] - 6s - loss: 0.3642 - acc: 0.8399 - val_loss: 0.4433 - val_acc: 0.8224\n",
      "Epoch 38/50\n",
      "41/40 [==============================] - 6s - loss: 0.3518 - acc: 0.8239 - val_loss: 0.3651 - val_acc: 0.8723\n",
      "Epoch 39/50\n",
      "41/40 [==============================] - 6s - loss: 0.3386 - acc: 0.8377 - val_loss: 0.3412 - val_acc: 0.8723\n",
      "Epoch 40/50\n",
      "41/40 [==============================] - 6s - loss: 0.3504 - acc: 0.8372 - val_loss: 0.3591 - val_acc: 0.8287\n",
      "Epoch 41/50\n",
      "41/40 [==============================] - 6s - loss: 0.3277 - acc: 0.8543 - val_loss: 0.3470 - val_acc: 0.8816\n",
      "Epoch 42/50\n",
      "41/40 [==============================] - 6s - loss: 0.3592 - acc: 0.8277 - val_loss: 0.4671 - val_acc: 0.7227\n",
      "Epoch 43/50\n",
      "41/40 [==============================] - 6s - loss: 0.3607 - acc: 0.8345 - val_loss: 0.3438 - val_acc: 0.8692\n",
      "Epoch 44/50\n",
      "41/40 [==============================] - 6s - loss: 0.3214 - acc: 0.8517 - val_loss: 0.4249 - val_acc: 0.8411\n",
      "Epoch 45/50\n",
      "41/40 [==============================] - 6s - loss: 0.3277 - acc: 0.8521 - val_loss: 0.4251 - val_acc: 0.8162\n",
      "Epoch 46/50\n",
      "41/40 [==============================] - 6s - loss: 0.3089 - acc: 0.8681 - val_loss: 0.4429 - val_acc: 0.7913\n",
      "Epoch 47/50\n",
      "41/40 [==============================] - 6s - loss: 0.3197 - acc: 0.8464 - val_loss: 0.3533 - val_acc: 0.8660\n",
      "Epoch 48/50\n",
      "41/40 [==============================] - 6s - loss: 0.3522 - acc: 0.8406 - val_loss: 0.3965 - val_acc: 0.8318\n",
      "Epoch 49/50\n",
      "41/40 [==============================] - 6s - loss: 0.3114 - acc: 0.8510 - val_loss: 0.4075 - val_acc: 0.8069\n",
      "Epoch 50/50\n",
      "41/40 [==============================] - 6s - loss: 0.3120 - acc: 0.8494 - val_loss: 0.3463 - val_acc: 0.8879\n"
     ]
    }
   ],
   "source": [
    "vgg19.train(32, 50, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet = LeNetModel(Xtr, ytr, Xv, yv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size: 32\n",
      "Epochs: 50\n",
      "Epoch 1/50\n",
      "41/40 [==============================] - 2s - loss: 0.6516 - acc: 0.6302 - val_loss: 0.5950 - val_acc: 0.6417\n",
      "Epoch 2/50\n",
      "41/40 [==============================] - 1s - loss: 0.6099 - acc: 0.6588 - val_loss: 0.6305 - val_acc: 0.5888\n",
      "Epoch 3/50\n",
      "41/40 [==============================] - 1s - loss: 0.5688 - acc: 0.7099 - val_loss: 0.6078 - val_acc: 0.6573\n",
      "Epoch 4/50\n",
      "41/40 [==============================] - 1s - loss: 0.5451 - acc: 0.7369 - val_loss: 0.6821 - val_acc: 0.5919\n",
      "Epoch 5/50\n",
      "41/40 [==============================] - 1s - loss: 0.5338 - acc: 0.7343 - val_loss: 0.5238 - val_acc: 0.6978\n",
      "Epoch 6/50\n",
      "41/40 [==============================] - 1s - loss: 0.4997 - acc: 0.7465 - val_loss: 0.5024 - val_acc: 0.7757\n",
      "Epoch 7/50\n",
      "41/40 [==============================] - 1s - loss: 0.4895 - acc: 0.7552 - val_loss: 0.5509 - val_acc: 0.6542\n",
      "Epoch 8/50\n",
      "41/40 [==============================] - 1s - loss: 0.4619 - acc: 0.7694 - val_loss: 0.4909 - val_acc: 0.7196\n",
      "Epoch 9/50\n",
      "41/40 [==============================] - 1s - loss: 0.4741 - acc: 0.7656 - val_loss: 0.5029 - val_acc: 0.6947\n",
      "Epoch 10/50\n",
      "41/40 [==============================] - 1s - loss: 0.4708 - acc: 0.7788 - val_loss: 0.4229 - val_acc: 0.8411\n",
      "Epoch 11/50\n",
      "41/40 [==============================] - 1s - loss: 0.4133 - acc: 0.8048 - val_loss: 0.4636 - val_acc: 0.7134\n",
      "Epoch 12/50\n",
      "41/40 [==============================] - 1s - loss: 0.3960 - acc: 0.8071 - val_loss: 0.5129 - val_acc: 0.6916\n",
      "Epoch 13/50\n",
      "41/40 [==============================] - 1s - loss: 0.3942 - acc: 0.8177 - val_loss: 0.4491 - val_acc: 0.7726\n",
      "Epoch 14/50\n",
      "41/40 [==============================] - 1s - loss: 0.3945 - acc: 0.8189 - val_loss: 0.3925 - val_acc: 0.8349\n",
      "Epoch 15/50\n",
      "41/40 [==============================] - 1s - loss: 0.3742 - acc: 0.8246 - val_loss: 0.3853 - val_acc: 0.8162\n",
      "Epoch 16/50\n",
      "41/40 [==============================] - 1s - loss: 0.3735 - acc: 0.8353 - val_loss: 0.3666 - val_acc: 0.8536\n",
      "Epoch 17/50\n",
      "41/40 [==============================] - 1s - loss: 0.3778 - acc: 0.8220 - val_loss: 0.4277 - val_acc: 0.7445\n",
      "Epoch 18/50\n",
      "41/40 [==============================] - 1s - loss: 0.3622 - acc: 0.8414 - val_loss: 0.4261 - val_acc: 0.7944\n",
      "Epoch 19/50\n",
      "41/40 [==============================] - 1s - loss: 0.3701 - acc: 0.8177 - val_loss: 0.4918 - val_acc: 0.6947\n",
      "Epoch 20/50\n",
      "41/40 [==============================] - 1s - loss: 0.3567 - acc: 0.8266 - val_loss: 0.3425 - val_acc: 0.8411\n",
      "Epoch 21/50\n",
      "41/40 [==============================] - 1s - loss: 0.3633 - acc: 0.8327 - val_loss: 0.3588 - val_acc: 0.8287\n",
      "Epoch 22/50\n",
      "41/40 [==============================] - 1s - loss: 0.3524 - acc: 0.8284 - val_loss: 0.3455 - val_acc: 0.8349\n",
      "Epoch 23/50\n",
      "41/40 [==============================] - 1s - loss: 0.3424 - acc: 0.8490 - val_loss: 0.3358 - val_acc: 0.8442\n",
      "Epoch 24/50\n",
      "41/40 [==============================] - 1s - loss: 0.3474 - acc: 0.8319 - val_loss: 0.3748 - val_acc: 0.8224\n",
      "Epoch 25/50\n",
      "41/40 [==============================] - 1s - loss: 0.3294 - acc: 0.8418 - val_loss: 0.3445 - val_acc: 0.8442\n",
      "Epoch 26/50\n",
      "41/40 [==============================] - 1s - loss: 0.3282 - acc: 0.8559 - val_loss: 0.3651 - val_acc: 0.8131\n",
      "Epoch 27/50\n",
      "41/40 [==============================] - 1s - loss: 0.3206 - acc: 0.8383 - val_loss: 0.3883 - val_acc: 0.8037\n",
      "Epoch 28/50\n",
      "41/40 [==============================] - 1s - loss: 0.3234 - acc: 0.8487 - val_loss: 0.4680 - val_acc: 0.7321\n",
      "Epoch 29/50\n",
      "41/40 [==============================] - 1s - loss: 0.3241 - acc: 0.8429 - val_loss: 0.3818 - val_acc: 0.8006\n",
      "Epoch 30/50\n",
      "41/40 [==============================] - 1s - loss: 0.3371 - acc: 0.8308 - val_loss: 0.3428 - val_acc: 0.8411\n",
      "Epoch 31/50\n",
      "41/40 [==============================] - 1s - loss: 0.3229 - acc: 0.8612 - val_loss: 0.4489 - val_acc: 0.7477\n",
      "Epoch 32/50\n",
      "41/40 [==============================] - 1s - loss: 0.3056 - acc: 0.8521 - val_loss: 0.3050 - val_acc: 0.8847\n",
      "Epoch 33/50\n",
      "41/40 [==============================] - 1s - loss: 0.3099 - acc: 0.8559 - val_loss: 0.3604 - val_acc: 0.8318\n",
      "Epoch 34/50\n",
      "41/40 [==============================] - 1s - loss: 0.3058 - acc: 0.8604 - val_loss: 0.3114 - val_acc: 0.8723\n",
      "Epoch 35/50\n",
      "41/40 [==============================] - 1s - loss: 0.3071 - acc: 0.8658 - val_loss: 0.3451 - val_acc: 0.8380\n",
      "Epoch 36/50\n",
      "41/40 [==============================] - 1s - loss: 0.3128 - acc: 0.8510 - val_loss: 0.4045 - val_acc: 0.8006\n",
      "Epoch 37/50\n",
      "41/40 [==============================] - 1s - loss: 0.3032 - acc: 0.8612 - val_loss: 0.3083 - val_acc: 0.85050.8\n",
      "Epoch 38/50\n",
      "41/40 [==============================] - 4s - loss: 0.3085 - acc: 0.8461 - val_loss: 0.3356 - val_acc: 0.8380\n",
      "Epoch 39/50\n",
      "41/40 [==============================] - 2s - loss: 0.3197 - acc: 0.8482 - val_loss: 0.3329 - val_acc: 0.8380\n",
      "Epoch 40/50\n",
      "41/40 [==============================] - 3s - loss: 0.3092 - acc: 0.8540 - val_loss: 0.3358 - val_acc: 0.8567\n",
      "Epoch 41/50\n",
      "41/40 [==============================] - 6s - loss: 0.3094 - acc: 0.8490 - val_loss: 0.3178 - val_acc: 0.8505\n",
      "Epoch 42/50\n",
      "41/40 [==============================] - 1s - loss: 0.2864 - acc: 0.8696 - val_loss: 0.3032 - val_acc: 0.8847\n",
      "Epoch 43/50\n",
      "41/40 [==============================] - 1s - loss: 0.3014 - acc: 0.8543 - val_loss: 0.3546 - val_acc: 0.8224\n",
      "Epoch 44/50\n",
      "41/40 [==============================] - 1s - loss: 0.2978 - acc: 0.8555 - val_loss: 0.2930 - val_acc: 0.8754\n",
      "Epoch 45/50\n",
      "41/40 [==============================] - 1s - loss: 0.2852 - acc: 0.8601 - val_loss: 0.3015 - val_acc: 0.8847\n",
      "Epoch 46/50\n",
      "41/40 [==============================] - 1s - loss: 0.2931 - acc: 0.8559 - val_loss: 0.3146 - val_acc: 0.8505\n",
      "Epoch 47/50\n",
      "41/40 [==============================] - 1s - loss: 0.2863 - acc: 0.8605 - val_loss: 0.2964 - val_acc: 0.8785\n",
      "Epoch 48/50\n",
      "41/40 [==============================] - 1s - loss: 0.2873 - acc: 0.8688 - val_loss: 0.2865 - val_acc: 0.8847\n",
      "Epoch 49/50\n",
      "41/40 [==============================] - 1s - loss: 0.2748 - acc: 0.8757 - val_loss: 0.2888 - val_acc: 0.8754\n",
      "Epoch 50/50\n",
      "41/40 [==============================] - 1s - loss: 0.2817 - acc: 0.8609 - val_loss: 0.3452 - val_acc: 0.8224\n"
     ]
    }
   ],
   "source": [
    "lenet.train(32, 50, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<models.DaveModel object at 0x000001B59AD50278>\n",
      "<models.DaveVGG object at 0x000001B6758626D8>\n",
      "<models.SimpleModel object at 0x000001B58C3559B0>\n",
      "<models.DaveVGG19 object at 0x000001B58D512908>\n",
      "<models.LeNetModel object at 0x000001B676FBC518>\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/QuantScientist/Deep-Learning-Boot-Camp/blob/master/Kaggle-PyTorch/README.md\n",
    "print(model)\n",
    "print(vgg)\n",
    "print(simple)\n",
    "print(vgg19)\n",
    "print(lenet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = helpers.get_images(test)\n",
    "\n",
    "pred1 = model.predict(X_test)\n",
    "pred2 = vgg.predict(X_test)\n",
    "pred3 = simple.predict(X_test)\n",
    "pred4 = vgg19.predict(X_test)\n",
    "pred5 = lenet.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8424, 2)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True, False, ..., False,  True, False], dtype=bool)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pred1[pred1[1] > 0.5]\n",
    "pred1[pred1[:,1] > 0.5]\n",
    "pred1[:,1] > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        True\n",
       "1        True\n",
       "2       False\n",
       "3        True\n",
       "4       False\n",
       "5        True\n",
       "6       False\n",
       "7        True\n",
       "8       False\n",
       "9       False\n",
       "10      False\n",
       "11      False\n",
       "12      False\n",
       "13       True\n",
       "14       True\n",
       "15      False\n",
       "16      False\n",
       "17      False\n",
       "18       True\n",
       "19       True\n",
       "20      False\n",
       "21       True\n",
       "22       True\n",
       "23      False\n",
       "24      False\n",
       "25      False\n",
       "26       True\n",
       "27      False\n",
       "28      False\n",
       "29       True\n",
       "        ...  \n",
       "8394    False\n",
       "8395    False\n",
       "8396    False\n",
       "8397     True\n",
       "8398    False\n",
       "8399    False\n",
       "8400    False\n",
       "8401    False\n",
       "8402    False\n",
       "8403     True\n",
       "8404     True\n",
       "8405     True\n",
       "8406    False\n",
       "8407    False\n",
       "8408    False\n",
       "8409     True\n",
       "8410    False\n",
       "8411    False\n",
       "8412     True\n",
       "8413    False\n",
       "8414    False\n",
       "8415     True\n",
       "8416    False\n",
       "8417    False\n",
       "8418     True\n",
       "8419    False\n",
       "8420    False\n",
       "8421    False\n",
       "8422     True\n",
       "8423    False\n",
       "Length: 8424, dtype: bool"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame()\n",
    "results[\"model\"] = pred1[:,1] > 0.5\n",
    "results[\"model_likelihood\"] = pred1[:,1]\n",
    "results[\"vgg\"] = pred2[:,1] > 0.5\n",
    "results[\"vgg_likelihood\"] = pred2[:,1]\n",
    "results[\"simple\"] = pred3[:,1] > 0.5\n",
    "results[\"simple_likelihood\"] = pred3[:,1]\n",
    "results[\"vgg19\"] = pred4[:,1] > 0.5\n",
    "results[\"vgg19_likelihood\"] = pred4[:,1]\n",
    "# results[\"model\"] == True & results[\"vgg\"] == True & results[\"simple\"] == True & results[\"vgg19\"] == True\n",
    "results.apply(lambda x: x[\"model\"] == True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>model_likelihood</th>\n",
       "      <th>vgg</th>\n",
       "      <th>vgg_likelihood</th>\n",
       "      <th>simple</th>\n",
       "      <th>simple_likelihood</th>\n",
       "      <th>vgg19</th>\n",
       "      <th>vgg19_likelihood</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>0.696621</td>\n",
       "      <td>False</td>\n",
       "      <td>2.313375e-01</td>\n",
       "      <td>True</td>\n",
       "      <td>0.725662</td>\n",
       "      <td>False</td>\n",
       "      <td>2.622276e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>0.838054</td>\n",
       "      <td>True</td>\n",
       "      <td>7.063180e-01</td>\n",
       "      <td>True</td>\n",
       "      <td>0.550063</td>\n",
       "      <td>False</td>\n",
       "      <td>3.387524e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>False</td>\n",
       "      <td>0.003580</td>\n",
       "      <td>False</td>\n",
       "      <td>1.255908e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>0.980969</td>\n",
       "      <td>True</td>\n",
       "      <td>9.908686e-01</td>\n",
       "      <td>True</td>\n",
       "      <td>0.741207</td>\n",
       "      <td>True</td>\n",
       "      <td>9.221749e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>0.051844</td>\n",
       "      <td>False</td>\n",
       "      <td>2.547195e-01</td>\n",
       "      <td>False</td>\n",
       "      <td>0.212795</td>\n",
       "      <td>False</td>\n",
       "      <td>1.431807e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>True</td>\n",
       "      <td>0.667646</td>\n",
       "      <td>True</td>\n",
       "      <td>8.055308e-01</td>\n",
       "      <td>True</td>\n",
       "      <td>0.729096</td>\n",
       "      <td>False</td>\n",
       "      <td>3.579509e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>False</td>\n",
       "      <td>0.114867</td>\n",
       "      <td>False</td>\n",
       "      <td>1.705254e-01</td>\n",
       "      <td>True</td>\n",
       "      <td>0.588717</td>\n",
       "      <td>False</td>\n",
       "      <td>3.390283e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>True</td>\n",
       "      <td>0.999790</td>\n",
       "      <td>True</td>\n",
       "      <td>9.995858e-01</td>\n",
       "      <td>True</td>\n",
       "      <td>0.768419</td>\n",
       "      <td>True</td>\n",
       "      <td>9.963431e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>False</td>\n",
       "      <td>0.008093</td>\n",
       "      <td>False</td>\n",
       "      <td>5.179535e-02</td>\n",
       "      <td>False</td>\n",
       "      <td>0.320062</td>\n",
       "      <td>False</td>\n",
       "      <td>9.277523e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>False</td>\n",
       "      <td>0.000606</td>\n",
       "      <td>False</td>\n",
       "      <td>4.815210e-04</td>\n",
       "      <td>False</td>\n",
       "      <td>0.019523</td>\n",
       "      <td>False</td>\n",
       "      <td>1.862418e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>False</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>False</td>\n",
       "      <td>0.003033</td>\n",
       "      <td>False</td>\n",
       "      <td>6.072825e-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>False</td>\n",
       "      <td>0.130430</td>\n",
       "      <td>False</td>\n",
       "      <td>1.307817e-01</td>\n",
       "      <td>False</td>\n",
       "      <td>0.359081</td>\n",
       "      <td>False</td>\n",
       "      <td>1.417150e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>False</td>\n",
       "      <td>0.041462</td>\n",
       "      <td>False</td>\n",
       "      <td>1.188748e-02</td>\n",
       "      <td>False</td>\n",
       "      <td>0.400673</td>\n",
       "      <td>False</td>\n",
       "      <td>2.975202e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>True</td>\n",
       "      <td>0.924223</td>\n",
       "      <td>True</td>\n",
       "      <td>7.470577e-01</td>\n",
       "      <td>False</td>\n",
       "      <td>0.448507</td>\n",
       "      <td>False</td>\n",
       "      <td>1.001399e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>True</td>\n",
       "      <td>0.514070</td>\n",
       "      <td>True</td>\n",
       "      <td>9.884865e-01</td>\n",
       "      <td>True</td>\n",
       "      <td>0.649154</td>\n",
       "      <td>False</td>\n",
       "      <td>4.754047e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>False</td>\n",
       "      <td>0.172888</td>\n",
       "      <td>False</td>\n",
       "      <td>1.226207e-02</td>\n",
       "      <td>False</td>\n",
       "      <td>0.413565</td>\n",
       "      <td>False</td>\n",
       "      <td>3.182007e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>False</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>False</td>\n",
       "      <td>3.410003e-34</td>\n",
       "      <td>False</td>\n",
       "      <td>0.009533</td>\n",
       "      <td>False</td>\n",
       "      <td>9.549291e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>False</td>\n",
       "      <td>0.193871</td>\n",
       "      <td>False</td>\n",
       "      <td>1.661145e-01</td>\n",
       "      <td>False</td>\n",
       "      <td>0.082362</td>\n",
       "      <td>False</td>\n",
       "      <td>1.551616e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>True</td>\n",
       "      <td>0.671361</td>\n",
       "      <td>True</td>\n",
       "      <td>9.499999e-01</td>\n",
       "      <td>False</td>\n",
       "      <td>0.431943</td>\n",
       "      <td>False</td>\n",
       "      <td>6.499150e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>True</td>\n",
       "      <td>0.967728</td>\n",
       "      <td>True</td>\n",
       "      <td>7.967408e-01</td>\n",
       "      <td>True</td>\n",
       "      <td>0.697392</td>\n",
       "      <td>True</td>\n",
       "      <td>7.559910e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>False</td>\n",
       "      <td>0.031253</td>\n",
       "      <td>False</td>\n",
       "      <td>6.949332e-02</td>\n",
       "      <td>False</td>\n",
       "      <td>0.361352</td>\n",
       "      <td>False</td>\n",
       "      <td>2.631676e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>True</td>\n",
       "      <td>0.872811</td>\n",
       "      <td>True</td>\n",
       "      <td>5.827228e-01</td>\n",
       "      <td>True</td>\n",
       "      <td>0.659550</td>\n",
       "      <td>False</td>\n",
       "      <td>3.896827e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>True</td>\n",
       "      <td>0.527656</td>\n",
       "      <td>True</td>\n",
       "      <td>8.136707e-01</td>\n",
       "      <td>True</td>\n",
       "      <td>0.627958</td>\n",
       "      <td>True</td>\n",
       "      <td>6.392693e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>False</td>\n",
       "      <td>0.014607</td>\n",
       "      <td>False</td>\n",
       "      <td>7.653201e-02</td>\n",
       "      <td>False</td>\n",
       "      <td>0.349465</td>\n",
       "      <td>False</td>\n",
       "      <td>1.948517e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>False</td>\n",
       "      <td>0.002533</td>\n",
       "      <td>False</td>\n",
       "      <td>6.558131e-02</td>\n",
       "      <td>True</td>\n",
       "      <td>0.550855</td>\n",
       "      <td>False</td>\n",
       "      <td>5.061068e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>False</td>\n",
       "      <td>0.002119</td>\n",
       "      <td>False</td>\n",
       "      <td>1.243657e-01</td>\n",
       "      <td>False</td>\n",
       "      <td>0.020218</td>\n",
       "      <td>False</td>\n",
       "      <td>6.156570e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>True</td>\n",
       "      <td>0.743502</td>\n",
       "      <td>False</td>\n",
       "      <td>1.497932e-01</td>\n",
       "      <td>True</td>\n",
       "      <td>0.693705</td>\n",
       "      <td>False</td>\n",
       "      <td>3.478920e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>False</td>\n",
       "      <td>0.056231</td>\n",
       "      <td>False</td>\n",
       "      <td>1.520122e-01</td>\n",
       "      <td>False</td>\n",
       "      <td>0.089960</td>\n",
       "      <td>False</td>\n",
       "      <td>9.169323e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>False</td>\n",
       "      <td>0.060982</td>\n",
       "      <td>False</td>\n",
       "      <td>2.381467e-01</td>\n",
       "      <td>False</td>\n",
       "      <td>0.232281</td>\n",
       "      <td>False</td>\n",
       "      <td>1.338733e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>True</td>\n",
       "      <td>0.999097</td>\n",
       "      <td>True</td>\n",
       "      <td>9.986550e-01</td>\n",
       "      <td>True</td>\n",
       "      <td>0.711902</td>\n",
       "      <td>True</td>\n",
       "      <td>9.839527e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8394</th>\n",
       "      <td>False</td>\n",
       "      <td>0.001947</td>\n",
       "      <td>False</td>\n",
       "      <td>7.529721e-03</td>\n",
       "      <td>False</td>\n",
       "      <td>0.472772</td>\n",
       "      <td>False</td>\n",
       "      <td>2.301898e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8395</th>\n",
       "      <td>False</td>\n",
       "      <td>0.043094</td>\n",
       "      <td>True</td>\n",
       "      <td>8.076941e-01</td>\n",
       "      <td>False</td>\n",
       "      <td>0.341695</td>\n",
       "      <td>False</td>\n",
       "      <td>8.171711e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8396</th>\n",
       "      <td>False</td>\n",
       "      <td>0.002405</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>False</td>\n",
       "      <td>0.003704</td>\n",
       "      <td>False</td>\n",
       "      <td>9.780127e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8397</th>\n",
       "      <td>True</td>\n",
       "      <td>0.639393</td>\n",
       "      <td>True</td>\n",
       "      <td>6.899255e-01</td>\n",
       "      <td>False</td>\n",
       "      <td>0.470708</td>\n",
       "      <td>False</td>\n",
       "      <td>4.141081e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8398</th>\n",
       "      <td>False</td>\n",
       "      <td>0.191979</td>\n",
       "      <td>True</td>\n",
       "      <td>7.329673e-01</td>\n",
       "      <td>False</td>\n",
       "      <td>0.164560</td>\n",
       "      <td>False</td>\n",
       "      <td>4.898270e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8399</th>\n",
       "      <td>False</td>\n",
       "      <td>0.003250</td>\n",
       "      <td>True</td>\n",
       "      <td>9.877773e-01</td>\n",
       "      <td>True</td>\n",
       "      <td>0.684621</td>\n",
       "      <td>False</td>\n",
       "      <td>1.976709e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8400</th>\n",
       "      <td>False</td>\n",
       "      <td>0.015784</td>\n",
       "      <td>False</td>\n",
       "      <td>4.388450e-02</td>\n",
       "      <td>False</td>\n",
       "      <td>0.063372</td>\n",
       "      <td>False</td>\n",
       "      <td>2.946251e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8401</th>\n",
       "      <td>False</td>\n",
       "      <td>0.208668</td>\n",
       "      <td>False</td>\n",
       "      <td>2.139233e-01</td>\n",
       "      <td>False</td>\n",
       "      <td>0.409223</td>\n",
       "      <td>False</td>\n",
       "      <td>3.147234e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8402</th>\n",
       "      <td>False</td>\n",
       "      <td>0.133065</td>\n",
       "      <td>False</td>\n",
       "      <td>4.767716e-01</td>\n",
       "      <td>False</td>\n",
       "      <td>0.072493</td>\n",
       "      <td>False</td>\n",
       "      <td>1.359869e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8403</th>\n",
       "      <td>True</td>\n",
       "      <td>0.745644</td>\n",
       "      <td>False</td>\n",
       "      <td>4.302611e-01</td>\n",
       "      <td>False</td>\n",
       "      <td>0.238871</td>\n",
       "      <td>False</td>\n",
       "      <td>1.993451e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8404</th>\n",
       "      <td>True</td>\n",
       "      <td>0.986814</td>\n",
       "      <td>False</td>\n",
       "      <td>4.925995e-01</td>\n",
       "      <td>False</td>\n",
       "      <td>0.256277</td>\n",
       "      <td>False</td>\n",
       "      <td>1.240769e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8405</th>\n",
       "      <td>True</td>\n",
       "      <td>0.623255</td>\n",
       "      <td>False</td>\n",
       "      <td>4.230748e-02</td>\n",
       "      <td>False</td>\n",
       "      <td>0.291689</td>\n",
       "      <td>False</td>\n",
       "      <td>2.795568e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8406</th>\n",
       "      <td>False</td>\n",
       "      <td>0.011979</td>\n",
       "      <td>True</td>\n",
       "      <td>8.359008e-01</td>\n",
       "      <td>False</td>\n",
       "      <td>0.048110</td>\n",
       "      <td>False</td>\n",
       "      <td>3.909766e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8407</th>\n",
       "      <td>False</td>\n",
       "      <td>0.393058</td>\n",
       "      <td>False</td>\n",
       "      <td>2.980890e-02</td>\n",
       "      <td>False</td>\n",
       "      <td>0.481391</td>\n",
       "      <td>False</td>\n",
       "      <td>4.053885e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8408</th>\n",
       "      <td>False</td>\n",
       "      <td>0.029712</td>\n",
       "      <td>False</td>\n",
       "      <td>3.037384e-05</td>\n",
       "      <td>False</td>\n",
       "      <td>0.020769</td>\n",
       "      <td>False</td>\n",
       "      <td>1.820684e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8409</th>\n",
       "      <td>True</td>\n",
       "      <td>0.998811</td>\n",
       "      <td>True</td>\n",
       "      <td>9.864587e-01</td>\n",
       "      <td>True</td>\n",
       "      <td>0.734496</td>\n",
       "      <td>True</td>\n",
       "      <td>9.168021e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8410</th>\n",
       "      <td>False</td>\n",
       "      <td>0.001546</td>\n",
       "      <td>False</td>\n",
       "      <td>1.085022e-02</td>\n",
       "      <td>False</td>\n",
       "      <td>0.269858</td>\n",
       "      <td>False</td>\n",
       "      <td>1.420071e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8411</th>\n",
       "      <td>False</td>\n",
       "      <td>0.122472</td>\n",
       "      <td>True</td>\n",
       "      <td>8.684720e-01</td>\n",
       "      <td>False</td>\n",
       "      <td>0.317466</td>\n",
       "      <td>False</td>\n",
       "      <td>1.335748e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8412</th>\n",
       "      <td>True</td>\n",
       "      <td>0.923808</td>\n",
       "      <td>True</td>\n",
       "      <td>9.363378e-01</td>\n",
       "      <td>True</td>\n",
       "      <td>0.715602</td>\n",
       "      <td>True</td>\n",
       "      <td>7.381798e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8413</th>\n",
       "      <td>False</td>\n",
       "      <td>0.014477</td>\n",
       "      <td>False</td>\n",
       "      <td>2.525859e-01</td>\n",
       "      <td>False</td>\n",
       "      <td>0.305664</td>\n",
       "      <td>False</td>\n",
       "      <td>1.228922e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8414</th>\n",
       "      <td>False</td>\n",
       "      <td>0.004873</td>\n",
       "      <td>False</td>\n",
       "      <td>3.326475e-03</td>\n",
       "      <td>False</td>\n",
       "      <td>0.037803</td>\n",
       "      <td>False</td>\n",
       "      <td>1.459402e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8415</th>\n",
       "      <td>True</td>\n",
       "      <td>0.607061</td>\n",
       "      <td>True</td>\n",
       "      <td>5.708901e-01</td>\n",
       "      <td>False</td>\n",
       "      <td>0.317617</td>\n",
       "      <td>False</td>\n",
       "      <td>7.810270e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8416</th>\n",
       "      <td>False</td>\n",
       "      <td>0.004588</td>\n",
       "      <td>False</td>\n",
       "      <td>1.478721e-03</td>\n",
       "      <td>False</td>\n",
       "      <td>0.053147</td>\n",
       "      <td>False</td>\n",
       "      <td>7.924372e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8417</th>\n",
       "      <td>False</td>\n",
       "      <td>0.140831</td>\n",
       "      <td>False</td>\n",
       "      <td>3.923457e-01</td>\n",
       "      <td>False</td>\n",
       "      <td>0.103526</td>\n",
       "      <td>False</td>\n",
       "      <td>1.020998e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8418</th>\n",
       "      <td>True</td>\n",
       "      <td>0.997275</td>\n",
       "      <td>True</td>\n",
       "      <td>9.808847e-01</td>\n",
       "      <td>True</td>\n",
       "      <td>0.688246</td>\n",
       "      <td>True</td>\n",
       "      <td>9.145825e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8419</th>\n",
       "      <td>False</td>\n",
       "      <td>0.000527</td>\n",
       "      <td>False</td>\n",
       "      <td>8.703639e-03</td>\n",
       "      <td>True</td>\n",
       "      <td>0.510296</td>\n",
       "      <td>False</td>\n",
       "      <td>3.316268e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8420</th>\n",
       "      <td>False</td>\n",
       "      <td>0.155198</td>\n",
       "      <td>False</td>\n",
       "      <td>2.255798e-01</td>\n",
       "      <td>False</td>\n",
       "      <td>0.062634</td>\n",
       "      <td>False</td>\n",
       "      <td>1.432562e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8421</th>\n",
       "      <td>False</td>\n",
       "      <td>0.024497</td>\n",
       "      <td>False</td>\n",
       "      <td>1.715579e-01</td>\n",
       "      <td>True</td>\n",
       "      <td>0.697350</td>\n",
       "      <td>False</td>\n",
       "      <td>3.442084e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8422</th>\n",
       "      <td>True</td>\n",
       "      <td>0.999468</td>\n",
       "      <td>True</td>\n",
       "      <td>9.982898e-01</td>\n",
       "      <td>True</td>\n",
       "      <td>0.691537</td>\n",
       "      <td>True</td>\n",
       "      <td>8.975145e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8423</th>\n",
       "      <td>False</td>\n",
       "      <td>0.001985</td>\n",
       "      <td>False</td>\n",
       "      <td>4.568782e-01</td>\n",
       "      <td>False</td>\n",
       "      <td>0.012301</td>\n",
       "      <td>False</td>\n",
       "      <td>3.256163e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8424 rows  8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      model  model_likelihood    vgg  vgg_likelihood  simple  \\\n",
       "0      True          0.696621  False    2.313375e-01    True   \n",
       "1      True          0.838054   True    7.063180e-01    True   \n",
       "2     False          0.000163  False    0.000000e+00   False   \n",
       "3      True          0.980969   True    9.908686e-01    True   \n",
       "4     False          0.051844  False    2.547195e-01   False   \n",
       "5      True          0.667646   True    8.055308e-01    True   \n",
       "6     False          0.114867  False    1.705254e-01    True   \n",
       "7      True          0.999790   True    9.995858e-01    True   \n",
       "8     False          0.008093  False    5.179535e-02   False   \n",
       "9     False          0.000606  False    4.815210e-04   False   \n",
       "10    False          0.000045  False    0.000000e+00   False   \n",
       "11    False          0.130430  False    1.307817e-01   False   \n",
       "12    False          0.041462  False    1.188748e-02   False   \n",
       "13     True          0.924223   True    7.470577e-01   False   \n",
       "14     True          0.514070   True    9.884865e-01    True   \n",
       "15    False          0.172888  False    1.226207e-02   False   \n",
       "16    False          0.000900  False    3.410003e-34   False   \n",
       "17    False          0.193871  False    1.661145e-01   False   \n",
       "18     True          0.671361   True    9.499999e-01   False   \n",
       "19     True          0.967728   True    7.967408e-01    True   \n",
       "20    False          0.031253  False    6.949332e-02   False   \n",
       "21     True          0.872811   True    5.827228e-01    True   \n",
       "22     True          0.527656   True    8.136707e-01    True   \n",
       "23    False          0.014607  False    7.653201e-02   False   \n",
       "24    False          0.002533  False    6.558131e-02    True   \n",
       "25    False          0.002119  False    1.243657e-01   False   \n",
       "26     True          0.743502  False    1.497932e-01    True   \n",
       "27    False          0.056231  False    1.520122e-01   False   \n",
       "28    False          0.060982  False    2.381467e-01   False   \n",
       "29     True          0.999097   True    9.986550e-01    True   \n",
       "...     ...               ...    ...             ...     ...   \n",
       "8394  False          0.001947  False    7.529721e-03   False   \n",
       "8395  False          0.043094   True    8.076941e-01   False   \n",
       "8396  False          0.002405  False    0.000000e+00   False   \n",
       "8397   True          0.639393   True    6.899255e-01   False   \n",
       "8398  False          0.191979   True    7.329673e-01   False   \n",
       "8399  False          0.003250   True    9.877773e-01    True   \n",
       "8400  False          0.015784  False    4.388450e-02   False   \n",
       "8401  False          0.208668  False    2.139233e-01   False   \n",
       "8402  False          0.133065  False    4.767716e-01   False   \n",
       "8403   True          0.745644  False    4.302611e-01   False   \n",
       "8404   True          0.986814  False    4.925995e-01   False   \n",
       "8405   True          0.623255  False    4.230748e-02   False   \n",
       "8406  False          0.011979   True    8.359008e-01   False   \n",
       "8407  False          0.393058  False    2.980890e-02   False   \n",
       "8408  False          0.029712  False    3.037384e-05   False   \n",
       "8409   True          0.998811   True    9.864587e-01    True   \n",
       "8410  False          0.001546  False    1.085022e-02   False   \n",
       "8411  False          0.122472   True    8.684720e-01   False   \n",
       "8412   True          0.923808   True    9.363378e-01    True   \n",
       "8413  False          0.014477  False    2.525859e-01   False   \n",
       "8414  False          0.004873  False    3.326475e-03   False   \n",
       "8415   True          0.607061   True    5.708901e-01   False   \n",
       "8416  False          0.004588  False    1.478721e-03   False   \n",
       "8417  False          0.140831  False    3.923457e-01   False   \n",
       "8418   True          0.997275   True    9.808847e-01    True   \n",
       "8419  False          0.000527  False    8.703639e-03    True   \n",
       "8420  False          0.155198  False    2.255798e-01   False   \n",
       "8421  False          0.024497  False    1.715579e-01    True   \n",
       "8422   True          0.999468   True    9.982898e-01    True   \n",
       "8423  False          0.001985  False    4.568782e-01   False   \n",
       "\n",
       "      simple_likelihood  vgg19  vgg19_likelihood  \n",
       "0              0.725662  False      2.622276e-01  \n",
       "1              0.550063  False      3.387524e-01  \n",
       "2              0.003580  False      1.255908e-17  \n",
       "3              0.741207   True      9.221749e-01  \n",
       "4              0.212795  False      1.431807e-02  \n",
       "5              0.729096  False      3.579509e-02  \n",
       "6              0.588717  False      3.390283e-02  \n",
       "7              0.768419   True      9.963431e-01  \n",
       "8              0.320062  False      9.277523e-02  \n",
       "9              0.019523  False      1.862418e-07  \n",
       "10             0.003033  False      6.072825e-18  \n",
       "11             0.359081  False      1.417150e-02  \n",
       "12             0.400673  False      2.975202e-02  \n",
       "13             0.448507  False      1.001399e-01  \n",
       "14             0.649154  False      4.754047e-02  \n",
       "15             0.413565  False      3.182007e-02  \n",
       "16             0.009533  False      9.549291e-13  \n",
       "17             0.082362  False      1.551616e-04  \n",
       "18             0.431943  False      6.499150e-02  \n",
       "19             0.697392   True      7.559910e-01  \n",
       "20             0.361352  False      2.631676e-02  \n",
       "21             0.659550  False      3.896827e-01  \n",
       "22             0.627958   True      6.392693e-01  \n",
       "23             0.349465  False      1.948517e-02  \n",
       "24             0.550855  False      5.061068e-02  \n",
       "25             0.020218  False      6.156570e-07  \n",
       "26             0.693705  False      3.478920e-01  \n",
       "27             0.089960  False      9.169323e-05  \n",
       "28             0.232281  False      1.338733e-02  \n",
       "29             0.711902   True      9.839527e-01  \n",
       "...                 ...    ...               ...  \n",
       "8394           0.472772  False      2.301898e-02  \n",
       "8395           0.341695  False      8.171711e-02  \n",
       "8396           0.003704  False      9.780127e-17  \n",
       "8397           0.470708  False      4.141081e-02  \n",
       "8398           0.164560  False      4.898270e-03  \n",
       "8399           0.684621  False      1.976709e-01  \n",
       "8400           0.063372  False      2.946251e-03  \n",
       "8401           0.409223  False      3.147234e-02  \n",
       "8402           0.072493  False      1.359869e-03  \n",
       "8403           0.238871  False      1.993451e-02  \n",
       "8404           0.256277  False      1.240769e-02  \n",
       "8405           0.291689  False      2.795568e-02  \n",
       "8406           0.048110  False      3.909766e-03  \n",
       "8407           0.481391  False      4.053885e-02  \n",
       "8408           0.020769  False      1.820684e-08  \n",
       "8409           0.734496   True      9.168021e-01  \n",
       "8410           0.269858  False      1.420071e-02  \n",
       "8411           0.317466  False      1.335748e-02  \n",
       "8412           0.715602   True      7.381798e-01  \n",
       "8413           0.305664  False      1.228922e-02  \n",
       "8414           0.037803  False      1.459402e-04  \n",
       "8415           0.317617  False      7.810270e-03  \n",
       "8416           0.053147  False      7.924372e-03  \n",
       "8417           0.103526  False      1.020998e-03  \n",
       "8418           0.688246   True      9.145825e-01  \n",
       "8419           0.510296  False      3.316268e-02  \n",
       "8420           0.062634  False      1.432562e-04  \n",
       "8421           0.697350  False      3.442084e-02  \n",
       "8422           0.691537   True      8.975145e-01  \n",
       "8423           0.012301  False      3.256163e-07  \n",
       "\n",
       "[8424 rows x 8 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainRunner = Trainer([\n",
    "    DaveModel(Xtr, ytr, Xv, yv),\n",
    "    DaveVGG(Xtr, ytr, Xv, yv),\n",
    "    DaveVGG19(Xtr, ytr, Xv, yv),\n",
    "    LeNetModel(Xtr, ytr, Xv, yv)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size: 32\n",
      "Epochs: 80\n",
      "Epoch 1/80\n",
      "41/40 [==============================] - 3s - loss: 0.5433 - acc: 0.7035 - val_loss: 0.6829 - val_acc: 0.5576\n",
      "Epoch 2/80\n",
      "41/40 [==============================] - 1s - loss: 0.4978 - acc: 0.7457 - val_loss: 0.6395 - val_acc: 0.6573\n",
      "Epoch 3/80\n",
      "41/40 [==============================] - 1s - loss: 0.4585 - acc: 0.7735 - val_loss: 0.6916 - val_acc: 0.6573\n",
      "Epoch 4/80\n",
      "41/40 [==============================] - 1s - loss: 0.4279 - acc: 0.7872 - val_loss: 0.7574 - val_acc: 0.6573\n",
      "Epoch 5/80\n",
      "41/40 [==============================] - 1s - loss: 0.4055 - acc: 0.8011 - val_loss: 0.9107 - val_acc: 0.6573\n",
      "Epoch 6/80\n",
      "41/40 [==============================] - 1s - loss: 0.3849 - acc: 0.8261 - val_loss: 1.0327 - val_acc: 0.6573\n",
      "Epoch 7/80\n",
      "41/40 [==============================] - 1s - loss: 0.3730 - acc: 0.8163 - val_loss: 1.2652 - val_acc: 0.6573\n",
      "Epoch 8/80\n",
      "41/40 [==============================] - 1s - loss: 0.3671 - acc: 0.8064 - val_loss: 1.2398 - val_acc: 0.6573\n",
      "Epoch 9/80\n",
      "41/40 [==============================] - 1s - loss: 0.3579 - acc: 0.8281 - val_loss: 1.4165 - val_acc: 0.6573\n",
      "Epoch 10/80\n",
      "41/40 [==============================] - 1s - loss: 0.3402 - acc: 0.8441 - val_loss: 1.3715 - val_acc: 0.6573\n",
      "Epoch 11/80\n",
      "41/40 [==============================] - 1s - loss: 0.3232 - acc: 0.8429 - val_loss: 1.0655 - val_acc: 0.6480\n",
      "Epoch 12/80\n",
      "41/40 [==============================] - 1s - loss: 0.3443 - acc: 0.8372 - val_loss: 0.9574 - val_acc: 0.6511\n",
      "Epoch 13/80\n",
      "41/40 [==============================] - 1s - loss: 0.3412 - acc: 0.8354 - val_loss: 0.7149 - val_acc: 0.6822\n",
      "Epoch 14/80\n",
      "41/40 [==============================] - 1s - loss: 0.3280 - acc: 0.8285 - val_loss: 0.4082 - val_acc: 0.7819\n",
      "Epoch 15/80\n",
      "41/40 [==============================] - 1s - loss: 0.3223 - acc: 0.8510 - val_loss: 0.3372 - val_acc: 0.8567\n",
      "Epoch 16/80\n",
      "41/40 [==============================] - 1s - loss: 0.3185 - acc: 0.8578 - val_loss: 0.3108 - val_acc: 0.8567\n",
      "Epoch 17/80\n",
      "41/40 [==============================] - 1s - loss: 0.3134 - acc: 0.8594 - val_loss: 0.3447 - val_acc: 0.7975\n",
      "Epoch 18/80\n",
      "41/40 [==============================] - 1s - loss: 0.2804 - acc: 0.8688 - val_loss: 0.3738 - val_acc: 0.8069\n",
      "Epoch 19/80\n",
      "41/40 [==============================] - 1s - loss: 0.3089 - acc: 0.8586 - val_loss: 0.3337 - val_acc: 0.8193\n",
      "Epoch 20/80\n",
      "41/40 [==============================] - 1s - loss: 0.2915 - acc: 0.8650 - val_loss: 0.3241 - val_acc: 0.8411\n",
      "Epoch 21/80\n",
      "41/40 [==============================] - 1s - loss: 0.2801 - acc: 0.8772 - val_loss: 0.3061 - val_acc: 0.8629\n",
      "Epoch 22/80\n",
      "41/40 [==============================] - 1s - loss: 0.2729 - acc: 0.8818 - val_loss: 0.2845 - val_acc: 0.8785\n",
      "Epoch 23/80\n",
      "41/40 [==============================] - 1s - loss: 0.2882 - acc: 0.8711 - val_loss: 0.2971 - val_acc: 0.8723\n",
      "Epoch 24/80\n",
      "41/40 [==============================] - 2s - loss: 0.2843 - acc: 0.8734 - val_loss: 0.2898 - val_acc: 0.8629\n",
      "Epoch 25/80\n",
      "41/40 [==============================] - 1s - loss: 0.2738 - acc: 0.8681 - val_loss: 0.3112 - val_acc: 0.8536\n",
      "Epoch 26/80\n",
      "41/40 [==============================] - 1s - loss: 0.2681 - acc: 0.8833 - val_loss: 0.3136 - val_acc: 0.8349\n",
      "Epoch 27/80\n",
      "41/40 [==============================] - 1s - loss: 0.2651 - acc: 0.8856 - val_loss: 0.3266 - val_acc: 0.8255\n",
      "Epoch 28/80\n",
      "41/40 [==============================] - 1s - loss: 0.2686 - acc: 0.8864 - val_loss: 0.5979 - val_acc: 0.7508\n",
      "Epoch 29/80\n",
      "41/40 [==============================] - 1s - loss: 0.2695 - acc: 0.8772 - val_loss: 0.3213 - val_acc: 0.8598\n",
      "Epoch 30/80\n",
      "41/40 [==============================] - 1s - loss: 0.2832 - acc: 0.8647 - val_loss: 0.3271 - val_acc: 0.8879\n",
      "Epoch 31/80\n",
      "41/40 [==============================] - 1s - loss: 0.2612 - acc: 0.8772 - val_loss: 0.2914 - val_acc: 0.8785\n",
      "Epoch 32/80\n",
      "41/40 [==============================] - 1s - loss: 0.2701 - acc: 0.8803 - val_loss: 0.2839 - val_acc: 0.8660\n",
      "Epoch 33/80\n",
      "41/40 [==============================] - 1s - loss: 0.2759 - acc: 0.8792 - val_loss: 0.3233 - val_acc: 0.8287\n",
      "Epoch 34/80\n",
      "41/40 [==============================] - 1s - loss: 0.2564 - acc: 0.8864 - val_loss: 0.3375 - val_acc: 0.8318\n",
      "Epoch 35/80\n",
      "41/40 [==============================] - 2s - loss: 0.2520 - acc: 0.8940 - val_loss: 0.2860 - val_acc: 0.9034\n",
      "Epoch 36/80\n",
      "41/40 [==============================] - 2s - loss: 0.2505 - acc: 0.8887 - val_loss: 0.2887 - val_acc: 0.8723\n",
      "Epoch 37/80\n",
      "41/40 [==============================] - ETA: 0s - loss: 0.2554 - acc: 0.892 - 2s - loss: 0.2604 - acc: 0.8868 - val_loss: 0.3064 - val_acc: 0.8474\n",
      "Epoch 38/80\n",
      "41/40 [==============================] - 2s - loss: 0.2507 - acc: 0.8932 - val_loss: 0.3199 - val_acc: 0.8287\n",
      "Epoch 39/80\n",
      "41/40 [==============================] - 1s - loss: 0.2453 - acc: 0.8720 - val_loss: 0.3339 - val_acc: 0.8224\n",
      "Epoch 40/80\n",
      "41/40 [==============================] - 1s - loss: 0.2399 - acc: 0.8810 - val_loss: 0.2791 - val_acc: 0.8723\n",
      "Epoch 41/80\n",
      "41/40 [==============================] - 1s - loss: 0.2560 - acc: 0.8940 - val_loss: 0.3305 - val_acc: 0.8318\n",
      "Epoch 42/80\n",
      "41/40 [==============================] - 1s - loss: 0.2507 - acc: 0.8784 - val_loss: 0.3514 - val_acc: 0.8380\n",
      "Epoch 43/80\n",
      "41/40 [==============================] - 1s - loss: 0.2486 - acc: 0.8914 - val_loss: 0.3244 - val_acc: 0.8255\n",
      "Epoch 44/80\n",
      "41/40 [==============================] - 1s - loss: 0.2945 - acc: 0.8720 - val_loss: 0.3020 - val_acc: 0.8785\n",
      "Epoch 45/80\n",
      "41/40 [==============================] - 1s - loss: 0.2661 - acc: 0.8784 - val_loss: 0.2830 - val_acc: 0.8910\n",
      "Epoch 46/80\n",
      "41/40 [==============================] - 1s - loss: 0.2465 - acc: 0.8899 - val_loss: 0.2681 - val_acc: 0.9065\n",
      "Epoch 47/80\n",
      "41/40 [==============================] - 1s - loss: 0.2640 - acc: 0.8853 - val_loss: 0.2687 - val_acc: 0.9034\n",
      "Epoch 48/80\n",
      "41/40 [==============================] - 1s - loss: 0.2442 - acc: 0.8940 - val_loss: 0.3094 - val_acc: 0.8536\n",
      "Epoch 49/80\n",
      "41/40 [==============================] - 1s - loss: 0.2351 - acc: 0.8906 - val_loss: 0.2844 - val_acc: 0.8785\n",
      "Epoch 50/80\n",
      "41/40 [==============================] - 1s - loss: 0.2260 - acc: 0.8955 - val_loss: 0.3268 - val_acc: 0.8224\n",
      "Epoch 51/80\n",
      "41/40 [==============================] - 1s - loss: 0.2261 - acc: 0.8963 - val_loss: 0.2659 - val_acc: 0.8723\n",
      "Epoch 52/80\n",
      "41/40 [==============================] - 1s - loss: 0.2250 - acc: 0.8986 - val_loss: 0.2510 - val_acc: 0.8972\n",
      "Epoch 53/80\n",
      "41/40 [==============================] - 1s - loss: 0.2515 - acc: 0.8952 - val_loss: 0.2468 - val_acc: 0.9065\n",
      "Epoch 54/80\n",
      "41/40 [==============================] - 1s - loss: 0.2284 - acc: 0.8952 - val_loss: 0.2487 - val_acc: 0.8910\n",
      "Epoch 55/80\n",
      "41/40 [==============================] - 2s - loss: 0.2403 - acc: 0.8952 - val_loss: 0.2450 - val_acc: 0.9128\n",
      "Epoch 56/80\n",
      "41/40 [==============================] - 1s - loss: 0.2228 - acc: 0.9085 - val_loss: 0.2820 - val_acc: 0.8442\n",
      "Epoch 57/80\n",
      "41/40 [==============================] - 1s - loss: 0.2554 - acc: 0.8834 - val_loss: 0.2530 - val_acc: 0.9065\n",
      "Epoch 58/80\n",
      "41/40 [==============================] - 1s - loss: 0.2385 - acc: 0.8978 - val_loss: 0.3022 - val_acc: 0.8598\n",
      "Epoch 59/80\n",
      "41/40 [==============================] - 1s - loss: 0.2409 - acc: 0.8865 - val_loss: 0.2895 - val_acc: 0.8692\n",
      "Epoch 60/80\n",
      "41/40 [==============================] - 1s - loss: 0.2308 - acc: 0.9054 - val_loss: 0.2818 - val_acc: 0.8910\n",
      "Epoch 61/80\n",
      "41/40 [==============================] - 1s - loss: 0.2077 - acc: 0.9161 - val_loss: 0.2804 - val_acc: 0.8660\n",
      "Epoch 62/80\n",
      "41/40 [==============================] - 1s - loss: 0.2145 - acc: 0.9115 - val_loss: 0.2823 - val_acc: 0.8785\n",
      "Epoch 63/80\n",
      "41/40 [==============================] - 1s - loss: 0.1995 - acc: 0.9169 - val_loss: 0.3147 - val_acc: 0.8474\n",
      "Epoch 64/80\n",
      "41/40 [==============================] - 1s - loss: 0.2098 - acc: 0.9062 - val_loss: 0.3170 - val_acc: 0.8349\n",
      "Epoch 65/80\n",
      "41/40 [==============================] - 1s - loss: 0.2164 - acc: 0.9135 - val_loss: 0.2675 - val_acc: 0.8816\n",
      "Epoch 66/80\n",
      "41/40 [==============================] - 1s - loss: 0.2027 - acc: 0.9184 - val_loss: 0.3012 - val_acc: 0.8536\n",
      "Epoch 67/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/40 [==============================] - 1s - loss: 0.2101 - acc: 0.9154 - val_loss: 0.2652 - val_acc: 0.8723\n",
      "Epoch 68/80\n",
      "41/40 [==============================] - 1s - loss: 0.2309 - acc: 0.9013 - val_loss: 0.2785 - val_acc: 0.8816\n",
      "Epoch 69/80\n",
      "41/40 [==============================] - 1s - loss: 0.2206 - acc: 0.8998 - val_loss: 0.3268 - val_acc: 0.8442\n",
      "Epoch 70/80\n",
      "41/40 [==============================] - 1s - loss: 0.2037 - acc: 0.9192 - val_loss: 0.2695 - val_acc: 0.8910\n",
      "Epoch 71/80\n",
      "41/40 [==============================] - 1s - loss: 0.2034 - acc: 0.9154 - val_loss: 0.2699 - val_acc: 0.8598\n",
      "Epoch 72/80\n",
      "41/40 [==============================] - 1s - loss: 0.1921 - acc: 0.9143 - val_loss: 0.2451 - val_acc: 0.8816\n",
      "Epoch 73/80\n",
      "41/40 [==============================] - 1s - loss: 0.1869 - acc: 0.9192 - val_loss: 0.2462 - val_acc: 0.8723\n",
      "Epoch 74/80\n",
      "41/40 [==============================] - 1s - loss: 0.2050 - acc: 0.9108 - val_loss: 0.2789 - val_acc: 0.8785\n",
      "Epoch 75/80\n",
      "41/40 [==============================] - 1s - loss: 0.2224 - acc: 0.8975 - val_loss: 0.2637 - val_acc: 0.8723\n",
      "Epoch 76/80\n",
      "41/40 [==============================] - 1s - loss: 0.1958 - acc: 0.9192 - val_loss: 0.3020 - val_acc: 0.8598\n",
      "Epoch 77/80\n",
      "41/40 [==============================] - 1s - loss: 0.1946 - acc: 0.9192 - val_loss: 0.2414 - val_acc: 0.8879\n",
      "Epoch 78/80\n",
      "41/40 [==============================] - 1s - loss: 0.1958 - acc: 0.9230 - val_loss: 0.2604 - val_acc: 0.8785\n",
      "Epoch 79/80\n",
      "41/40 [==============================] - 1s - loss: 0.1961 - acc: 0.9222 - val_loss: 0.2757 - val_acc: 0.8629\n",
      "Epoch 80/80\n",
      "41/40 [==============================] - 1s - loss: 0.2092 - acc: 0.9196 - val_loss: 0.3514 - val_acc: 0.8442\n",
      "Batch Size: 32\n",
      "Epochs: 80\n",
      "Epoch 1/80\n",
      "41/40 [==============================] - 7s - loss: 0.6940 - acc: 0.5147 - val_loss: 0.6912 - val_acc: 0.6573\n",
      "Epoch 2/80\n",
      "41/40 [==============================] - 5s - loss: 0.6935 - acc: 0.5490 - val_loss: 0.7022 - val_acc: 0.3427\n",
      "Epoch 3/80\n",
      "41/40 [==============================] - 5s - loss: 0.6808 - acc: 0.5967 - val_loss: 0.6688 - val_acc: 0.4642\n",
      "Epoch 4/80\n",
      "41/40 [==============================] - 5s - loss: 0.6743 - acc: 0.6179 - val_loss: 0.6606 - val_acc: 0.6480\n",
      "Epoch 5/80\n",
      "41/40 [==============================] - 5s - loss: 0.6586 - acc: 0.6260 - val_loss: 0.7015 - val_acc: 0.4860\n",
      "Epoch 6/80\n",
      "41/40 [==============================] - 5s - loss: 0.5971 - acc: 0.6878 - val_loss: 0.8774 - val_acc: 0.6573\n",
      "Epoch 7/80\n",
      "41/40 [==============================] - 5s - loss: 0.6227 - acc: 0.6428 - val_loss: 0.5713 - val_acc: 0.6262\n",
      "Epoch 8/80\n",
      "41/40 [==============================] - 5s - loss: 0.5707 - acc: 0.6992 - val_loss: 0.7464 - val_acc: 0.5234\n",
      "Epoch 9/80\n",
      "41/40 [==============================] - 5s - loss: 0.5844 - acc: 0.6942 - val_loss: 0.6016 - val_acc: 0.6355\n",
      "Epoch 10/80\n",
      "41/40 [==============================] - 5s - loss: 0.5576 - acc: 0.7293 - val_loss: 0.6302 - val_acc: 0.6324\n",
      "Epoch 11/80\n",
      "41/40 [==============================] - 5s - loss: 0.5560 - acc: 0.7045 - val_loss: 0.6951 - val_acc: 0.5607\n",
      "Epoch 12/80\n",
      "41/40 [==============================] - 5s - loss: 0.5511 - acc: 0.7137 - val_loss: 0.6525 - val_acc: 0.6293\n",
      "Epoch 13/80\n",
      "41/40 [==============================] - 5s - loss: 0.5444 - acc: 0.7312 - val_loss: 0.6846 - val_acc: 0.5701\n",
      "Epoch 14/80\n",
      "41/40 [==============================] - 5s - loss: 0.5891 - acc: 0.6885 - val_loss: 0.5814 - val_acc: 0.6137\n",
      "Epoch 15/80\n",
      "41/40 [==============================] - 5s - loss: 0.5669 - acc: 0.6966 - val_loss: 0.5603 - val_acc: 0.6417\n",
      "Epoch 16/80\n",
      "41/40 [==============================] - 5s - loss: 0.5513 - acc: 0.7068 - val_loss: 0.5564 - val_acc: 0.6636\n",
      "Epoch 17/80\n",
      "41/40 [==============================] - 5s - loss: 0.5331 - acc: 0.7259 - val_loss: 0.5948 - val_acc: 0.6137\n",
      "Epoch 18/80\n",
      "41/40 [==============================] - 5s - loss: 0.5247 - acc: 0.7335 - val_loss: 0.6189 - val_acc: 0.6199\n",
      "Epoch 19/80\n",
      "41/40 [==============================] - 5s - loss: 0.5071 - acc: 0.7483 - val_loss: 0.5847 - val_acc: 0.6698\n",
      "Epoch 20/80\n",
      "41/40 [==============================] - 5s - loss: 0.4571 - acc: 0.7788 - val_loss: 0.5437 - val_acc: 0.6978\n",
      "Epoch 21/80\n",
      "41/40 [==============================] - 5s - loss: 0.4525 - acc: 0.7884 - val_loss: 0.5266 - val_acc: 0.7321\n",
      "Epoch 22/80\n",
      "41/40 [==============================] - 5s - loss: 0.4454 - acc: 0.7751 - val_loss: 0.4719 - val_acc: 0.7570\n",
      "Epoch 23/80\n",
      "41/40 [==============================] - 5s - loss: 0.4082 - acc: 0.8162 - val_loss: 0.4835 - val_acc: 0.7414\n",
      "Epoch 24/80\n",
      "41/40 [==============================] - 5s - loss: 0.4131 - acc: 0.7976 - val_loss: 0.4745 - val_acc: 0.7352\n",
      "Epoch 25/80\n",
      "41/40 [==============================] - 5s - loss: 0.3744 - acc: 0.8223 - val_loss: 0.4143 - val_acc: 0.8100\n",
      "Epoch 26/80\n",
      "41/40 [==============================] - 5s - loss: 0.3789 - acc: 0.8139 - val_loss: 0.4506 - val_acc: 0.7913\n",
      "Epoch 27/80\n",
      "41/40 [==============================] - 5s - loss: 0.3823 - acc: 0.8365 - val_loss: 0.4116 - val_acc: 0.8255\n",
      "Epoch 28/80\n",
      "41/40 [==============================] - 5s - loss: 0.4585 - acc: 0.8048 - val_loss: 0.4157 - val_acc: 0.7882\n",
      "Epoch 29/80\n",
      "41/40 [==============================] - 5s - loss: 0.3741 - acc: 0.8235 - val_loss: 0.3692 - val_acc: 0.8069\n",
      "Epoch 30/80\n",
      "41/40 [==============================] - 5s - loss: 0.3544 - acc: 0.8437 - val_loss: 0.4590 - val_acc: 0.7196\n",
      "Epoch 31/80\n",
      "41/40 [==============================] - 5s - loss: 0.3386 - acc: 0.8582 - val_loss: 0.4461 - val_acc: 0.7601\n",
      "Epoch 32/80\n",
      "41/40 [==============================] - 5s - loss: 0.3632 - acc: 0.8372 - val_loss: 0.3796 - val_acc: 0.7913\n",
      "Epoch 33/80\n",
      "41/40 [==============================] - 5s - loss: 0.3807 - acc: 0.8281 - val_loss: 0.3600 - val_acc: 0.8349\n",
      "Epoch 34/80\n",
      "41/40 [==============================] - 5s - loss: 0.3419 - acc: 0.8487 - val_loss: 0.3413 - val_acc: 0.8380\n",
      "Epoch 35/80\n",
      "41/40 [==============================] - 5s - loss: 0.3114 - acc: 0.8566 - val_loss: 0.3109 - val_acc: 0.8474\n",
      "Epoch 36/80\n",
      "41/40 [==============================] - 5s - loss: 0.3182 - acc: 0.8609 - val_loss: 0.3354 - val_acc: 0.8380\n",
      "Epoch 37/80\n",
      "41/40 [==============================] - 5s - loss: 0.3285 - acc: 0.8502 - val_loss: 0.4060 - val_acc: 0.8006\n",
      "Epoch 38/80\n",
      "41/40 [==============================] - 5s - loss: 0.3231 - acc: 0.8566 - val_loss: 0.4274 - val_acc: 0.8193\n",
      "Epoch 39/80\n",
      "41/40 [==============================] - 5s - loss: 0.3331 - acc: 0.8502 - val_loss: 0.3539 - val_acc: 0.8224\n",
      "Epoch 40/80\n",
      "41/40 [==============================] - 5s - loss: 0.3129 - acc: 0.8650 - val_loss: 0.3715 - val_acc: 0.8037\n",
      "Epoch 41/80\n",
      "41/40 [==============================] - 5s - loss: 0.3194 - acc: 0.8594 - val_loss: 0.3569 - val_acc: 0.8069\n",
      "Epoch 42/80\n",
      "41/40 [==============================] - 5s - loss: 0.3067 - acc: 0.8571 - val_loss: 0.3167 - val_acc: 0.8505\n",
      "Epoch 43/80\n",
      "41/40 [==============================] - 5s - loss: 0.3134 - acc: 0.8677 - val_loss: 0.3417 - val_acc: 0.8224\n",
      "Epoch 44/80\n",
      "41/40 [==============================] - 5s - loss: 0.3425 - acc: 0.8555 - val_loss: 0.3252 - val_acc: 0.8287\n",
      "Epoch 45/80\n",
      "41/40 [==============================] - 5s - loss: 0.3381 - acc: 0.8502 - val_loss: 0.3264 - val_acc: 0.8505\n",
      "Epoch 46/80\n",
      "41/40 [==============================] - 5s - loss: 0.3216 - acc: 0.8632 - val_loss: 0.3425 - val_acc: 0.8318\n",
      "Epoch 47/80\n",
      "41/40 [==============================] - 5s - loss: 0.3259 - acc: 0.8522 - val_loss: 0.3034 - val_acc: 0.8598\n",
      "Epoch 48/80\n",
      "41/40 [==============================] - 5s - loss: 0.3028 - acc: 0.8627 - val_loss: 0.3930 - val_acc: 0.7944\n",
      "Epoch 49/80\n",
      "41/40 [==============================] - 5s - loss: 0.3232 - acc: 0.8482 - val_loss: 0.3140 - val_acc: 0.8411\n",
      "Epoch 50/80\n",
      "41/40 [==============================] - 5s - loss: 0.2928 - acc: 0.8726 - val_loss: 0.3433 - val_acc: 0.8380\n",
      "Epoch 51/80\n",
      "41/40 [==============================] - 5s - loss: 0.3066 - acc: 0.8536 - val_loss: 0.3148 - val_acc: 0.8349\n",
      "Epoch 52/80\n",
      "41/40 [==============================] - 5s - loss: 0.2914 - acc: 0.8650 - val_loss: 0.3499 - val_acc: 0.8224\n",
      "Epoch 53/80\n",
      "41/40 [==============================] - 5s - loss: 0.2972 - acc: 0.8624 - val_loss: 0.3341 - val_acc: 0.8318\n",
      "Epoch 54/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/40 [==============================] - 5s - loss: 0.2909 - acc: 0.8612 - val_loss: 0.3454 - val_acc: 0.8162\n",
      "Epoch 55/80\n",
      "41/40 [==============================] - 5s - loss: 0.2967 - acc: 0.8552 - val_loss: 0.3609 - val_acc: 0.8287\n",
      "Epoch 56/80\n",
      "41/40 [==============================] - 5s - loss: 0.3260 - acc: 0.8559 - val_loss: 0.3405 - val_acc: 0.8536\n",
      "Epoch 57/80\n",
      "41/40 [==============================] - 5s - loss: 0.3104 - acc: 0.8677 - val_loss: 0.3542 - val_acc: 0.8255\n",
      "Epoch 58/80\n",
      "41/40 [==============================] - 5s - loss: 0.3120 - acc: 0.8673 - val_loss: 0.3484 - val_acc: 0.8255\n",
      "Epoch 59/80\n",
      "41/40 [==============================] - 5s - loss: 0.2899 - acc: 0.8719 - val_loss: 0.3601 - val_acc: 0.8349\n",
      "Epoch 60/80\n",
      "41/40 [==============================] - 5s - loss: 0.2952 - acc: 0.8670 - val_loss: 0.3163 - val_acc: 0.8629\n",
      "Epoch 61/80\n",
      "41/40 [==============================] - 5s - loss: 0.2854 - acc: 0.8792 - val_loss: 0.3623 - val_acc: 0.8380\n",
      "Epoch 62/80\n",
      "41/40 [==============================] - 5s - loss: 0.2911 - acc: 0.8734 - val_loss: 0.3231 - val_acc: 0.8411\n",
      "Epoch 63/80\n",
      "41/40 [==============================] - 5s - loss: 0.2902 - acc: 0.8662 - val_loss: 0.3138 - val_acc: 0.8505\n",
      "Epoch 64/80\n",
      "41/40 [==============================] - 5s - loss: 0.2927 - acc: 0.8681 - val_loss: 0.3567 - val_acc: 0.8069\n",
      "Epoch 65/80\n",
      "41/40 [==============================] - 5s - loss: 0.2709 - acc: 0.8807 - val_loss: 0.3221 - val_acc: 0.8349\n",
      "Epoch 66/80\n",
      "41/40 [==============================] - 5s - loss: 0.2793 - acc: 0.8742 - val_loss: 0.2861 - val_acc: 0.8660\n",
      "Epoch 67/80\n",
      "41/40 [==============================] - 5s - loss: 0.2939 - acc: 0.8670 - val_loss: 0.3039 - val_acc: 0.8536\n",
      "Epoch 68/80\n",
      "41/40 [==============================] - 5s - loss: 0.2857 - acc: 0.8632 - val_loss: 0.3207 - val_acc: 0.8162\n",
      "Epoch 69/80\n",
      "41/40 [==============================] - 5s - loss: 0.2676 - acc: 0.8879 - val_loss: 0.2889 - val_acc: 0.8692\n",
      "Epoch 70/80\n",
      "41/40 [==============================] - 5s - loss: 0.2641 - acc: 0.8887 - val_loss: 0.4650 - val_acc: 0.7570\n",
      "Epoch 71/80\n",
      "41/40 [==============================] - 5s - loss: 0.2860 - acc: 0.8795 - val_loss: 0.3310 - val_acc: 0.8349\n",
      "Epoch 72/80\n",
      "41/40 [==============================] - 5s - loss: 0.2554 - acc: 0.9001 - val_loss: 0.3061 - val_acc: 0.8723\n",
      "Epoch 73/80\n",
      "41/40 [==============================] - 5s - loss: 0.2694 - acc: 0.8803 - val_loss: 0.3180 - val_acc: 0.8598\n",
      "Epoch 74/80\n",
      "41/40 [==============================] - 5s - loss: 0.2643 - acc: 0.8921 - val_loss: 0.3279 - val_acc: 0.8505\n",
      "Epoch 75/80\n",
      "41/40 [==============================] - 5s - loss: 0.2745 - acc: 0.8784 - val_loss: 0.3745 - val_acc: 0.8505\n",
      "Epoch 76/80\n",
      "41/40 [==============================] - 5s - loss: 0.2799 - acc: 0.8757 - val_loss: 0.3026 - val_acc: 0.8879\n",
      "Epoch 77/80\n",
      "41/40 [==============================] - 5s - loss: 0.2964 - acc: 0.8804 - val_loss: 0.3588 - val_acc: 0.8287\n",
      "Epoch 78/80\n",
      "41/40 [==============================] - 5s - loss: 0.2848 - acc: 0.8757 - val_loss: 0.3132 - val_acc: 0.8629\n",
      "Epoch 79/80\n",
      "41/40 [==============================] - 5s - loss: 0.2613 - acc: 0.8899 - val_loss: 0.3228 - val_acc: 0.8505\n",
      "Epoch 80/80\n",
      "41/40 [==============================] - 5s - loss: 0.2601 - acc: 0.8887 - val_loss: 0.3289 - val_acc: 0.8567\n",
      "Batch Size: 32\n",
      "Epochs: 80\n",
      "Epoch 1/80\n",
      "41/40 [==============================] - 7s - loss: 0.6910 - acc: 0.5273 - val_loss: 0.6891 - val_acc: 0.6573\n",
      "Epoch 2/80\n",
      "41/40 [==============================] - 6s - loss: 0.6909 - acc: 0.5265 - val_loss: 0.6548 - val_acc: 0.6168\n",
      "Epoch 3/80\n",
      "41/40 [==============================] - 6s - loss: 0.6408 - acc: 0.5600 - val_loss: 0.6620 - val_acc: 0.4735\n",
      "Epoch 4/80\n",
      "41/40 [==============================] - 6s - loss: 0.6685 - acc: 0.6054 - val_loss: 0.7419 - val_acc: 0.3801\n",
      "Epoch 5/80\n",
      "41/40 [==============================] - 6s - loss: 0.6374 - acc: 0.6339 - val_loss: 0.6535 - val_acc: 0.5296\n",
      "Epoch 6/80\n",
      "41/40 [==============================] - 6s - loss: 0.6144 - acc: 0.6786 - val_loss: 0.6110 - val_acc: 0.6075\n",
      "Epoch 7/80\n",
      "41/40 [==============================] - 6s - loss: 0.6088 - acc: 0.6942 - val_loss: 0.5748 - val_acc: 0.6324\n",
      "Epoch 8/80\n",
      "41/40 [==============================] - 6s - loss: 0.5816 - acc: 0.6890 - val_loss: 0.7365 - val_acc: 0.4143\n",
      "Epoch 9/80\n",
      "41/40 [==============================] - 6s - loss: 0.5840 - acc: 0.6916 - val_loss: 0.6001 - val_acc: 0.6324\n",
      "Epoch 10/80\n",
      "41/40 [==============================] - 6s - loss: 0.5649 - acc: 0.7194 - val_loss: 0.6885 - val_acc: 0.5327\n",
      "Epoch 11/80\n",
      "41/40 [==============================] - 6s - loss: 0.5725 - acc: 0.6909 - val_loss: 0.6002 - val_acc: 0.6355\n",
      "Epoch 12/80\n",
      "41/40 [==============================] - 6s - loss: 0.5587 - acc: 0.7023 - val_loss: 0.5900 - val_acc: 0.6511\n",
      "Epoch 13/80\n",
      "41/40 [==============================] - 6s - loss: 0.5655 - acc: 0.7073 - val_loss: 0.7007 - val_acc: 0.5296\n",
      "Epoch 14/80\n",
      "41/40 [==============================] - 6s - loss: 0.5613 - acc: 0.7110 - val_loss: 0.6607 - val_acc: 0.6480\n",
      "Epoch 15/80\n",
      "41/40 [==============================] - 6s - loss: 0.5501 - acc: 0.7064 - val_loss: 0.6851 - val_acc: 0.5607\n",
      "Epoch 16/80\n",
      "41/40 [==============================] - 6s - loss: 0.5335 - acc: 0.7339 - val_loss: 0.6280 - val_acc: 0.6293\n",
      "Epoch 17/80\n",
      "41/40 [==============================] - 6s - loss: 0.5485 - acc: 0.7088 - val_loss: 0.6038 - val_acc: 0.6386\n",
      "Epoch 18/80\n",
      "41/40 [==============================] - 6s - loss: 0.5400 - acc: 0.7377 - val_loss: 0.6292 - val_acc: 0.6386\n",
      "Epoch 19/80\n",
      "41/40 [==============================] - 6s - loss: 0.5471 - acc: 0.7190 - val_loss: 0.5801 - val_acc: 0.6480\n",
      "Epoch 20/80\n",
      "41/40 [==============================] - 6s - loss: 0.5488 - acc: 0.7171 - val_loss: 0.6142 - val_acc: 0.6106\n",
      "Epoch 21/80\n",
      "41/40 [==============================] - 6s - loss: 0.5325 - acc: 0.7285 - val_loss: 0.6488 - val_acc: 0.6137\n",
      "Epoch 22/80\n",
      "41/40 [==============================] - 6s - loss: 0.5336 - acc: 0.7244 - val_loss: 0.6225 - val_acc: 0.6324\n",
      "Epoch 23/80\n",
      "41/40 [==============================] - 6s - loss: 0.5396 - acc: 0.7194 - val_loss: 0.6476 - val_acc: 0.6386\n",
      "Epoch 24/80\n",
      "41/40 [==============================] - 6s - loss: 0.5191 - acc: 0.7346 - val_loss: 0.5801 - val_acc: 0.6604\n",
      "Epoch 25/80\n",
      "41/40 [==============================] - 6s - loss: 0.5092 - acc: 0.7445 - val_loss: 0.5826 - val_acc: 0.6355\n",
      "Epoch 26/80\n",
      "41/40 [==============================] - 6s - loss: 0.5223 - acc: 0.7251 - val_loss: 0.6278 - val_acc: 0.6449\n",
      "Epoch 27/80\n",
      "41/40 [==============================] - 6s - loss: 0.4926 - acc: 0.7552 - val_loss: 0.5096 - val_acc: 0.7445\n",
      "Epoch 28/80\n",
      "41/40 [==============================] - 6s - loss: 0.4581 - acc: 0.7971 - val_loss: 0.5551 - val_acc: 0.6885\n",
      "Epoch 29/80\n",
      "41/40 [==============================] - 6s - loss: 0.4102 - acc: 0.8105 - val_loss: 0.3968 - val_acc: 0.8318\n",
      "Epoch 30/80\n",
      "41/40 [==============================] - 6s - loss: 0.3994 - acc: 0.8128 - val_loss: 0.4177 - val_acc: 0.8411\n",
      "Epoch 31/80\n",
      "41/40 [==============================] - 6s - loss: 0.3991 - acc: 0.8292 - val_loss: 0.5339 - val_acc: 0.7072\n",
      "Epoch 32/80\n",
      "41/40 [==============================] - 6s - loss: 0.3855 - acc: 0.8200 - val_loss: 0.3764 - val_acc: 0.8224\n",
      "Epoch 33/80\n",
      "41/40 [==============================] - 6s - loss: 0.3447 - acc: 0.8421 - val_loss: 0.4501 - val_acc: 0.7882\n",
      "Epoch 34/80\n",
      "41/40 [==============================] - 6s - loss: 0.3621 - acc: 0.8243 - val_loss: 0.4027 - val_acc: 0.7975\n",
      "Epoch 35/80\n",
      "41/40 [==============================] - 6s - loss: 0.3582 - acc: 0.8421 - val_loss: 0.4969 - val_acc: 0.7570\n",
      "Epoch 36/80\n",
      "41/40 [==============================] - 6s - loss: 0.3598 - acc: 0.8376 - val_loss: 0.3285 - val_acc: 0.8536\n",
      "Epoch 37/80\n",
      "41/40 [==============================] - 6s - loss: 0.3751 - acc: 0.8224 - val_loss: 0.3812 - val_acc: 0.8505\n",
      "Epoch 38/80\n",
      "41/40 [==============================] - 6s - loss: 0.3486 - acc: 0.8437 - val_loss: 0.3564 - val_acc: 0.8598\n",
      "Epoch 39/80\n",
      "41/40 [==============================] - 6s - loss: 0.3438 - acc: 0.8414 - val_loss: 0.4490 - val_acc: 0.7695\n",
      "Epoch 40/80\n",
      "41/40 [==============================] - 6s - loss: 0.3236 - acc: 0.8444 - val_loss: 0.3691 - val_acc: 0.8287\n",
      "Epoch 41/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/40 [==============================] - 6s - loss: 0.3233 - acc: 0.8566 - val_loss: 0.3841 - val_acc: 0.8349\n",
      "Epoch 42/80\n",
      "41/40 [==============================] - 6s - loss: 0.3071 - acc: 0.8566 - val_loss: 0.3410 - val_acc: 0.8567\n",
      "Epoch 43/80\n",
      "41/40 [==============================] - 6s - loss: 0.3089 - acc: 0.8574 - val_loss: 0.3901 - val_acc: 0.8442\n",
      "Epoch 44/80\n",
      "41/40 [==============================] - 6s - loss: 0.3071 - acc: 0.8662 - val_loss: 0.3982 - val_acc: 0.8100\n",
      "Epoch 45/80\n",
      "41/40 [==============================] - 6s - loss: 0.3240 - acc: 0.8521 - val_loss: 0.3164 - val_acc: 0.8505\n",
      "Epoch 46/80\n",
      "41/40 [==============================] - 6s - loss: 0.3270 - acc: 0.8467 - val_loss: 0.3741 - val_acc: 0.8224\n",
      "Epoch 47/80\n",
      "41/40 [==============================] - 6s - loss: 0.2887 - acc: 0.8787 - val_loss: 0.3579 - val_acc: 0.8598\n",
      "Epoch 48/80\n",
      "41/40 [==============================] - 6s - loss: 0.2982 - acc: 0.8693 - val_loss: 0.3766 - val_acc: 0.8380\n",
      "Epoch 49/80\n",
      "41/40 [==============================] - 6s - loss: 0.2815 - acc: 0.8738 - val_loss: 0.3800 - val_acc: 0.8380\n",
      "Epoch 50/80\n",
      "41/40 [==============================] - 6s - loss: 0.2898 - acc: 0.8810 - val_loss: 0.3377 - val_acc: 0.8754\n",
      "Epoch 51/80\n",
      "41/40 [==============================] - 6s - loss: 0.2850 - acc: 0.8761 - val_loss: 0.3377 - val_acc: 0.8754\n",
      "Epoch 52/80\n",
      "41/40 [==============================] - 6s - loss: 0.2867 - acc: 0.8681 - val_loss: 0.3845 - val_acc: 0.8567\n",
      "Epoch 53/80\n",
      "41/40 [==============================] - 6s - loss: 0.2768 - acc: 0.8803 - val_loss: 0.3260 - val_acc: 0.8879\n",
      "Epoch 54/80\n",
      "41/40 [==============================] - 6s - loss: 0.3379 - acc: 0.8433 - val_loss: 0.3358 - val_acc: 0.8224\n",
      "Epoch 55/80\n",
      "41/40 [==============================] - 6s - loss: 0.3039 - acc: 0.8780 - val_loss: 0.3258 - val_acc: 0.8442\n",
      "Epoch 56/80\n",
      "41/40 [==============================] - 6s - loss: 0.2702 - acc: 0.8894 - val_loss: 0.2896 - val_acc: 0.8692\n",
      "Epoch 57/80\n",
      "41/40 [==============================] - 6s - loss: 0.2714 - acc: 0.8780 - val_loss: 0.3259 - val_acc: 0.8723\n",
      "Epoch 58/80\n",
      "41/40 [==============================] - 6s - loss: 0.2919 - acc: 0.8749 - val_loss: 0.3250 - val_acc: 0.8754\n",
      "Epoch 59/80\n",
      "41/40 [==============================] - 6s - loss: 0.2678 - acc: 0.8708 - val_loss: 0.3063 - val_acc: 0.8536\n",
      "Epoch 60/80\n",
      "41/40 [==============================] - 6s - loss: 0.2733 - acc: 0.8826 - val_loss: 0.3284 - val_acc: 0.8660\n",
      "Epoch 61/80\n",
      "41/40 [==============================] - 6s - loss: 0.2685 - acc: 0.8818 - val_loss: 0.3017 - val_acc: 0.8879\n",
      "Epoch 62/80\n",
      "41/40 [==============================] - 6s - loss: 0.2585 - acc: 0.8963 - val_loss: 0.3075 - val_acc: 0.8847\n",
      "Epoch 63/80\n",
      "41/40 [==============================] - 6s - loss: 0.2656 - acc: 0.8787 - val_loss: 0.3743 - val_acc: 0.8287\n",
      "Epoch 64/80\n",
      "41/40 [==============================] - 6s - loss: 0.2656 - acc: 0.8841 - val_loss: 0.3432 - val_acc: 0.8660\n",
      "Epoch 65/80\n",
      "41/40 [==============================] - 6s - loss: 0.2548 - acc: 0.8826 - val_loss: 0.3125 - val_acc: 0.8879\n",
      "Epoch 66/80\n",
      "41/40 [==============================] - 6s - loss: 0.2436 - acc: 0.8986 - val_loss: 0.3340 - val_acc: 0.8847\n",
      "Epoch 67/80\n",
      "41/40 [==============================] - 6s - loss: 0.2431 - acc: 0.8937 - val_loss: 0.3477 - val_acc: 0.8754\n",
      "Epoch 68/80\n",
      "41/40 [==============================] - 6s - loss: 0.2309 - acc: 0.8986 - val_loss: 0.3178 - val_acc: 0.8879\n",
      "Epoch 69/80\n",
      "41/40 [==============================] - 6s - loss: 0.2488 - acc: 0.8940 - val_loss: 0.3260 - val_acc: 0.8941\n",
      "Epoch 70/80\n",
      "41/40 [==============================] - 6s - loss: 0.2561 - acc: 0.8940 - val_loss: 0.4346 - val_acc: 0.8193\n",
      "Epoch 71/80\n",
      "41/40 [==============================] - 6s - loss: 0.2664 - acc: 0.8769 - val_loss: 0.3809 - val_acc: 0.8411\n",
      "Epoch 72/80\n",
      "41/40 [==============================] - 6s - loss: 0.2462 - acc: 0.8993 - val_loss: 0.3178 - val_acc: 0.8816\n",
      "Epoch 73/80\n",
      "41/40 [==============================] - 6s - loss: 0.2427 - acc: 0.8970 - val_loss: 0.3241 - val_acc: 0.8879\n",
      "Epoch 74/80\n",
      "41/40 [==============================] - 6s - loss: 0.2288 - acc: 0.8955 - val_loss: 0.3602 - val_acc: 0.8567\n",
      "Epoch 75/80\n",
      "41/40 [==============================] - 6s - loss: 0.2754 - acc: 0.8841 - val_loss: 0.3620 - val_acc: 0.8505\n",
      "Epoch 76/80\n",
      "41/40 [==============================] - 6s - loss: 0.2314 - acc: 0.9031 - val_loss: 0.3178 - val_acc: 0.8692\n",
      "Epoch 77/80\n",
      "41/40 [==============================] - 6s - loss: 0.2321 - acc: 0.8975 - val_loss: 0.3276 - val_acc: 0.8785\n",
      "Epoch 78/80\n",
      "41/40 [==============================] - 6s - loss: 0.2122 - acc: 0.9123 - val_loss: 0.3494 - val_acc: 0.8847\n",
      "Epoch 79/80\n",
      "41/40 [==============================] - 6s - loss: 0.2409 - acc: 0.9016 - val_loss: 0.4523 - val_acc: 0.8442\n",
      "Epoch 80/80\n",
      "41/40 [==============================] - 6s - loss: 0.2203 - acc: 0.9047 - val_loss: 0.3103 - val_acc: 0.8910\n",
      "Batch Size: 32\n",
      "Epochs: 80\n",
      "Epoch 1/80\n",
      "41/40 [==============================] - 2s - loss: 0.6922 - acc: 0.5841 - val_loss: 0.7180 - val_acc: 0.5483\n",
      "Epoch 2/80\n",
      "41/40 [==============================] - 1s - loss: 0.6061 - acc: 0.6679 - val_loss: 0.6526 - val_acc: 0.5857\n",
      "Epoch 3/80\n",
      "41/40 [==============================] - 1s - loss: 0.5907 - acc: 0.6756 - val_loss: 0.7727 - val_acc: 0.5047\n",
      "Epoch 4/80\n",
      "41/40 [==============================] - 1s - loss: 0.6022 - acc: 0.6569 - val_loss: 0.7034 - val_acc: 0.5732\n",
      "Epoch 5/80\n",
      "41/40 [==============================] - 1s - loss: 0.5773 - acc: 0.6893 - val_loss: 0.5968 - val_acc: 0.6355\n",
      "Epoch 6/80\n",
      "41/40 [==============================] - 1s - loss: 0.5572 - acc: 0.7072 - val_loss: 0.7448 - val_acc: 0.5483\n",
      "Epoch 7/80\n",
      "41/40 [==============================] - 1s - loss: 0.5321 - acc: 0.7175 - val_loss: 0.5466 - val_acc: 0.6729\n",
      "Epoch 8/80\n",
      "41/40 [==============================] - 1s - loss: 0.5108 - acc: 0.7419 - val_loss: 0.5081 - val_acc: 0.7383\n",
      "Epoch 9/80\n",
      "41/40 [==============================] - 1s - loss: 0.4946 - acc: 0.7579 - val_loss: 0.5791 - val_acc: 0.6293\n",
      "Epoch 10/80\n",
      "41/40 [==============================] - 1s - loss: 0.5014 - acc: 0.7572 - val_loss: 0.5064 - val_acc: 0.7165\n",
      "Epoch 11/80\n",
      "41/40 [==============================] - 1s - loss: 0.4616 - acc: 0.7839 - val_loss: 0.4649 - val_acc: 0.7850\n",
      "Epoch 12/80\n",
      "41/40 [==============================] - 1s - loss: 0.4668 - acc: 0.7831 - val_loss: 0.5354 - val_acc: 0.6854\n",
      "Epoch 13/80\n",
      "41/40 [==============================] - 1s - loss: 0.4439 - acc: 0.7850 - val_loss: 0.4409 - val_acc: 0.7913\n",
      "Epoch 14/80\n",
      "41/40 [==============================] - 1s - loss: 0.4484 - acc: 0.7880 - val_loss: 0.5102 - val_acc: 0.7259\n",
      "Epoch 15/80\n",
      "41/40 [==============================] - 1s - loss: 0.4353 - acc: 0.8010 - val_loss: 0.4440 - val_acc: 0.8380\n",
      "Epoch 16/80\n",
      "41/40 [==============================] - 1s - loss: 0.4280 - acc: 0.7866 - val_loss: 0.4754 - val_acc: 0.7726\n",
      "Epoch 17/80\n",
      "41/40 [==============================] - 1s - loss: 0.4212 - acc: 0.7922 - val_loss: 0.4620 - val_acc: 0.7695\n",
      "Epoch 18/80\n",
      "41/40 [==============================] - 1s - loss: 0.4044 - acc: 0.8101 - val_loss: 0.4320 - val_acc: 0.8100\n",
      "Epoch 19/80\n",
      "41/40 [==============================] - 1s - loss: 0.4055 - acc: 0.8075 - val_loss: 0.3955 - val_acc: 0.8380\n",
      "Epoch 20/80\n",
      "41/40 [==============================] - 1s - loss: 0.3785 - acc: 0.8139 - val_loss: 0.4499 - val_acc: 0.7664\n",
      "Epoch 21/80\n",
      "41/40 [==============================] - 1s - loss: 0.4011 - acc: 0.8093 - val_loss: 0.4180 - val_acc: 0.8006\n",
      "Epoch 22/80\n",
      "41/40 [==============================] - 1s - loss: 0.3777 - acc: 0.8254 - val_loss: 0.3724 - val_acc: 0.8505\n",
      "Epoch 23/80\n",
      "41/40 [==============================] - 1s - loss: 0.3783 - acc: 0.8254 - val_loss: 0.3956 - val_acc: 0.8411\n",
      "Epoch 24/80\n",
      "41/40 [==============================] - 1s - loss: 0.4046 - acc: 0.8083 - val_loss: 0.4047 - val_acc: 0.8037\n",
      "Epoch 25/80\n",
      "41/40 [==============================] - 1s - loss: 0.3576 - acc: 0.8360 - val_loss: 0.4232 - val_acc: 0.7819\n",
      "Epoch 26/80\n",
      "41/40 [==============================] - 1s - loss: 0.3623 - acc: 0.8353 - val_loss: 0.3815 - val_acc: 0.8069\n",
      "Epoch 27/80\n",
      "41/40 [==============================] - 1s - loss: 0.3453 - acc: 0.8338 - val_loss: 0.4085 - val_acc: 0.7944\n",
      "Epoch 28/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/40 [==============================] - 1s - loss: 0.3500 - acc: 0.8376 - val_loss: 0.4136 - val_acc: 0.8006\n",
      "Epoch 29/80\n",
      "41/40 [==============================] - 1s - loss: 0.3803 - acc: 0.8194 - val_loss: 0.3728 - val_acc: 0.8474\n",
      "Epoch 30/80\n",
      "41/40 [==============================] - 1s - loss: 0.3566 - acc: 0.8449 - val_loss: 0.3691 - val_acc: 0.8318\n",
      "Epoch 31/80\n",
      "41/40 [==============================] - 1s - loss: 0.3463 - acc: 0.8353 - val_loss: 0.4032 - val_acc: 0.8037\n",
      "Epoch 32/80\n",
      "41/40 [==============================] - 1s - loss: 0.3319 - acc: 0.8460 - val_loss: 0.3827 - val_acc: 0.8349\n",
      "Epoch 33/80\n",
      "41/40 [==============================] - 1s - loss: 0.3398 - acc: 0.8452 - val_loss: 0.3817 - val_acc: 0.8224\n",
      "Epoch 34/80\n",
      "41/40 [==============================] - 1s - loss: 0.3332 - acc: 0.8449 - val_loss: 0.3900 - val_acc: 0.8131\n",
      "Epoch 35/80\n",
      "41/40 [==============================] - 1s - loss: 0.3432 - acc: 0.8505 - val_loss: 0.5680 - val_acc: 0.6885\n",
      "Epoch 36/80\n",
      "41/40 [==============================] - 1s - loss: 0.3542 - acc: 0.8383 - val_loss: 0.4873 - val_acc: 0.7352\n",
      "Epoch 37/80\n",
      "41/40 [==============================] - 1s - loss: 0.3510 - acc: 0.8395 - val_loss: 0.4562 - val_acc: 0.7601\n",
      "Epoch 38/80\n",
      "41/40 [==============================] - 1s - loss: 0.3844 - acc: 0.8216 - val_loss: 0.5149 - val_acc: 0.7165\n",
      "Epoch 39/80\n",
      "41/40 [==============================] - 1s - loss: 0.3316 - acc: 0.8513 - val_loss: 0.3613 - val_acc: 0.8318\n",
      "Epoch 40/80\n",
      "41/40 [==============================] - 1s - loss: 0.3271 - acc: 0.8388 - val_loss: 0.3437 - val_acc: 0.8411\n",
      "Epoch 41/80\n",
      "41/40 [==============================] - 1s - loss: 0.3218 - acc: 0.8380 - val_loss: 0.3908 - val_acc: 0.7944\n",
      "Epoch 42/80\n",
      "41/40 [==============================] - 1s - loss: 0.3173 - acc: 0.8426 - val_loss: 0.4644 - val_acc: 0.7477\n",
      "Epoch 43/80\n",
      "41/40 [==============================] - 1s - loss: 0.3211 - acc: 0.8589 - val_loss: 0.3469 - val_acc: 0.8567\n",
      "Epoch 44/80\n",
      "41/40 [==============================] - 1s - loss: 0.3048 - acc: 0.8604 - val_loss: 0.6943 - val_acc: 0.6355\n",
      "Epoch 45/80\n",
      "41/40 [==============================] - 1s - loss: 0.3336 - acc: 0.8498 - val_loss: 0.3483 - val_acc: 0.8567\n",
      "Epoch 46/80\n",
      "41/40 [==============================] - 1s - loss: 0.3149 - acc: 0.8533 - val_loss: 0.3992 - val_acc: 0.8037\n",
      "Epoch 47/80\n",
      "41/40 [==============================] - 1s - loss: 0.3108 - acc: 0.8635 - val_loss: 0.3953 - val_acc: 0.8131\n",
      "Epoch 48/80\n",
      "41/40 [==============================] - 1s - loss: 0.3292 - acc: 0.8482 - val_loss: 0.3237 - val_acc: 0.8567\n",
      "Epoch 49/80\n",
      "41/40 [==============================] - 1s - loss: 0.2977 - acc: 0.8696 - val_loss: 0.3748 - val_acc: 0.8069\n",
      "Epoch 50/80\n",
      "41/40 [==============================] - 1s - loss: 0.3099 - acc: 0.8536 - val_loss: 0.3553 - val_acc: 0.8318\n",
      "Epoch 51/80\n",
      "41/40 [==============================] - 1s - loss: 0.3179 - acc: 0.8510 - val_loss: 0.3853 - val_acc: 0.8100\n",
      "Epoch 52/80\n",
      "41/40 [==============================] - 1s - loss: 0.3422 - acc: 0.8330 - val_loss: 0.3340 - val_acc: 0.8505\n",
      "Epoch 53/80\n",
      "41/40 [==============================] - 1s - loss: 0.3088 - acc: 0.8528 - val_loss: 0.4038 - val_acc: 0.7913\n",
      "Epoch 54/80\n",
      "41/40 [==============================] - 1s - loss: 0.3068 - acc: 0.8559 - val_loss: 0.3738 - val_acc: 0.8442\n",
      "Epoch 55/80\n",
      "41/40 [==============================] - 1s - loss: 0.2868 - acc: 0.8627 - val_loss: 0.4228 - val_acc: 0.7757\n",
      "Epoch 56/80\n",
      "41/40 [==============================] - 1s - loss: 0.3050 - acc: 0.8681 - val_loss: 0.3809 - val_acc: 0.8100\n",
      "Epoch 57/80\n",
      "41/40 [==============================] - 1s - loss: 0.2962 - acc: 0.8627 - val_loss: 0.3455 - val_acc: 0.8505\n",
      "Epoch 58/80\n",
      "41/40 [==============================] - 1s - loss: 0.3352 - acc: 0.8360 - val_loss: 0.3908 - val_acc: 0.7726\n",
      "Epoch 59/80\n",
      "41/40 [==============================] - 1s - loss: 0.2957 - acc: 0.8681 - val_loss: 0.3854 - val_acc: 0.8193\n",
      "Epoch 60/80\n",
      "41/40 [==============================] - 1s - loss: 0.2966 - acc: 0.8586 - val_loss: 0.3574 - val_acc: 0.8505\n",
      "Epoch 61/80\n",
      "41/40 [==============================] - 1s - loss: 0.2977 - acc: 0.8604 - val_loss: 0.3479 - val_acc: 0.8536\n",
      "Epoch 62/80\n",
      "41/40 [==============================] - 1s - loss: 0.2925 - acc: 0.8662 - val_loss: 0.3370 - val_acc: 0.8629\n",
      "Epoch 63/80\n",
      "41/40 [==============================] - 1s - loss: 0.2774 - acc: 0.8780 - val_loss: 0.3440 - val_acc: 0.8411\n",
      "Epoch 64/80\n",
      "41/40 [==============================] - 1s - loss: 0.2960 - acc: 0.8627 - val_loss: 0.3761 - val_acc: 0.8380\n",
      "Epoch 65/80\n",
      "41/40 [==============================] - 1s - loss: 0.2880 - acc: 0.8700 - val_loss: 0.3467 - val_acc: 0.8505\n",
      "Epoch 66/80\n",
      "41/40 [==============================] - 1s - loss: 0.2831 - acc: 0.8780 - val_loss: 0.3109 - val_acc: 0.8785\n",
      "Epoch 67/80\n",
      "41/40 [==============================] - 1s - loss: 0.2905 - acc: 0.8635 - val_loss: 0.3402 - val_acc: 0.8785\n",
      "Epoch 68/80\n",
      "41/40 [==============================] - 1s - loss: 0.2740 - acc: 0.8716 - val_loss: 0.3618 - val_acc: 0.8287\n",
      "Epoch 69/80\n",
      "41/40 [==============================] - 1s - loss: 0.2853 - acc: 0.8688 - val_loss: 0.3805 - val_acc: 0.8255\n",
      "Epoch 70/80\n",
      "41/40 [==============================] - 1s - loss: 0.2953 - acc: 0.8704 - val_loss: 0.3772 - val_acc: 0.7913\n",
      "Epoch 71/80\n",
      "41/40 [==============================] - 1s - loss: 0.2869 - acc: 0.8681 - val_loss: 0.3501 - val_acc: 0.8255\n",
      "Epoch 72/80\n",
      "41/40 [==============================] - 1s - loss: 0.2940 - acc: 0.8696 - val_loss: 0.3624 - val_acc: 0.8318\n",
      "Epoch 73/80\n",
      "41/40 [==============================] - 1s - loss: 0.2814 - acc: 0.8810 - val_loss: 0.3515 - val_acc: 0.8287\n",
      "Epoch 74/80\n",
      "41/40 [==============================] - 1s - loss: 0.2784 - acc: 0.8826 - val_loss: 0.3189 - val_acc: 0.8847\n",
      "Epoch 75/80\n",
      "41/40 [==============================] - 1s - loss: 0.2716 - acc: 0.8696 - val_loss: 0.3366 - val_acc: 0.8910\n",
      "Epoch 76/80\n",
      "41/40 [==============================] - 1s - loss: 0.2923 - acc: 0.8685 - val_loss: 0.3401 - val_acc: 0.8536\n",
      "Epoch 77/80\n",
      "41/40 [==============================] - 1s - loss: 0.3035 - acc: 0.8589 - val_loss: 0.3146 - val_acc: 0.8910\n",
      "Epoch 78/80\n",
      "41/40 [==============================] - 1s - loss: 0.2839 - acc: 0.8738 - val_loss: 0.3583 - val_acc: 0.8474\n",
      "Epoch 79/80\n",
      "41/40 [==============================] - 1s - loss: 0.2743 - acc: 0.8677 - val_loss: 0.3264 - val_acc: 0.8754\n",
      "Epoch 80/80\n",
      "41/40 [==============================] - 1s - loss: 0.2802 - acc: 0.8723 - val_loss: 0.4294 - val_acc: 0.7726\n"
     ]
    }
   ],
   "source": [
    "trainRunner.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict = []\n",
    "X_test = helpers.get_images(test)\n",
    "\n",
    "for model in trainRunner.models:\n",
    "    predict.append(model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[  6.35758042e-01,   3.64241958e-01],\n",
       "        [  3.83053869e-01,   6.16946161e-01],\n",
       "        [  9.97210562e-01,   2.78946874e-03],\n",
       "        ..., \n",
       "        [  9.96540070e-01,   3.45997745e-03],\n",
       "        [  1.68584913e-04,   9.99831438e-01],\n",
       "        [  9.99880195e-01,   1.19770673e-04]], dtype=float32),\n",
       " array([[  9.32299376e-01,   6.77006245e-02],\n",
       "        [  1.04142636e-01,   8.95857394e-01],\n",
       "        [  1.00000000e+00,   1.05711100e-12],\n",
       "        ..., \n",
       "        [  9.87797320e-01,   1.22026354e-02],\n",
       "        [  1.09363720e-03,   9.98906374e-01],\n",
       "        [  9.91443098e-01,   8.55688658e-03]], dtype=float32),\n",
       " array([[  9.82888699e-01,   1.71112884e-02],\n",
       "        [  6.75934106e-02,   9.32406545e-01],\n",
       "        [  9.99994874e-01,   5.14015119e-06],\n",
       "        ..., \n",
       "        [  9.98232067e-01,   1.76796538e-03],\n",
       "        [  1.96016952e-02,   9.80398238e-01],\n",
       "        [  9.99751627e-01,   2.48362194e-04]], dtype=float32),\n",
       " array([[  5.05347669e-01,   4.94652271e-01],\n",
       "        [  2.61327207e-01,   7.38672853e-01],\n",
       "        [  1.00000000e+00,   1.69022060e-10],\n",
       "        ..., \n",
       "        [  5.01972973e-01,   4.98026997e-01],\n",
       "        [  3.38894734e-03,   9.96611059e-01],\n",
       "        [  9.83280540e-01,   1.67194828e-02]], dtype=float32)]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "        prediction = np.zeros((8424, ))\n",
    "        \n",
    "        for pred in predict:\n",
    "            prediction += pred[:, 1]\n",
    "            \n",
    "        prediction = prediction / len(predict)\n",
    "        \n",
    "        submission = pd.DataFrame(test, columns=[\"id\"])\n",
    "        \n",
    "        submission[\"is_iceberg\"] = prediction\n",
    "\n",
    "        test_func = lambda p: round(p[\"is_iceberg\"], 4)\n",
    "        submission[\"is_iceberg\"] = test_func(submission)\n",
    "        submission[\"is_iceberg\"] = submission[\"is_iceberg\"].round(4)\n",
    "        submission.to_csv(\"submission.csv\", float_format='%g', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainRunner = Trainer([\n",
    "    DaveModel(X, y, Xv, yv),\n",
    "    DaveVGG(X, y, Xv, yv),\n",
    "    DaveVGG19(X, y, Xv, yv),\n",
    "    LeNetModel(X, y, Xv, yv)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size: 32\n",
      "Epochs: 80\n",
      "Epoch 1/80\n",
      "51/50 [==============================] - 4s - loss: 0.5621 - acc: 0.6912 - val_loss: 0.6700 - val_acc: 0.6573\n",
      "Epoch 2/80\n",
      "51/50 [==============================] - 2s - loss: 0.4752 - acc: 0.7426 - val_loss: 0.6561 - val_acc: 0.6573\n",
      "Epoch 3/80\n",
      "51/50 [==============================] - 2s - loss: 0.4347 - acc: 0.7793 - val_loss: 0.6443 - val_acc: 0.6573\n",
      "Epoch 4/80\n",
      "51/50 [==============================] - 2s - loss: 0.3936 - acc: 0.7960 - val_loss: 0.6396 - val_acc: 0.6511\n",
      "Epoch 5/80\n",
      "51/50 [==============================] - 2s - loss: 0.3818 - acc: 0.8088 - val_loss: 0.6201 - val_acc: 0.6573\n",
      "Epoch 6/80\n",
      "51/50 [==============================] - 2s - loss: 0.3556 - acc: 0.8259 - val_loss: 0.5933 - val_acc: 0.6542\n",
      "Epoch 7/80\n",
      "51/50 [==============================] - 2s - loss: 0.3608 - acc: 0.8207 - val_loss: 0.5851 - val_acc: 0.6511\n",
      "Epoch 8/80\n",
      "51/50 [==============================] - 2s - loss: 0.3402 - acc: 0.8327 - val_loss: 0.5997 - val_acc: 0.6542\n",
      "Epoch 9/80\n",
      "51/50 [==============================] - 2s - loss: 0.3256 - acc: 0.8292 - val_loss: 0.4320 - val_acc: 0.7882\n",
      "Epoch 10/80\n",
      "51/50 [==============================] - 2s - loss: 0.3201 - acc: 0.8468 - val_loss: 0.3604 - val_acc: 0.8287\n",
      "Epoch 11/80\n",
      "51/50 [==============================] - 2s - loss: 0.3071 - acc: 0.8535 - val_loss: 0.3418 - val_acc: 0.8474\n",
      "Epoch 12/80\n",
      "51/50 [==============================] - 2s - loss: 0.3179 - acc: 0.8461 - val_loss: 0.3355 - val_acc: 0.8629\n",
      "Epoch 13/80\n",
      "51/50 [==============================] - 2s - loss: 0.3252 - acc: 0.8341 - val_loss: 0.3156 - val_acc: 0.8380\n",
      "Epoch 14/80\n",
      "51/50 [==============================] - 2s - loss: 0.3133 - acc: 0.8394 - val_loss: 0.3065 - val_acc: 0.8723\n",
      "Epoch 15/80\n",
      "51/50 [==============================] - 2s - loss: 0.2897 - acc: 0.8604 - val_loss: 0.2762 - val_acc: 0.8567\n",
      "Epoch 16/80\n",
      "51/50 [==============================] - 2s - loss: 0.2982 - acc: 0.8511 - val_loss: 0.2752 - val_acc: 0.8847\n",
      "Epoch 17/80\n",
      "51/50 [==============================] - 2s - loss: 0.2996 - acc: 0.8578 - val_loss: 0.2677 - val_acc: 0.9190\n",
      "Epoch 18/80\n",
      "51/50 [==============================] - 2s - loss: 0.2817 - acc: 0.8602 - val_loss: 0.2812 - val_acc: 0.9065\n",
      "Epoch 19/80\n",
      "51/50 [==============================] - 2s - loss: 0.2875 - acc: 0.8627 - val_loss: 0.2501 - val_acc: 0.8847\n",
      "Epoch 20/80\n",
      "51/50 [==============================] - 2s - loss: 0.3022 - acc: 0.8511 - val_loss: 0.2544 - val_acc: 0.8941\n",
      "Epoch 21/80\n",
      "51/50 [==============================] - 2s - loss: 0.2755 - acc: 0.8688 - val_loss: 0.2468 - val_acc: 0.9003\n",
      "Epoch 22/80\n",
      "51/50 [==============================] - 2s - loss: 0.2924 - acc: 0.8621 - val_loss: 0.2496 - val_acc: 0.9065\n",
      "Epoch 23/80\n",
      "51/50 [==============================] - 2s - loss: 0.2745 - acc: 0.8615 - val_loss: 0.2460 - val_acc: 0.9003\n",
      "Epoch 24/80\n",
      "51/50 [==============================] - 2s - loss: 0.2782 - acc: 0.8707 - val_loss: 0.2590 - val_acc: 0.8972\n",
      "Epoch 25/80\n",
      "51/50 [==============================] - 2s - loss: 0.2846 - acc: 0.8676 - val_loss: 0.2505 - val_acc: 0.8941\n",
      "Epoch 26/80\n",
      "51/50 [==============================] - 2s - loss: 0.2769 - acc: 0.8708 - val_loss: 0.2391 - val_acc: 0.9159\n",
      "Epoch 27/80\n",
      "51/50 [==============================] - 2s - loss: 0.2808 - acc: 0.8621 - val_loss: 0.2574 - val_acc: 0.8754\n",
      "Epoch 28/80\n",
      "51/50 [==============================] - 2s - loss: 0.2617 - acc: 0.8738 - val_loss: 0.2428 - val_acc: 0.9097\n",
      "Epoch 29/80\n",
      "51/50 [==============================] - 2s - loss: 0.2663 - acc: 0.8793 - val_loss: 0.2250 - val_acc: 0.9097\n",
      "Epoch 30/80\n",
      "51/50 [==============================] - 2s - loss: 0.2704 - acc: 0.8744 - val_loss: 0.2747 - val_acc: 0.8567\n",
      "Epoch 31/80\n",
      "51/50 [==============================] - 2s - loss: 0.2656 - acc: 0.8762 - val_loss: 0.2603 - val_acc: 0.8754\n",
      "Epoch 32/80\n",
      "51/50 [==============================] - 2s - loss: 0.2680 - acc: 0.8683 - val_loss: 0.2383 - val_acc: 0.9128\n",
      "Epoch 33/80\n",
      "51/50 [==============================] - 2s - loss: 0.2608 - acc: 0.8768 - val_loss: 0.2340 - val_acc: 0.8567\n",
      "Epoch 34/80\n",
      "51/50 [==============================] - 2s - loss: 0.2722 - acc: 0.8725 - val_loss: 0.2127 - val_acc: 0.9252\n",
      "Epoch 35/80\n",
      "51/50 [==============================] - 2s - loss: 0.2468 - acc: 0.8866 - val_loss: 0.2134 - val_acc: 0.9190\n",
      "Epoch 36/80\n",
      "51/50 [==============================] - 2s - loss: 0.2478 - acc: 0.8811 - val_loss: 0.1968 - val_acc: 0.9346\n",
      "Epoch 37/80\n",
      "51/50 [==============================] - 2s - loss: 0.2534 - acc: 0.8842 - val_loss: 0.1879 - val_acc: 0.9346\n",
      "Epoch 38/80\n",
      "51/50 [==============================] - 2s - loss: 0.2624 - acc: 0.8780 - val_loss: 0.2351 - val_acc: 0.9221\n",
      "Epoch 39/80\n",
      "51/50 [==============================] - 2s - loss: 0.2434 - acc: 0.8933 - val_loss: 0.2614 - val_acc: 0.8660\n",
      "Epoch 40/80\n",
      "51/50 [==============================] - 2s - loss: 0.2463 - acc: 0.8903 - val_loss: 0.2205 - val_acc: 0.8972\n",
      "Epoch 41/80\n",
      "51/50 [==============================] - 2s - loss: 0.2264 - acc: 0.8952 - val_loss: 0.2805 - val_acc: 0.8598\n",
      "Epoch 42/80\n",
      "51/50 [==============================] - 2s - loss: 0.2591 - acc: 0.8813 - val_loss: 0.2001 - val_acc: 0.9190\n",
      "Epoch 43/80\n",
      "51/50 [==============================] - 2s - loss: 0.2488 - acc: 0.8855 - val_loss: 0.2096 - val_acc: 0.9065\n",
      "Epoch 44/80\n",
      "51/50 [==============================] - 2s - loss: 0.2430 - acc: 0.8872 - val_loss: 0.2527 - val_acc: 0.9190\n",
      "Epoch 45/80\n",
      "51/50 [==============================] - 2s - loss: 0.2491 - acc: 0.8977 - val_loss: 0.1966 - val_acc: 0.9377\n",
      "Epoch 46/80\n",
      "51/50 [==============================] - 2s - loss: 0.2529 - acc: 0.8878 - val_loss: 0.1972 - val_acc: 0.9097\n",
      "Epoch 47/80\n",
      "51/50 [==============================] - 2s - loss: 0.2329 - acc: 0.8989 - val_loss: 0.1903 - val_acc: 0.9159\n",
      "Epoch 48/80\n",
      "51/50 [==============================] - 2s - loss: 0.2457 - acc: 0.8996 - val_loss: 0.2137 - val_acc: 0.9097\n",
      "Epoch 49/80\n",
      "51/50 [==============================] - 2s - loss: 0.2335 - acc: 0.8891 - val_loss: 0.2417 - val_acc: 0.8941\n",
      "Epoch 50/80\n",
      "51/50 [==============================] - 2s - loss: 0.2288 - acc: 0.8983 - val_loss: 0.1999 - val_acc: 0.9283\n",
      "Epoch 51/80\n",
      "51/50 [==============================] - 2s - loss: 0.2353 - acc: 0.8976 - val_loss: 0.2819 - val_acc: 0.8474\n",
      "Epoch 52/80\n",
      "51/50 [==============================] - 2s - loss: 0.2447 - acc: 0.8837 - val_loss: 0.1950 - val_acc: 0.9190\n",
      "Epoch 53/80\n",
      "51/50 [==============================] - 2s - loss: 0.2467 - acc: 0.8885 - val_loss: 0.1926 - val_acc: 0.9283\n",
      "Epoch 54/80\n",
      "51/50 [==============================] - 2s - loss: 0.2363 - acc: 0.8965 - val_loss: 0.1880 - val_acc: 0.9439\n",
      "Epoch 55/80\n",
      "51/50 [==============================] - 2s - loss: 0.2216 - acc: 0.9032 - val_loss: 0.1831 - val_acc: 0.9283\n",
      "Epoch 56/80\n",
      "51/50 [==============================] - 2s - loss: 0.2285 - acc: 0.8941 - val_loss: 0.1856 - val_acc: 0.9283\n",
      "Epoch 57/80\n",
      "51/50 [==============================] - 2s - loss: 0.2297 - acc: 0.8983 - val_loss: 0.2087 - val_acc: 0.9252\n",
      "Epoch 58/80\n",
      "51/50 [==============================] - 2s - loss: 0.2316 - acc: 0.9020 - val_loss: 0.2087 - val_acc: 0.9190\n",
      "Epoch 59/80\n",
      "51/50 [==============================] - 2s - loss: 0.2157 - acc: 0.9050 - val_loss: 0.1933 - val_acc: 0.9221\n",
      "Epoch 60/80\n",
      "51/50 [==============================] - 2s - loss: 0.2292 - acc: 0.8978 - val_loss: 0.1964 - val_acc: 0.9159\n",
      "Epoch 61/80\n",
      "51/50 [==============================] - 2s - loss: 0.2327 - acc: 0.8940 - val_loss: 0.2084 - val_acc: 0.8941\n",
      "Epoch 62/80\n",
      "51/50 [==============================] - 2s - loss: 0.2162 - acc: 0.8977 - val_loss: 0.1810 - val_acc: 0.9221\n",
      "Epoch 63/80\n",
      "51/50 [==============================] - 2s - loss: 0.2265 - acc: 0.8996 - val_loss: 0.1804 - val_acc: 0.9439\n",
      "Epoch 64/80\n",
      "51/50 [==============================] - 2s - loss: 0.2222 - acc: 0.9032 - val_loss: 0.2775 - val_acc: 0.8754\n",
      "Epoch 65/80\n",
      "51/50 [==============================] - 2s - loss: 0.2147 - acc: 0.9050 - val_loss: 0.2135 - val_acc: 0.9034\n",
      "Epoch 66/80\n",
      "51/50 [==============================] - 2s - loss: 0.1985 - acc: 0.9185 - val_loss: 0.1620 - val_acc: 0.9408\n",
      "Epoch 67/80\n",
      "51/50 [==============================] - 2s - loss: 0.2010 - acc: 0.9123 - val_loss: 0.2128 - val_acc: 0.8910\n",
      "Epoch 68/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/50 [==============================] - 2s - loss: 0.2108 - acc: 0.9081 - val_loss: 0.1693 - val_acc: 0.9315\n",
      "Epoch 69/80\n",
      "51/50 [==============================] - 2s - loss: 0.2122 - acc: 0.9179 - val_loss: 0.1918 - val_acc: 0.9034\n",
      "Epoch 70/80\n",
      "51/50 [==============================] - 2s - loss: 0.2087 - acc: 0.9117 - val_loss: 0.1670 - val_acc: 0.9221\n",
      "Epoch 71/80\n",
      "51/50 [==============================] - 2s - loss: 0.2092 - acc: 0.9112 - val_loss: 0.1524 - val_acc: 0.9470\n",
      "Epoch 72/80\n",
      "51/50 [==============================] - 2s - loss: 0.2028 - acc: 0.9118 - val_loss: 0.1548 - val_acc: 0.9408\n",
      "Epoch 73/80\n",
      "51/50 [==============================] - 2s - loss: 0.2126 - acc: 0.9063 - val_loss: 0.1588 - val_acc: 0.9377\n",
      "Epoch 74/80\n",
      "51/50 [==============================] - 2s - loss: 0.2002 - acc: 0.9228 - val_loss: 0.1720 - val_acc: 0.9346\n",
      "Epoch 75/80\n",
      "51/50 [==============================] - 2s - loss: 0.2020 - acc: 0.9112 - val_loss: 0.2119 - val_acc: 0.9003\n",
      "Epoch 76/80\n",
      "51/50 [==============================] - 2s - loss: 0.1915 - acc: 0.9209 - val_loss: 0.1554 - val_acc: 0.9315\n",
      "Epoch 77/80\n",
      "51/50 [==============================] - 2s - loss: 0.1952 - acc: 0.9215 - val_loss: 0.1732 - val_acc: 0.9283\n",
      "Epoch 78/80\n",
      "51/50 [==============================] - 2s - loss: 0.1889 - acc: 0.9252 - val_loss: 0.1868 - val_acc: 0.9128\n",
      "Epoch 79/80\n",
      "51/50 [==============================] - 2s - loss: 0.2118 - acc: 0.9112 - val_loss: 0.1874 - val_acc: 0.9252\n",
      "Epoch 80/80\n",
      "51/50 [==============================] - 2s - loss: 0.1929 - acc: 0.9197 - val_loss: 0.1597 - val_acc: 0.9315\n",
      "Batch Size: 32\n",
      "Epochs: 80\n",
      "Epoch 1/80\n",
      "51/50 [==============================] - 9s - loss: 0.6867 - acc: 0.5148 - val_loss: 0.6465 - val_acc: 0.6573\n",
      "Epoch 2/80\n",
      "51/50 [==============================] - 7s - loss: 0.6578 - acc: 0.5810 - val_loss: 0.6215 - val_acc: 0.4735\n",
      "Epoch 3/80\n",
      "51/50 [==============================] - 7s - loss: 0.6444 - acc: 0.5883 - val_loss: 0.6266 - val_acc: 0.4766\n",
      "Epoch 4/80\n",
      "51/50 [==============================] - 7s - loss: 0.6431 - acc: 0.5919 - val_loss: 0.6501 - val_acc: 0.4891\n",
      "Epoch 5/80\n",
      "51/50 [==============================] - 7s - loss: 0.6352 - acc: 0.6102 - val_loss: 0.6886 - val_acc: 0.5327\n",
      "Epoch 6/80\n",
      "51/50 [==============================] - 7s - loss: 0.6478 - acc: 0.6067 - val_loss: 0.6938 - val_acc: 0.4206\n",
      "Epoch 7/80\n",
      "51/50 [==============================] - 7s - loss: 0.6460 - acc: 0.6090 - val_loss: 0.8642 - val_acc: 0.4548\n",
      "Epoch 8/80\n",
      "51/50 [==============================] - 7s - loss: 0.6930 - acc: 0.5195 - val_loss: 0.7010 - val_acc: 0.3738\n",
      "Epoch 9/80\n",
      "51/50 [==============================] - 7s - loss: 0.6444 - acc: 0.6043 - val_loss: 0.5900 - val_acc: 0.6168\n",
      "Epoch 10/80\n",
      "51/50 [==============================] - 7s - loss: 0.6056 - acc: 0.6759 - val_loss: 0.7506 - val_acc: 0.5016\n",
      "Epoch 11/80\n",
      "51/50 [==============================] - 7s - loss: 0.6544 - acc: 0.5729 - val_loss: 0.6426 - val_acc: 0.5234\n",
      "Epoch 12/80\n",
      "51/50 [==============================] - 7s - loss: 0.6357 - acc: 0.6114 - val_loss: 0.6124 - val_acc: 0.6355\n",
      "Epoch 13/80\n",
      "51/50 [==============================] - 7s - loss: 0.5969 - acc: 0.6807 - val_loss: 0.5897 - val_acc: 0.6511\n",
      "Epoch 14/80\n",
      "51/50 [==============================] - 7s - loss: 0.5766 - acc: 0.6913 - val_loss: 0.5819 - val_acc: 0.6137\n",
      "Epoch 15/80\n",
      "51/50 [==============================] - 7s - loss: 0.5539 - acc: 0.7053 - val_loss: 0.6149 - val_acc: 0.6480\n",
      "Epoch 16/80\n",
      "51/50 [==============================] - 7s - loss: 0.5780 - acc: 0.6797 - val_loss: 0.5686 - val_acc: 0.6199\n",
      "Epoch 17/80\n",
      "51/50 [==============================] - 7s - loss: 0.5440 - acc: 0.7144 - val_loss: 0.5747 - val_acc: 0.6324\n",
      "Epoch 18/80\n",
      "51/50 [==============================] - 7s - loss: 0.5492 - acc: 0.7046 - val_loss: 0.5462 - val_acc: 0.6729\n",
      "Epoch 19/80\n",
      "51/50 [==============================] - 7s - loss: 0.5107 - acc: 0.7285 - val_loss: 0.6046 - val_acc: 0.7134\n",
      "Epoch 20/80\n",
      "51/50 [==============================] - 7s - loss: 0.5263 - acc: 0.7299 - val_loss: 0.4722 - val_acc: 0.8037\n",
      "Epoch 21/80\n",
      "51/50 [==============================] - 7s - loss: 0.4864 - acc: 0.7543 - val_loss: 0.5111 - val_acc: 0.7414\n",
      "Epoch 22/80\n",
      "51/50 [==============================] - 7s - loss: 0.4496 - acc: 0.7904 - val_loss: 0.5128 - val_acc: 0.7383\n",
      "Epoch 23/80\n",
      "51/50 [==============================] - 7s - loss: 0.4248 - acc: 0.7971 - val_loss: 0.5283 - val_acc: 0.6947\n",
      "Epoch 24/80\n",
      "51/50 [==============================] - 7s - loss: 0.3959 - acc: 0.8211 - val_loss: 0.4383 - val_acc: 0.7726\n",
      "Epoch 25/80\n",
      "51/50 [==============================] - 7s - loss: 0.3757 - acc: 0.8265 - val_loss: 0.3587 - val_acc: 0.8255\n",
      "Epoch 26/80\n",
      "51/50 [==============================] - 7s - loss: 0.3896 - acc: 0.8229 - val_loss: 0.3048 - val_acc: 0.8692\n",
      "Epoch 27/80\n",
      "51/50 [==============================] - 7s - loss: 0.3573 - acc: 0.8315 - val_loss: 0.3625 - val_acc: 0.8442\n",
      "Epoch 28/80\n",
      "51/50 [==============================] - 7s - loss: 0.3549 - acc: 0.8377 - val_loss: 0.4918 - val_acc: 0.7321\n",
      "Epoch 29/80\n",
      "51/50 [==============================] - 7s - loss: 0.3702 - acc: 0.8247 - val_loss: 0.3859 - val_acc: 0.8193\n",
      "Epoch 30/80\n",
      "51/50 [==============================] - 7s - loss: 0.3285 - acc: 0.8529 - val_loss: 0.3151 - val_acc: 0.8847\n",
      "Epoch 31/80\n",
      "51/50 [==============================] - 7s - loss: 0.3283 - acc: 0.8334 - val_loss: 0.3167 - val_acc: 0.8505\n",
      "Epoch 32/80\n",
      "51/50 [==============================] - 7s - loss: 0.3257 - acc: 0.8406 - val_loss: 0.3057 - val_acc: 0.8505\n",
      "Epoch 33/80\n",
      "51/50 [==============================] - 7s - loss: 0.3299 - acc: 0.8530 - val_loss: 0.4761 - val_acc: 0.7882\n",
      "Epoch 34/80\n",
      "51/50 [==============================] - 7s - loss: 0.3278 - acc: 0.8364 - val_loss: 0.3241 - val_acc: 0.8380\n",
      "Epoch 35/80\n",
      "51/50 [==============================] - 7s - loss: 0.3259 - acc: 0.8549 - val_loss: 0.2965 - val_acc: 0.8442\n",
      "Epoch 36/80\n",
      "51/50 [==============================] - 7s - loss: 0.3479 - acc: 0.8408 - val_loss: 0.2694 - val_acc: 0.8847\n",
      "Epoch 37/80\n",
      "51/50 [==============================] - 7s - loss: 0.3186 - acc: 0.8523 - val_loss: 0.2912 - val_acc: 0.8598\n",
      "Epoch 38/80\n",
      "51/50 [==============================] - 7s - loss: 0.2931 - acc: 0.8682 - val_loss: 0.2705 - val_acc: 0.8941\n",
      "Epoch 39/80\n",
      "51/50 [==============================] - 7s - loss: 0.2808 - acc: 0.8750 - val_loss: 0.2704 - val_acc: 0.8910\n",
      "Epoch 40/80\n",
      "51/50 [==============================] - 7s - loss: 0.2762 - acc: 0.8731 - val_loss: 0.3103 - val_acc: 0.8754\n",
      "Epoch 41/80\n",
      "51/50 [==============================] - 7s - loss: 0.2803 - acc: 0.8799 - val_loss: 0.2926 - val_acc: 0.8847\n",
      "Epoch 42/80\n",
      "51/50 [==============================] - 7s - loss: 0.2772 - acc: 0.8781 - val_loss: 0.2544 - val_acc: 0.8972\n",
      "Epoch 43/80\n",
      "51/50 [==============================] - 7s - loss: 0.2717 - acc: 0.8775 - val_loss: 0.2595 - val_acc: 0.8972\n",
      "Epoch 44/80\n",
      "51/50 [==============================] - 7s - loss: 0.3115 - acc: 0.8584 - val_loss: 0.3006 - val_acc: 0.8723\n",
      "Epoch 45/80\n",
      "51/50 [==============================] - 7s - loss: 0.2552 - acc: 0.8878 - val_loss: 0.2367 - val_acc: 0.9065\n",
      "Epoch 46/80\n",
      "51/50 [==============================] - 7s - loss: 0.2713 - acc: 0.8762 - val_loss: 0.2779 - val_acc: 0.8941\n",
      "Epoch 47/80\n",
      "51/50 [==============================] - 7s - loss: 0.2627 - acc: 0.8830 - val_loss: 0.2641 - val_acc: 0.8910\n",
      "Epoch 48/80\n",
      "51/50 [==============================] - 7s - loss: 0.2742 - acc: 0.8774 - val_loss: 0.2237 - val_acc: 0.9003\n",
      "Epoch 49/80\n",
      "51/50 [==============================] - 7s - loss: 0.2539 - acc: 0.8909 - val_loss: 0.2210 - val_acc: 0.9034\n",
      "Epoch 50/80\n",
      "51/50 [==============================] - 7s - loss: 0.2687 - acc: 0.8854 - val_loss: 0.2177 - val_acc: 0.9065\n",
      "Epoch 51/80\n",
      "51/50 [==============================] - 7s - loss: 0.2523 - acc: 0.8898 - val_loss: 0.2147 - val_acc: 0.9034\n",
      "Epoch 52/80\n",
      "51/50 [==============================] - 7s - loss: 0.2621 - acc: 0.8775 - val_loss: 0.2443 - val_acc: 0.8941\n",
      "Epoch 53/80\n",
      "51/50 [==============================] - 7s - loss: 0.2689 - acc: 0.8824 - val_loss: 0.2440 - val_acc: 0.9190\n",
      "Epoch 54/80\n",
      "51/50 [==============================] - 7s - loss: 0.2568 - acc: 0.8922 - val_loss: 0.2068 - val_acc: 0.9221\n",
      "Epoch 55/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/50 [==============================] - 7s - loss: 0.2716 - acc: 0.8776 - val_loss: 0.2491 - val_acc: 0.8972\n",
      "Epoch 56/80\n",
      "51/50 [==============================] - 7s - loss: 0.2469 - acc: 0.8854 - val_loss: 0.2477 - val_acc: 0.8910\n",
      "Epoch 57/80\n",
      "51/50 [==============================] - 7s - loss: 0.2359 - acc: 0.9007 - val_loss: 0.2057 - val_acc: 0.9190\n",
      "Epoch 58/80\n",
      "51/50 [==============================] - 7s - loss: 0.2180 - acc: 0.9074 - val_loss: 0.2119 - val_acc: 0.9097\n",
      "Epoch 59/80\n",
      "51/50 [==============================] - 7s - loss: 0.2628 - acc: 0.8928 - val_loss: 0.2712 - val_acc: 0.8816\n",
      "Epoch 60/80\n",
      "51/50 [==============================] - 7s - loss: 0.3141 - acc: 0.8683 - val_loss: 0.2630 - val_acc: 0.8879\n",
      "Epoch 61/80\n",
      "51/50 [==============================] - 7s - loss: 0.2554 - acc: 0.8884 - val_loss: 0.2063 - val_acc: 0.9003\n",
      "Epoch 62/80\n",
      "51/50 [==============================] - 7s - loss: 0.2443 - acc: 0.8989 - val_loss: 0.2052 - val_acc: 0.9034\n",
      "Epoch 63/80\n",
      "51/50 [==============================] - 7s - loss: 0.2315 - acc: 0.9013 - val_loss: 0.2091 - val_acc: 0.9097\n",
      "Epoch 64/80\n",
      "51/50 [==============================] - 7s - loss: 0.2315 - acc: 0.9025 - val_loss: 0.2213 - val_acc: 0.8941\n",
      "Epoch 65/80\n",
      "51/50 [==============================] - 7s - loss: 0.2227 - acc: 0.8989 - val_loss: 0.2070 - val_acc: 0.9221\n",
      "Epoch 66/80\n",
      "51/50 [==============================] - 7s - loss: 0.2347 - acc: 0.9038 - val_loss: 0.1870 - val_acc: 0.9221\n",
      "Epoch 67/80\n",
      "51/50 [==============================] - 7s - loss: 0.2276 - acc: 0.9001 - val_loss: 0.2053 - val_acc: 0.9065\n",
      "Epoch 68/80\n",
      "51/50 [==============================] - 7s - loss: 0.2347 - acc: 0.8989 - val_loss: 0.2131 - val_acc: 0.9221\n",
      "Epoch 69/80\n",
      "51/50 [==============================] - 7s - loss: 0.2145 - acc: 0.9081 - val_loss: 0.1764 - val_acc: 0.9346\n",
      "Epoch 70/80\n",
      "51/50 [==============================] - 7s - loss: 0.2214 - acc: 0.9044 - val_loss: 0.1998 - val_acc: 0.9221\n",
      "Epoch 71/80\n",
      "51/50 [==============================] - 7s - loss: 0.2461 - acc: 0.8983 - val_loss: 0.1898 - val_acc: 0.9252\n",
      "Epoch 72/80\n",
      "51/50 [==============================] - 7s - loss: 0.2403 - acc: 0.9044 - val_loss: 0.1775 - val_acc: 0.9377\n",
      "Epoch 73/80\n",
      "51/50 [==============================] - 7s - loss: 0.2257 - acc: 0.9081 - val_loss: 0.2145 - val_acc: 0.9159\n",
      "Epoch 74/80\n",
      "51/50 [==============================] - 7s - loss: 0.2412 - acc: 0.8965 - val_loss: 0.1959 - val_acc: 0.9252\n",
      "Epoch 75/80\n",
      "51/50 [==============================] - 7s - loss: 0.2357 - acc: 0.9044 - val_loss: 0.2041 - val_acc: 0.9128\n",
      "Epoch 76/80\n",
      "51/50 [==============================] - 7s - loss: 0.2168 - acc: 0.9087 - val_loss: 0.1977 - val_acc: 0.9128\n",
      "Epoch 77/80\n",
      "51/50 [==============================] - 7s - loss: 0.2190 - acc: 0.9074 - val_loss: 0.2048 - val_acc: 0.9128\n",
      "Epoch 78/80\n",
      "51/50 [==============================] - 7s - loss: 0.2244 - acc: 0.9056 - val_loss: 0.2435 - val_acc: 0.8847\n",
      "Epoch 79/80\n",
      "51/50 [==============================] - 7s - loss: 0.2144 - acc: 0.9075 - val_loss: 0.1868 - val_acc: 0.9190\n",
      "Epoch 80/80\n",
      "51/50 [==============================] - 7s - loss: 0.2455 - acc: 0.8921 - val_loss: 0.2333 - val_acc: 0.8847\n",
      "Batch Size: 32\n",
      "Epochs: 80\n",
      "Epoch 1/80\n",
      "51/50 [==============================] - 9s - loss: 0.6569 - acc: 0.5520 - val_loss: 0.6152 - val_acc: 0.5140\n",
      "Epoch 2/80\n",
      "51/50 [==============================] - 7s - loss: 0.6300 - acc: 0.6365 - val_loss: 0.6641 - val_acc: 0.5171\n",
      "Epoch 3/80\n",
      "51/50 [==============================] - 7s - loss: 0.6309 - acc: 0.6447 - val_loss: 0.6471 - val_acc: 0.6012\n",
      "Epoch 4/80\n",
      "51/50 [==============================] - 7s - loss: 0.6694 - acc: 0.5478 - val_loss: 0.6196 - val_acc: 0.6573\n",
      "Epoch 5/80\n",
      "51/50 [==============================] - 7s - loss: 0.6248 - acc: 0.6255 - val_loss: 0.5702 - val_acc: 0.6386\n",
      "Epoch 6/80\n",
      "51/50 [==============================] - 7s - loss: 0.5854 - acc: 0.6760 - val_loss: 0.5600 - val_acc: 0.6511\n",
      "Epoch 7/80\n",
      "51/50 [==============================] - 7s - loss: 0.6044 - acc: 0.6477 - val_loss: 0.5505 - val_acc: 0.6293\n",
      "Epoch 8/80\n",
      "51/50 [==============================] - 7s - loss: 0.5839 - acc: 0.6912 - val_loss: 0.6322 - val_acc: 0.6012\n",
      "Epoch 9/80\n",
      "51/50 [==============================] - 7s - loss: 0.6050 - acc: 0.6728 - val_loss: 0.5650 - val_acc: 0.6480\n",
      "Epoch 10/80\n",
      "51/50 [==============================] - 7s - loss: 0.5751 - acc: 0.6979 - val_loss: 0.5710 - val_acc: 0.6542\n",
      "Epoch 11/80\n",
      "51/50 [==============================] - 7s - loss: 0.5672 - acc: 0.6919 - val_loss: 0.5708 - val_acc: 0.6324\n",
      "Epoch 12/80\n",
      "51/50 [==============================] - 7s - loss: 0.5594 - acc: 0.6966 - val_loss: 0.5658 - val_acc: 0.6417\n",
      "Epoch 13/80\n",
      "51/50 [==============================] - 7s - loss: 0.5710 - acc: 0.6938 - val_loss: 0.6252 - val_acc: 0.6044\n",
      "Epoch 14/80\n",
      "51/50 [==============================] - 7s - loss: 0.5555 - acc: 0.7010 - val_loss: 0.5611 - val_acc: 0.6511\n",
      "Epoch 15/80\n",
      "51/50 [==============================] - 7s - loss: 0.5695 - acc: 0.6881 - val_loss: 0.5530 - val_acc: 0.6480\n",
      "Epoch 16/80\n",
      "51/50 [==============================] - 7s - loss: 0.5574 - acc: 0.7082 - val_loss: 0.6539 - val_acc: 0.5607\n",
      "Epoch 17/80\n",
      "51/50 [==============================] - 7s - loss: 0.5570 - acc: 0.7010 - val_loss: 0.5566 - val_acc: 0.6729\n",
      "Epoch 18/80\n",
      "51/50 [==============================] - 7s - loss: 0.5345 - acc: 0.7229 - val_loss: 0.6637 - val_acc: 0.6355\n",
      "Epoch 19/80\n",
      "51/50 [==============================] - 7s - loss: 0.5561 - acc: 0.7131 - val_loss: 0.5876 - val_acc: 0.6168\n",
      "Epoch 20/80\n",
      "51/50 [==============================] - 7s - loss: 0.5384 - acc: 0.7278 - val_loss: 0.5507 - val_acc: 0.6604\n",
      "Epoch 21/80\n",
      "51/50 [==============================] - 7s - loss: 0.5195 - acc: 0.7347 - val_loss: 0.5984 - val_acc: 0.5981\n",
      "Epoch 22/80\n",
      "51/50 [==============================] - 7s - loss: 0.5449 - acc: 0.7206 - val_loss: 0.6076 - val_acc: 0.6168\n",
      "Epoch 23/80\n",
      "51/50 [==============================] - 7s - loss: 0.4964 - acc: 0.7574 - val_loss: 0.4990 - val_acc: 0.7414\n",
      "Epoch 24/80\n",
      "51/50 [==============================] - 7s - loss: 0.4506 - acc: 0.7825 - val_loss: 0.3780 - val_acc: 0.8131\n",
      "Epoch 25/80\n",
      "51/50 [==============================] - 7s - loss: 0.4591 - acc: 0.7910 - val_loss: 0.4632 - val_acc: 0.7944\n",
      "Epoch 26/80\n",
      "51/50 [==============================] - 7s - loss: 0.4480 - acc: 0.7931 - val_loss: 0.4294 - val_acc: 0.7632\n",
      "Epoch 27/80\n",
      "51/50 [==============================] - 7s - loss: 0.3967 - acc: 0.8161 - val_loss: 0.3690 - val_acc: 0.8318\n",
      "Epoch 28/80\n",
      "51/50 [==============================] - 7s - loss: 0.4146 - acc: 0.8155 - val_loss: 0.3778 - val_acc: 0.8224\n",
      "Epoch 29/80\n",
      "51/50 [==============================] - 7s - loss: 0.3785 - acc: 0.8173 - val_loss: 0.3575 - val_acc: 0.8660\n",
      "Epoch 30/80\n",
      "51/50 [==============================] - 7s - loss: 0.3739 - acc: 0.8406 - val_loss: 0.4537 - val_acc: 0.7695\n",
      "Epoch 31/80\n",
      "51/50 [==============================] - 7s - loss: 0.3474 - acc: 0.8333 - val_loss: 0.4208 - val_acc: 0.7539\n",
      "Epoch 32/80\n",
      "51/50 [==============================] - 7s - loss: 0.3699 - acc: 0.8265 - val_loss: 0.3272 - val_acc: 0.8380\n",
      "Epoch 33/80\n",
      "51/50 [==============================] - 7s - loss: 0.3346 - acc: 0.8390 - val_loss: 0.2851 - val_acc: 0.8255\n",
      "Epoch 34/80\n",
      "51/50 [==============================] - 7s - loss: 0.3505 - acc: 0.8297 - val_loss: 0.2811 - val_acc: 0.8474\n",
      "Epoch 35/80\n",
      "51/50 [==============================] - 7s - loss: 0.3497 - acc: 0.8346 - val_loss: 0.6079 - val_acc: 0.6978\n",
      "Epoch 36/80\n",
      "51/50 [==============================] - 7s - loss: 0.3712 - acc: 0.8345 - val_loss: 0.3319 - val_acc: 0.8411\n",
      "Epoch 37/80\n",
      "51/50 [==============================] - 7s - loss: 0.3142 - acc: 0.8529 - val_loss: 0.2871 - val_acc: 0.8785\n",
      "Epoch 38/80\n",
      "51/50 [==============================] - 7s - loss: 0.3338 - acc: 0.8412 - val_loss: 0.2697 - val_acc: 0.8941\n",
      "Epoch 39/80\n",
      "51/50 [==============================] - 7s - loss: 0.3294 - acc: 0.8494 - val_loss: 0.2543 - val_acc: 0.8847\n",
      "Epoch 40/80\n",
      "51/50 [==============================] - 7s - loss: 0.3154 - acc: 0.8450 - val_loss: 0.2815 - val_acc: 0.8972\n",
      "Epoch 41/80\n",
      "51/50 [==============================] - 7s - loss: 0.3000 - acc: 0.8475 - val_loss: 0.2553 - val_acc: 0.8847\n",
      "Epoch 42/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/50 [==============================] - 7s - loss: 0.3088 - acc: 0.8602 - val_loss: 0.3351 - val_acc: 0.8598\n",
      "Epoch 43/80\n",
      "51/50 [==============================] - 7s - loss: 0.3250 - acc: 0.8492 - val_loss: 0.2548 - val_acc: 0.9097\n",
      "Epoch 44/80\n",
      "51/50 [==============================] - 7s - loss: 0.3089 - acc: 0.8683 - val_loss: 0.2535 - val_acc: 0.9003\n",
      "Epoch 45/80\n",
      "51/50 [==============================] - 7s - loss: 0.2930 - acc: 0.8750 - val_loss: 0.3132 - val_acc: 0.8598\n",
      "Epoch 46/80\n",
      "51/50 [==============================] - 7s - loss: 0.3132 - acc: 0.8590 - val_loss: 0.2943 - val_acc: 0.8723\n",
      "Epoch 47/80\n",
      "51/50 [==============================] - 7s - loss: 0.2820 - acc: 0.8682 - val_loss: 0.2345 - val_acc: 0.9159\n",
      "Epoch 48/80\n",
      "51/50 [==============================] - 7s - loss: 0.2850 - acc: 0.8787 - val_loss: 0.3099 - val_acc: 0.8567\n",
      "Epoch 49/80\n",
      "51/50 [==============================] - 7s - loss: 0.2841 - acc: 0.8634 - val_loss: 0.2497 - val_acc: 0.8536\n",
      "Epoch 50/80\n",
      "51/50 [==============================] - 7s - loss: 0.2898 - acc: 0.8659 - val_loss: 0.3106 - val_acc: 0.8723\n",
      "Epoch 51/80\n",
      "51/50 [==============================] - 7s - loss: 0.3140 - acc: 0.8493 - val_loss: 0.3061 - val_acc: 0.8723\n",
      "Epoch 52/80\n",
      "51/50 [==============================] - 7s - loss: 0.2694 - acc: 0.8743 - val_loss: 0.2581 - val_acc: 0.9003\n",
      "Epoch 53/80\n",
      "51/50 [==============================] - 7s - loss: 0.3246 - acc: 0.8597 - val_loss: 0.2775 - val_acc: 0.8847\n",
      "Epoch 54/80\n",
      "51/50 [==============================] - 7s - loss: 0.2919 - acc: 0.8694 - val_loss: 0.2593 - val_acc: 0.8910\n",
      "Epoch 55/80\n",
      "51/50 [==============================] - 7s - loss: 0.2649 - acc: 0.8768 - val_loss: 0.2369 - val_acc: 0.9128\n",
      "Epoch 56/80\n",
      "51/50 [==============================] - 7s - loss: 0.2737 - acc: 0.8781 - val_loss: 0.2199 - val_acc: 0.9283\n",
      "Epoch 57/80\n",
      "51/50 [==============================] - 7s - loss: 0.2730 - acc: 0.8720 - val_loss: 0.2072 - val_acc: 0.9190\n",
      "Epoch 58/80\n",
      "51/50 [==============================] - 7s - loss: 0.2652 - acc: 0.8860 - val_loss: 0.2180 - val_acc: 0.9034\n",
      "Epoch 59/80\n",
      "51/50 [==============================] - 7s - loss: 0.2841 - acc: 0.8740 - val_loss: 0.2158 - val_acc: 0.9190\n",
      "Epoch 60/80\n",
      "51/50 [==============================] - 7s - loss: 0.2797 - acc: 0.8652 - val_loss: 0.2148 - val_acc: 0.9097\n",
      "Epoch 61/80\n",
      "51/50 [==============================] - 7s - loss: 0.2656 - acc: 0.8897 - val_loss: 0.3261 - val_acc: 0.8474\n",
      "Epoch 62/80\n",
      "51/50 [==============================] - 7s - loss: 0.2843 - acc: 0.8731 - val_loss: 0.2686 - val_acc: 0.8879\n",
      "Epoch 63/80\n",
      "51/50 [==============================] - 7s - loss: 0.2517 - acc: 0.8848 - val_loss: 0.2454 - val_acc: 0.8941\n",
      "Epoch 64/80\n",
      "51/50 [==============================] - 7s - loss: 0.2710 - acc: 0.8757 - val_loss: 0.2608 - val_acc: 0.8941\n",
      "Epoch 65/80\n",
      "51/50 [==============================] - 7s - loss: 0.2590 - acc: 0.8891 - val_loss: 0.2414 - val_acc: 0.9003\n",
      "Epoch 66/80\n",
      "51/50 [==============================] - 7s - loss: 0.2572 - acc: 0.8732 - val_loss: 0.2459 - val_acc: 0.8660\n",
      "Epoch 67/80\n",
      "51/50 [==============================] - 7s - loss: 0.2577 - acc: 0.8860 - val_loss: 0.2225 - val_acc: 0.9065\n",
      "Epoch 68/80\n",
      "51/50 [==============================] - 7s - loss: 0.2592 - acc: 0.8891 - val_loss: 0.2293 - val_acc: 0.9034\n",
      "Epoch 69/80\n",
      "51/50 [==============================] - 7s - loss: 0.2689 - acc: 0.8811 - val_loss: 0.2483 - val_acc: 0.9128\n",
      "Epoch 70/80\n",
      "51/50 [==============================] - 7s - loss: 0.2603 - acc: 0.8884 - val_loss: 0.2236 - val_acc: 0.9097\n",
      "Epoch 71/80\n",
      "51/50 [==============================] - 7s - loss: 0.2401 - acc: 0.8995 - val_loss: 0.2144 - val_acc: 0.9346\n",
      "Epoch 72/80\n",
      "51/50 [==============================] - 7s - loss: 0.2441 - acc: 0.8892 - val_loss: 0.2171 - val_acc: 0.9190\n",
      "Epoch 73/80\n",
      "51/50 [==============================] - 7s - loss: 0.2483 - acc: 0.8873 - val_loss: 0.2428 - val_acc: 0.9034\n",
      "Epoch 74/80\n",
      "51/50 [==============================] - 7s - loss: 0.2266 - acc: 0.9056 - val_loss: 0.1960 - val_acc: 0.9315\n",
      "Epoch 75/80\n",
      "51/50 [==============================] - 7s - loss: 0.2220 - acc: 0.8995 - val_loss: 0.1965 - val_acc: 0.9065\n",
      "Epoch 76/80\n",
      "51/50 [==============================] - 7s - loss: 0.2490 - acc: 0.8861 - val_loss: 0.2189 - val_acc: 0.9346\n",
      "Epoch 77/80\n",
      "51/50 [==============================] - 7s - loss: 0.2188 - acc: 0.9050 - val_loss: 0.2137 - val_acc: 0.9128\n",
      "Epoch 78/80\n",
      "51/50 [==============================] - 7s - loss: 0.2441 - acc: 0.8873 - val_loss: 0.1936 - val_acc: 0.9190\n",
      "Epoch 79/80\n",
      "51/50 [==============================] - 7s - loss: 0.2333 - acc: 0.9007 - val_loss: 0.1940 - val_acc: 0.9252\n",
      "Epoch 80/80\n",
      "51/50 [==============================] - 7s - loss: 0.2332 - acc: 0.9025 - val_loss: 0.1918 - val_acc: 0.9252\n",
      "Batch Size: 32\n",
      "Epochs: 80\n",
      "Epoch 1/80\n",
      "51/50 [==============================] - 2s - loss: 0.6610 - acc: 0.5888 - val_loss: 0.5964 - val_acc: 0.6386\n",
      "Epoch 2/80\n",
      "51/50 [==============================] - 1s - loss: 0.6130 - acc: 0.6623 - val_loss: 0.6099 - val_acc: 0.6231\n",
      "Epoch 3/80\n",
      "51/50 [==============================] - 2s - loss: 0.5787 - acc: 0.6898 - val_loss: 0.5544 - val_acc: 0.7040\n",
      "Epoch 4/80\n",
      "51/50 [==============================] - 2s - loss: 0.5772 - acc: 0.6791 - val_loss: 0.5290 - val_acc: 0.7570\n",
      "Epoch 5/80\n",
      "51/50 [==============================] - 2s - loss: 0.5311 - acc: 0.7223 - val_loss: 0.5235 - val_acc: 0.7009\n",
      "Epoch 6/80\n",
      "51/50 [==============================] - 2s - loss: 0.5105 - acc: 0.7322 - val_loss: 0.5790 - val_acc: 0.6480\n",
      "Epoch 7/80\n",
      "51/50 [==============================] - 2s - loss: 0.5100 - acc: 0.7293 - val_loss: 0.4886 - val_acc: 0.7445\n",
      "Epoch 8/80\n",
      "51/50 [==============================] - 2s - loss: 0.5056 - acc: 0.7310 - val_loss: 0.4936 - val_acc: 0.7539\n",
      "Epoch 9/80\n",
      "51/50 [==============================] - 2s - loss: 0.4983 - acc: 0.7408 - val_loss: 0.4947 - val_acc: 0.7259\n",
      "Epoch 10/80\n",
      "51/50 [==============================] - 2s - loss: 0.4727 - acc: 0.7738 - val_loss: 0.6076 - val_acc: 0.6137\n",
      "Epoch 11/80\n",
      "51/50 [==============================] - 2s - loss: 0.4542 - acc: 0.7745 - val_loss: 0.4448 - val_acc: 0.7819\n",
      "Epoch 12/80\n",
      "51/50 [==============================] - 2s - loss: 0.4312 - acc: 0.7868 - val_loss: 0.4515 - val_acc: 0.7601\n",
      "Epoch 13/80\n",
      "51/50 [==============================] - 2s - loss: 0.4282 - acc: 0.7918 - val_loss: 0.5683 - val_acc: 0.6511\n",
      "Epoch 14/80\n",
      "51/50 [==============================] - 2s - loss: 0.4494 - acc: 0.7867 - val_loss: 0.4264 - val_acc: 0.8006\n",
      "Epoch 15/80\n",
      "51/50 [==============================] - 2s - loss: 0.4475 - acc: 0.7708 - val_loss: 0.3862 - val_acc: 0.8567\n",
      "Epoch 16/80\n",
      "51/50 [==============================] - 2s - loss: 0.4083 - acc: 0.8021 - val_loss: 0.3970 - val_acc: 0.8100\n",
      "Epoch 17/80\n",
      "51/50 [==============================] - 2s - loss: 0.3950 - acc: 0.8076 - val_loss: 0.3681 - val_acc: 0.8442\n",
      "Epoch 18/80\n",
      "51/50 [==============================] - 2s - loss: 0.3960 - acc: 0.8039 - val_loss: 0.4056 - val_acc: 0.8255\n",
      "Epoch 19/80\n",
      "51/50 [==============================] - 2s - loss: 0.3766 - acc: 0.8144 - val_loss: 0.3650 - val_acc: 0.8349\n",
      "Epoch 20/80\n",
      "51/50 [==============================] - 2s - loss: 0.3941 - acc: 0.8138 - val_loss: 0.3884 - val_acc: 0.8255\n",
      "Epoch 21/80\n",
      "51/50 [==============================] - 2s - loss: 0.3739 - acc: 0.8229 - val_loss: 0.4112 - val_acc: 0.7850\n",
      "Epoch 22/80\n",
      "51/50 [==============================] - 2s - loss: 0.3823 - acc: 0.8137 - val_loss: 0.4342 - val_acc: 0.7632\n",
      "Epoch 23/80\n",
      "51/50 [==============================] - 2s - loss: 0.3606 - acc: 0.8222 - val_loss: 0.3190 - val_acc: 0.8879\n",
      "Epoch 24/80\n",
      "51/50 [==============================] - 2s - loss: 0.3639 - acc: 0.8243 - val_loss: 0.3759 - val_acc: 0.8162\n",
      "Epoch 25/80\n",
      "51/50 [==============================] - 2s - loss: 0.3646 - acc: 0.8242 - val_loss: 0.3206 - val_acc: 0.8910\n",
      "Epoch 26/80\n",
      "51/50 [==============================] - 2s - loss: 0.3648 - acc: 0.8254 - val_loss: 0.3530 - val_acc: 0.8255\n",
      "Epoch 27/80\n",
      "51/50 [==============================] - 2s - loss: 0.3463 - acc: 0.8432 - val_loss: 0.3348 - val_acc: 0.8474\n",
      "Epoch 28/80\n",
      "51/50 [==============================] - 2s - loss: 0.3486 - acc: 0.8358 - val_loss: 0.3023 - val_acc: 0.8941\n",
      "Epoch 29/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/50 [==============================] - 2s - loss: 0.3451 - acc: 0.8376 - val_loss: 0.3569 - val_acc: 0.8318\n",
      "Epoch 30/80\n",
      "51/50 [==============================] - 2s - loss: 0.3525 - acc: 0.8321 - val_loss: 0.3289 - val_acc: 0.8536\n",
      "Epoch 31/80\n",
      "51/50 [==============================] - 2s - loss: 0.3326 - acc: 0.8438 - val_loss: 0.3406 - val_acc: 0.8380\n",
      "Epoch 32/80\n",
      "51/50 [==============================] - 2s - loss: 0.3175 - acc: 0.8541 - val_loss: 0.3222 - val_acc: 0.8442\n",
      "Epoch 33/80\n",
      "51/50 [==============================] - 2s - loss: 0.3274 - acc: 0.8498 - val_loss: 0.2890 - val_acc: 0.8847\n",
      "Epoch 34/80\n",
      "51/50 [==============================] - 2s - loss: 0.3237 - acc: 0.8431 - val_loss: 0.2953 - val_acc: 0.8972\n",
      "Epoch 35/80\n",
      "51/50 [==============================] - 2s - loss: 0.3192 - acc: 0.8541 - val_loss: 0.3026 - val_acc: 0.8941\n",
      "Epoch 36/80\n",
      "51/50 [==============================] - 2s - loss: 0.3292 - acc: 0.8438 - val_loss: 0.2947 - val_acc: 0.8847\n",
      "Epoch 37/80\n",
      "51/50 [==============================] - 2s - loss: 0.3383 - acc: 0.8371 - val_loss: 0.3037 - val_acc: 0.8941\n",
      "Epoch 38/80\n",
      "51/50 [==============================] - 2s - loss: 0.3277 - acc: 0.8475 - val_loss: 0.3903 - val_acc: 0.7882\n",
      "Epoch 39/80\n",
      "51/50 [==============================] - 2s - loss: 0.3298 - acc: 0.8529 - val_loss: 0.2986 - val_acc: 0.8660\n",
      "Epoch 40/80\n",
      "51/50 [==============================] - 2s - loss: 0.3104 - acc: 0.8602 - val_loss: 0.3163 - val_acc: 0.8567\n",
      "Epoch 41/80\n",
      "51/50 [==============================] - 2s - loss: 0.3077 - acc: 0.8505 - val_loss: 0.3256 - val_acc: 0.8411\n",
      "Epoch 42/80\n",
      "51/50 [==============================] - 2s - loss: 0.3142 - acc: 0.8602 - val_loss: 0.2703 - val_acc: 0.9003\n",
      "Epoch 43/80\n",
      "51/50 [==============================] - 2s - loss: 0.3315 - acc: 0.8455 - val_loss: 0.3941 - val_acc: 0.8037\n",
      "Epoch 44/80\n",
      "51/50 [==============================] - 2s - loss: 0.3075 - acc: 0.8554 - val_loss: 0.2712 - val_acc: 0.9034\n",
      "Epoch 45/80\n",
      "51/50 [==============================] - 2s - loss: 0.3050 - acc: 0.8524 - val_loss: 0.2982 - val_acc: 0.8629\n",
      "Epoch 46/80\n",
      "51/50 [==============================] - 2s - loss: 0.2953 - acc: 0.8535 - val_loss: 0.2835 - val_acc: 0.8754\n",
      "Epoch 47/80\n",
      "51/50 [==============================] - 2s - loss: 0.3041 - acc: 0.8553 - val_loss: 0.2634 - val_acc: 0.9065\n",
      "Epoch 48/80\n",
      "51/50 [==============================] - 2s - loss: 0.2999 - acc: 0.8652 - val_loss: 0.2860 - val_acc: 0.8754\n",
      "Epoch 49/80\n",
      "51/50 [==============================] - 2s - loss: 0.2999 - acc: 0.8511 - val_loss: 0.2728 - val_acc: 0.8941\n",
      "Epoch 50/80\n",
      "51/50 [==============================] - 2s - loss: 0.2986 - acc: 0.8609 - val_loss: 0.3521 - val_acc: 0.8069\n",
      "Epoch 51/80\n",
      "51/50 [==============================] - 2s - loss: 0.3036 - acc: 0.8701 - val_loss: 0.2672 - val_acc: 0.9034\n",
      "Epoch 52/80\n",
      "51/50 [==============================] - 2s - loss: 0.3185 - acc: 0.8492 - val_loss: 0.2716 - val_acc: 0.8910\n",
      "Epoch 53/80\n",
      "51/50 [==============================] - ETA: 0s - loss: 0.2884 - acc: 0.865 - 2s - loss: 0.2910 - acc: 0.8609 - val_loss: 0.3354 - val_acc: 0.8349\n",
      "Epoch 54/80\n",
      "51/50 [==============================] - 2s - loss: 0.3058 - acc: 0.8493 - val_loss: 0.2783 - val_acc: 0.9065\n",
      "Epoch 55/80\n",
      "51/50 [==============================] - 2s - loss: 0.2860 - acc: 0.8731 - val_loss: 0.2741 - val_acc: 0.8785\n",
      "Epoch 56/80\n",
      "51/50 [==============================] - 2s - loss: 0.2771 - acc: 0.8743 - val_loss: 0.2600 - val_acc: 0.9097\n",
      "Epoch 57/80\n",
      "51/50 [==============================] - 2s - loss: 0.2870 - acc: 0.8786 - val_loss: 0.3225 - val_acc: 0.8255\n",
      "Epoch 58/80\n",
      "51/50 [==============================] - 2s - loss: 0.3037 - acc: 0.8596 - val_loss: 0.4360 - val_acc: 0.7601\n",
      "Epoch 59/80\n",
      "51/50 [==============================] - 2s - loss: 0.3173 - acc: 0.8482 - val_loss: 0.2649 - val_acc: 0.9003\n",
      "Epoch 60/80\n",
      "51/50 [==============================] - 2s - loss: 0.2839 - acc: 0.8805 - val_loss: 0.3040 - val_acc: 0.8598\n",
      "Epoch 61/80\n",
      "51/50 [==============================] - 2s - loss: 0.2778 - acc: 0.8799 - val_loss: 0.2762 - val_acc: 0.8723\n",
      "Epoch 62/80\n",
      "51/50 [==============================] - 2s - loss: 0.2906 - acc: 0.8780 - val_loss: 0.2894 - val_acc: 0.8847\n",
      "Epoch 63/80\n",
      "51/50 [==============================] - 2s - loss: 0.2940 - acc: 0.8744 - val_loss: 0.3079 - val_acc: 0.8380\n",
      "Epoch 64/80\n",
      "51/50 [==============================] - 2s - loss: 0.2929 - acc: 0.8719 - val_loss: 0.2612 - val_acc: 0.8910\n",
      "Epoch 65/80\n",
      "51/50 [==============================] - 2s - loss: 0.2856 - acc: 0.8750 - val_loss: 0.2465 - val_acc: 0.8972\n",
      "Epoch 66/80\n",
      "51/50 [==============================] - 2s - loss: 0.2767 - acc: 0.8682 - val_loss: 0.2490 - val_acc: 0.9065\n",
      "Epoch 67/80\n",
      "51/50 [==============================] - 2s - loss: 0.2661 - acc: 0.8756 - val_loss: 0.2390 - val_acc: 0.9128\n",
      "Epoch 68/80\n",
      "51/50 [==============================] - 2s - loss: 0.2789 - acc: 0.8708 - val_loss: 0.2475 - val_acc: 0.9190\n",
      "Epoch 69/80\n",
      "51/50 [==============================] - 2s - loss: 0.3000 - acc: 0.8605 - val_loss: 0.2739 - val_acc: 0.8692\n",
      "Epoch 70/80\n",
      "51/50 [==============================] - 2s - loss: 0.3097 - acc: 0.8517 - val_loss: 0.3004 - val_acc: 0.8536\n",
      "Epoch 71/80\n",
      "51/50 [==============================] - 2s - loss: 0.2866 - acc: 0.8652 - val_loss: 0.2755 - val_acc: 0.8972\n",
      "Epoch 72/80\n",
      "51/50 [==============================] - 2s - loss: 0.2881 - acc: 0.8701 - val_loss: 0.2519 - val_acc: 0.8941\n",
      "Epoch 73/80\n",
      "51/50 [==============================] - 2s - loss: 0.2608 - acc: 0.8774 - val_loss: 0.3326 - val_acc: 0.8131\n",
      "Epoch 74/80\n",
      "51/50 [==============================] - 2s - loss: 0.2685 - acc: 0.8786 - val_loss: 0.2377 - val_acc: 0.9034\n",
      "Epoch 75/80\n",
      "51/50 [==============================] - 2s - loss: 0.2606 - acc: 0.8866 - val_loss: 0.3593 - val_acc: 0.8193\n",
      "Epoch 76/80\n",
      "51/50 [==============================] - 2s - loss: 0.3023 - acc: 0.8652 - val_loss: 0.2769 - val_acc: 0.8816\n",
      "Epoch 77/80\n",
      "51/50 [==============================] - 2s - loss: 0.2592 - acc: 0.8866 - val_loss: 0.2369 - val_acc: 0.9159\n",
      "Epoch 78/80\n",
      "51/50 [==============================] - 2s - loss: 0.2587 - acc: 0.8836 - val_loss: 0.2558 - val_acc: 0.9034\n",
      "Epoch 79/80\n",
      "51/50 [==============================] - 2s - loss: 0.2606 - acc: 0.8885 - val_loss: 0.2747 - val_acc: 0.8754\n",
      "Epoch 80/80\n",
      "51/50 [==============================] - 2s - loss: 0.2680 - acc: 0.8738 - val_loss: 0.2496 - val_acc: 0.9097\n"
     ]
    }
   ],
   "source": [
    "trainRunner.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict = []\n",
    "X_test = helpers.get_images(test)\n",
    "\n",
    "for model in trainRunner.models:\n",
    "    predict.append(model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "        prediction = np.zeros((8424, ))\n",
    "        \n",
    "        for pred in predict:\n",
    "            prediction += pred[:, 1]\n",
    "            \n",
    "        prediction = prediction / len(predict)\n",
    "        \n",
    "        submission = pd.DataFrame(test, columns=[\"id\"])\n",
    "        \n",
    "        submission[\"is_iceberg\"] = prediction\n",
    "\n",
    "        test_func = lambda p: round(p[\"is_iceberg\"], 4)\n",
    "        submission[\"is_iceberg\"] = test_func(submission)\n",
    "        submission[\"is_iceberg\"] = submission[\"is_iceberg\"].round(4)\n",
    "        submission.to_csv(\"submission.csv\", float_format='%g', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
