{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import pandas as pd\n",
    "\n",
    "import cv2\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.applications import MobileNet\n",
    "from keras.layers import Dense, Input, Dropout, GlobalAveragePooling2D, Reshape, Conv2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_json(\"_RawData/train.json/data/processed/train.json\")\n",
    "test = pd.read_json(\"_RawData/test.json/data/processed/test.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train data\n",
    "x_band1 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train['band_1']])\n",
    "x_band2 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train['band_2']])\n",
    "\n",
    "X_train = np.concatenate([x_band1[:, :, :, np.newaxis],\n",
    "                          x_band2[:, :, :, np.newaxis],\n",
    "                          ((x_band1+x_band1)/2)[:, :, :, np.newaxis]], axis=-1)\n",
    "\n",
    "target_train=train['is_iceberg']\n",
    "\n",
    "del train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test data\n",
    "x_band1 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in test['band_1']])\n",
    "x_band2 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in test['band_2']])\n",
    "\n",
    "X_test = np.concatenate([x_band1[:, :, :, np.newaxis],\n",
    "                         x_band2[:, :, :, np.newaxis],\n",
    "                         ((x_band1+x_band1)/2)[:, :, :, np.newaxis]], axis=-1)\n",
    "\n",
    "id_test = test['id'].values\n",
    "\n",
    "del test; del x_band1; del x_band2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 224, 224, 3)       12        \n",
      "_________________________________________________________________\n",
      "mobilenet_1.00_224 (Model)   (None, 7, 7, 1024)        3228864   \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_5 ( (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 1025      \n",
      "=================================================================\n",
      "Total params: 3,229,901\n",
      "Trainable params: 3,208,007\n",
      "Non-trainable params: 21,894\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define CNN Model Architecture (Kaggle can't access the weights file)\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "img_channels = 3\n",
    "img_dim = (img_height, img_width, img_channels)\n",
    "\n",
    "def inceptionv3(img_dim=img_dim):\n",
    "    input_tensor = Input(shape=img_dim)\n",
    "    base_model = MobileNet(include_top=False,\n",
    "                   weights='imagenet',\n",
    "                   input_shape=img_dim)\n",
    "    \n",
    "    bn = BatchNormalization()(input_tensor)\n",
    "    x = base_model(bn)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "#     x = Reshape((1, 1, 1024), name='reshape_1')(x)\n",
    "    x = Dropout(0.2, name='dropout')(x)\n",
    "#     x = Conv2D(2, (1, 1),\n",
    "#                padding='same', name='conv_preds')(x)\n",
    "#     x = Dense('sigmoid', name='act_softmax')(x)\n",
    "#     x = Reshape((classes,), name='reshape_2')(x)\n",
    "    \n",
    "#     bn = BatchNormalization()(input_tensor)\n",
    "#     x = base_model(bn)\n",
    "#     x = GlobalAveragePooling2D()(x)\n",
    "#     x = Dropout(0.5)(x)\n",
    "    output = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(input_tensor, output)\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = inceptionv3()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train Model and predict\n",
    "def train_model(model, batch_size, epochs, img_size, x, y, test, n_fold, kf):\n",
    "        \n",
    "    train_scores = []; valid_scores = []\n",
    "    preds_test = np.zeros(len(test), dtype = np.float)\n",
    "\n",
    "    i = 1\n",
    "\n",
    "    for train_index, test_index in kf.split(x):\n",
    "        x_train = x[train_index]; x_valid = x[test_index]\n",
    "        y_train = y[train_index]; y_valid = y[test_index]\n",
    "\n",
    "        def augment(src, choice):\n",
    "            if choice == 0:\n",
    "                # Rotate 90\n",
    "                src = np.rot90(src, 1)\n",
    "            if choice == 1:\n",
    "                # flip vertically\n",
    "                src = np.flipud(src)\n",
    "            if choice == 2:\n",
    "                # Rotate 180\n",
    "                src = np.rot90(src, 2)\n",
    "            if choice == 3:\n",
    "                # flip horizontally\n",
    "                src = np.fliplr(src)\n",
    "            if choice == 4:\n",
    "                # Rotate 90 counter-clockwise\n",
    "                src = np.rot90(src, 3)\n",
    "            if choice == 5:\n",
    "                # Rotate 180 and flip horizontally\n",
    "                src = np.rot90(src, 2)\n",
    "                src = np.fliplr(src)\n",
    "            return src\n",
    "\n",
    "        def train_generator():\n",
    "            while True:\n",
    "                for start in range(0, len(x_train), batch_size):\n",
    "                    x_batch = []\n",
    "                    end = min(start + batch_size, len(x_train))\n",
    "                    y_batch = y_train[start:end]\n",
    "                    for img in x_train[start:end]:\n",
    "                        new_img = cv2.resize(img, img_size)\n",
    "                        new_img = augment(new_img, np.random.randint(6))\n",
    "                        x_batch.append(new_img)\n",
    "                    x_batch = np.array(x_batch, np.float32) / 255.\n",
    "                    y_batch = np.array(y_batch, np.uint8)\n",
    "                    yield x_batch, y_batch\n",
    "\n",
    "        def valid_generator():\n",
    "            while True:\n",
    "                for start in range(0, len(x_valid), batch_size):\n",
    "                    x_batch = []\n",
    "                    end = min(start + batch_size, len(x_valid))\n",
    "                    y_batch = y_valid[start:end]\n",
    "                    for img in x_valid[start:end]:\n",
    "                        new_img = cv2.resize(img, img_size)\n",
    "                        x_batch.append(new_img)\n",
    "                    x_batch = np.array(x_batch, np.float32) / 255.\n",
    "                    y_batch = np.array(y_batch, np.uint8)\n",
    "                    yield x_batch, y_batch\n",
    "\n",
    "        def test_generator():\n",
    "            while True:\n",
    "                for start in range(0, len(test), n_fold):\n",
    "                    x_batch = []\n",
    "                    end = min(start + n_fold, len(test))\n",
    "                    for img in test[start:end]:\n",
    "                        new_img = cv2.resize(img, img_size)\n",
    "                        x_batch.append(new_img)\n",
    "                    x_batch = np.array(x_batch, np.float32) / 255.\n",
    "                    yield x_batch\n",
    "                    \n",
    "        callbacks = [EarlyStopping(monitor='val_loss', patience=3, verbose=1, min_delta=1e-4),\n",
    "             ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=1, cooldown=1, \n",
    "                               verbose=1, min_lr=1e-7),\n",
    "             ModelCheckpoint(filepath='inception.fold_' + str(i) + '.hdf5', verbose=1,\n",
    "                             save_best_only=True, save_weights_only=True, mode='auto')]\n",
    "\n",
    "        train_steps = len(x_train) / batch_size\n",
    "        valid_steps = len(x_valid) / batch_size\n",
    "        test_steps = len(test) / n_fold\n",
    "        \n",
    "        model = model\n",
    "\n",
    "        model.compile(optimizer=Adam(lr=1e-4), loss='binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "        model.fit_generator(train_generator(), train_steps, epochs=epochs, verbose=1, \n",
    "                            callbacks=callbacks, validation_data=valid_generator(), \n",
    "                            validation_steps=valid_steps)\n",
    "\n",
    "        model.load_weights(filepath='inception.fold_' + str(i) + '.hdf5')\n",
    "\n",
    "        \n",
    "        print('----------------------------------------')\n",
    "        print('Running train evaluation on fold {}'.format(i))\n",
    "        train_score = model.evaluate_generator(train_generator(), steps=train_steps)        \n",
    "        print('Running validation evaluation on fold {}'.format(i))\n",
    "        valid_score = model.evaluate_generator(valid_generator(), steps=valid_steps)\n",
    "        print('----------------------------------------')   \n",
    "        \n",
    "        print('Train loss: {:0.5f}\\n Train acc: {:0.5f} for fold {}'.format(train_score[0],\n",
    "                                                                            train_score[1], i))\n",
    "        print('Valid loss: {:0.5f}\\n Valid acc: {:0.5f} for fold {}'.format(valid_score[0],\n",
    "                                                                            valid_score[1], i))\n",
    "        print('----------------------------------------')\n",
    "\n",
    "        train_scores.append(train_score[1])\n",
    "        valid_scores.append(valid_score[1])\n",
    "        print('Avg Train Acc: {:0.5f}\\nAvg Valid Acc: {:0.5f} after {} folds'.format\n",
    "              (np.mean(train_scores), np.mean(valid_scores), i))\n",
    "        print('----------------------------------------')\n",
    "        \n",
    "        print('Running test predictions with fold {}'.format(i))        \n",
    "        preds_test_fold = model.predict_generator(generator=test_generator(),\n",
    "                                              steps=test_steps, verbose=1)[:, -1]\n",
    "\n",
    "        preds_test += preds_test_fold\n",
    "\n",
    "        print('\\n\\n')\n",
    "\n",
    "        i += 1\n",
    "\n",
    "        if i <= n_fold:\n",
    "            print('Now beginning training for fold {}\\n\\n'.format(i))\n",
    "        else:\n",
    "            print('Finished training!')\n",
    "\n",
    "    preds_test /= n_fold\n",
    "\n",
    "    return preds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "178/178 [============================>.] - ETA: 0s - loss: 0.5756 - acc: 0.7107Epoch 00001: val_loss improved from inf to 0.88033, saving model to inception.fold_1.hdf5\n",
      "179/178 [==============================] - 31s 173ms/step - loss: 0.5751 - acc: 0.7123 - val_loss: 0.8803 - val_acc: 0.4785\n",
      "Epoch 2/50\n",
      "178/178 [============================>.] - ETA: 0s - loss: 0.3884 - acc: 0.8296Epoch 00002: val_loss did not improve\n",
      "179/178 [==============================] - 19s 108ms/step - loss: 0.3884 - acc: 0.8305 - val_loss: 1.1022 - val_acc: 0.4897\n",
      "Epoch 3/50\n",
      "178/178 [============================>.] - ETA: 0s - loss: 0.3693 - acc: 0.8230Epoch 00003: val_loss improved from 0.88033 to 0.48109, saving model to inception.fold_1.hdf5\n",
      "179/178 [==============================] - 20s 111ms/step - loss: 0.3685 - acc: 0.8240 - val_loss: 0.4811 - val_acc: 0.7383\n",
      "Epoch 4/50\n",
      "178/178 [============================>.] - ETA: 0s - loss: 0.3104 - acc: 0.8717Epoch 00004: val_loss improved from 0.48109 to 0.27658, saving model to inception.fold_1.hdf5\n",
      "179/178 [==============================] - 20s 110ms/step - loss: 0.3105 - acc: 0.8724 - val_loss: 0.2766 - val_acc: 0.8822\n",
      "Epoch 5/50\n",
      "178/178 [============================>.] - ETA: 0s - loss: 0.2499 - acc: 0.8979Epoch 00005: val_loss did not improve\n",
      "179/178 [==============================] - 19s 108ms/step - loss: 0.2520 - acc: 0.8985 - val_loss: 0.3213 - val_acc: 0.8692\n",
      "Epoch 6/50\n",
      "178/178 [============================>.] - ETA: 0s - loss: 0.2525 - acc: 0.8904\n",
      "Epoch 00006: reducing learning rate to 9.999999747378752e-06.\n",
      "Epoch 00006: val_loss did not improve\n",
      "179/178 [==============================] - 20s 113ms/step - loss: 0.2534 - acc: 0.8911 - val_loss: 0.3561 - val_acc: 0.8299\n",
      "Epoch 7/50\n",
      "178/178 [============================>.] - ETA: 0s - loss: 0.2213 - acc: 0.9054Epoch 00007: val_loss did not improve\n",
      "179/178 [==============================] - 19s 109ms/step - loss: 0.2209 - acc: 0.9060 - val_loss: 0.3143 - val_acc: 0.8785\n",
      "Epoch 00007: early stopping\n",
      "----------------------------------------\n",
      "Running train evaluation on fold 1\n",
      "Running validation evaluation on fold 1\n",
      "----------------------------------------\n",
      "Train loss: 0.19286\n",
      " Train acc: 0.91862 for fold 1\n",
      "Valid loss: 0.27658\n",
      " Valid acc: 0.88224 for fold 1\n",
      "----------------------------------------\n",
      "Avg Train Acc: 0.91862\n",
      "Avg Valid Acc: 0.88224 after 1 folds\n",
      "----------------------------------------\n",
      "Running test predictions with fold 1\n",
      "2808/2808 [==============================] - 44s 16ms/step\n",
      "\n",
      "\n",
      "\n",
      "Now beginning training for fold 2\n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "178/178 [============================>.] - ETA: 0s - loss: 0.4014 - acc: 0.8230Epoch 00001: val_loss improved from inf to 0.28851, saving model to inception.fold_2.hdf5\n",
      "179/178 [==============================] - 29s 163ms/step - loss: 0.4011 - acc: 0.8240 - val_loss: 0.2885 - val_acc: 0.8766\n",
      "Epoch 2/50\n",
      "178/178 [============================>.] - ETA: 0s - loss: 0.3328 - acc: 0.8539Epoch 00002: val_loss improved from 0.28851 to 0.25541, saving model to inception.fold_2.hdf5\n",
      "179/178 [==============================] - 20s 114ms/step - loss: 0.3320 - acc: 0.8547 - val_loss: 0.2554 - val_acc: 0.8841\n",
      "Epoch 3/50\n",
      "178/178 [============================>.] - ETA: 0s - loss: 0.2753 - acc: 0.8792Epoch 00003: val_loss did not improve\n",
      "179/178 [==============================] - 20s 112ms/step - loss: 0.2741 - acc: 0.8799 - val_loss: 0.2680 - val_acc: 0.8972\n",
      "Epoch 4/50\n",
      "178/178 [============================>.] - ETA: 0s - loss: 0.2388 - acc: 0.8923\n",
      "Epoch 00004: reducing learning rate to 9.999999747378752e-06.\n",
      "Epoch 00004: val_loss did not improve\n",
      "179/178 [==============================] - 21s 117ms/step - loss: 0.2387 - acc: 0.8929 - val_loss: 0.3636 - val_acc: 0.8523\n",
      "Epoch 5/50\n",
      "178/178 [============================>.] - ETA: 0s - loss: 0.1939 - acc: 0.9251Epoch 00005: val_loss improved from 0.25541 to 0.22982, saving model to inception.fold_2.hdf5\n",
      "179/178 [==============================] - 21s 115ms/step - loss: 0.1952 - acc: 0.9255 - val_loss: 0.2298 - val_acc: 0.9159\n",
      "Epoch 6/50\n",
      "178/178 [============================>.] - ETA: 0s - loss: 0.1659 - acc: 0.9363Epoch 00006: val_loss improved from 0.22982 to 0.22834, saving model to inception.fold_2.hdf5\n",
      "179/178 [==============================] - 20s 114ms/step - loss: 0.1665 - acc: 0.9367 - val_loss: 0.2283 - val_acc: 0.9178\n",
      "Epoch 7/50\n",
      "178/178 [============================>.] - ETA: 0s - loss: 0.1725 - acc: 0.9363Epoch 00007: val_loss improved from 0.22834 to 0.22484, saving model to inception.fold_2.hdf5\n",
      "179/178 [==============================] - 20s 114ms/step - loss: 0.1719 - acc: 0.9367 - val_loss: 0.2248 - val_acc: 0.9196\n",
      "Epoch 8/50\n",
      "178/178 [============================>.] - ETA: 0s - loss: 0.1397 - acc: 0.9522Epoch 00008: val_loss improved from 0.22484 to 0.22242, saving model to inception.fold_2.hdf5\n",
      "179/178 [==============================] - 21s 115ms/step - loss: 0.1396 - acc: 0.9525 - val_loss: 0.2224 - val_acc: 0.9196\n",
      "Epoch 9/50\n",
      "178/178 [============================>.] - ETA: 0s - loss: 0.1389 - acc: 0.9485Epoch 00009: val_loss improved from 0.22242 to 0.22038, saving model to inception.fold_2.hdf5\n",
      "179/178 [==============================] - 20s 114ms/step - loss: 0.1385 - acc: 0.9488 - val_loss: 0.2204 - val_acc: 0.9140\n",
      "Epoch 10/50\n",
      "178/178 [============================>.] - ETA: 0s - loss: 0.1228 - acc: 0.9597Epoch 00010: val_loss did not improve\n",
      "179/178 [==============================] - 20s 112ms/step - loss: 0.1234 - acc: 0.9600 - val_loss: 0.2230 - val_acc: 0.9159\n",
      "Epoch 11/50\n",
      "178/178 [============================>.] - ETA: 0s - loss: 0.1235 - acc: 0.9532\n",
      "Epoch 00011: reducing learning rate to 9.999999747378752e-07.\n",
      "Epoch 00011: val_loss did not improve\n",
      "179/178 [==============================] - 20s 112ms/step - loss: 0.1230 - acc: 0.9534 - val_loss: 0.2282 - val_acc: 0.9121\n",
      "Epoch 12/50\n",
      "178/178 [============================>.] - ETA: 0s - loss: 0.1249 - acc: 0.9597Epoch 00012: val_loss did not improve\n",
      "179/178 [==============================] - 20s 113ms/step - loss: 0.1251 - acc: 0.9600 - val_loss: 0.2290 - val_acc: 0.9140\n",
      "Epoch 00012: early stopping\n",
      "----------------------------------------\n",
      "Running train evaluation on fold 2\n",
      "Running validation evaluation on fold 2\n",
      "----------------------------------------\n",
      "Train loss: 0.09410\n",
      " Train acc: 0.96726 for fold 2\n",
      "Valid loss: 0.22038\n",
      " Valid acc: 0.91402 for fold 2\n",
      "----------------------------------------\n",
      "Avg Train Acc: 0.94294\n",
      "Avg Valid Acc: 0.89813 after 2 folds\n",
      "----------------------------------------\n",
      "Running test predictions with fold 2\n",
      "2808/2808 [==============================] - 46s 16ms/step\n",
      "\n",
      "\n",
      "\n",
      "Now beginning training for fold 3\n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "178/178 [============================>.] - ETA: 0s - loss: 0.3033 - acc: 0.8792Epoch 00001: val_loss improved from inf to 0.29157, saving model to inception.fold_3.hdf5\n",
      "179/178 [==============================] - 28s 157ms/step - loss: 0.3027 - acc: 0.8799 - val_loss: 0.2916 - val_acc: 0.8764\n",
      "Epoch 2/50\n",
      "178/178 [============================>.] - ETA: 0s - loss: 0.2516 - acc: 0.9007- EEpoch 00002: val_loss did not improve\n",
      "179/178 [==============================] - 20s 109ms/step - loss: 0.2515 - acc: 0.9013 - val_loss: 0.3106 - val_acc: 0.8895\n",
      "Epoch 3/50\n",
      "178/178 [============================>.] - ETA: 0s - loss: 0.2042 - acc: 0.9251\n",
      "Epoch 00003: reducing learning rate to 9.999999747378752e-06.\n",
      "Epoch 00003: val_loss did not improve\n",
      "179/178 [==============================] - 21s 116ms/step - loss: 0.2041 - acc: 0.9255 - val_loss: 0.3085 - val_acc: 0.8670\n",
      "Epoch 4/50\n",
      "178/178 [============================>.] - ETA: 0s - loss: 0.1898 - acc: 0.9307Epoch 00004: val_loss improved from 0.29157 to 0.24450, saving model to inception.fold_3.hdf5\n",
      "179/178 [==============================] - 20s 113ms/step - loss: 0.1913 - acc: 0.9283 - val_loss: 0.2445 - val_acc: 0.9064\n",
      "Epoch 5/50\n",
      "178/178 [============================>.] - ETA: 0s - loss: 0.1521 - acc: 0.9448Epoch 00005: val_loss improved from 0.24450 to 0.22938, saving model to inception.fold_3.hdf5\n",
      "179/178 [==============================] - 20s 113ms/step - loss: 0.1527 - acc: 0.9451 - val_loss: 0.2294 - val_acc: 0.9101\n",
      "Epoch 6/50\n",
      "178/178 [============================>.] - ETA: 0s - loss: 0.1253 - acc: 0.9522Epoch 00006: val_loss did not improve\n",
      "179/178 [==============================] - 20s 111ms/step - loss: 0.1255 - acc: 0.9525 - val_loss: 0.2350 - val_acc: 0.9101\n",
      "Epoch 7/50\n",
      "178/178 [============================>.] - ETA: 0s - loss: 0.1128 - acc: 0.9635\n",
      "Epoch 00007: reducing learning rate to 9.999999747378752e-07.\n",
      "Epoch 00007: val_loss did not improve\n",
      "179/178 [==============================] - 20s 111ms/step - loss: 0.1141 - acc: 0.9637 - val_loss: 0.2311 - val_acc: 0.9120\n",
      "Epoch 8/50\n",
      "178/178 [============================>.] - ETA: 0s - loss: 0.1137 - acc: 0.9569Epoch 00008: val_loss did not improve\n",
      "179/178 [==============================] - 20s 110ms/step - loss: 0.1137 - acc: 0.9572 - val_loss: 0.2312 - val_acc: 0.9120\n",
      "Epoch 00008: early stopping\n",
      "----------------------------------------\n",
      "Running train evaluation on fold 3\n",
      "Running validation evaluation on fold 3\n",
      "----------------------------------------\n",
      "Train loss: 0.07110\n",
      " Train acc: 0.97850 for fold 3\n",
      "Valid loss: 0.22938\n",
      " Valid acc: 0.91011 for fold 3\n",
      "----------------------------------------\n",
      "Avg Train Acc: 0.95479\n",
      "Avg Valid Acc: 0.90212 after 3 folds\n",
      "----------------------------------------\n",
      "Running test predictions with fold 3\n",
      "2808/2808 [==============================] - 46s 16ms/step\n",
      "\n",
      "\n",
      "\n",
      "Finished training!\n"
     ]
    }
   ],
   "source": [
    "batch_size = 6\n",
    "epochs = 50\n",
    "n_fold = 3\n",
    "img_size = (img_height, img_width)\n",
    "kf = KFold(n_splits=n_fold, shuffle=True)\n",
    "\n",
    "prediction = train_model(model, batch_size, epochs, img_size, X_train, \n",
    "                                target_train, X_test, n_fold, kf)\n",
    "\n",
    "submit = pd.DataFrame({'id': id_test, 'is_iceberg': prediction.reshape((prediction.shape[0]))})\n",
    "submit.to_csv('./submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
