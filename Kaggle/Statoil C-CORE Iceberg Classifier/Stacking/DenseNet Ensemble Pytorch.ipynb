{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:USE CUDA=True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__Python VERSION: 3.6.1 |Continuum Analytics, Inc.| (default, May 11 2017, 13:25:24) [MSC v.1900 64 bit (AMD64)]\n",
      "__pyTorch VERSION: 0.1.12\n",
      "3.6.1 |Continuum Analytics, Inc.| (default, May 11 2017, 13:25:24) [MSC v.1900 64 bit (AMD64)]\n",
      "18.5\n",
      "svmem(total=17056886784, available=9404092416, percent=44.9, used=7652794368, free=9404092416)\n",
      "memory GB: 2.3512229919433594\n",
      "(1604, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Using the GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Using the GPU\n",
      "INFO:__main__:Using the GPU\n",
      "INFO:__main__:<torch.optim.adam.Adam object at 0x000001DE7D72B0B8>\n",
      "INFO:__main__:BCELoss (\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1604, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'torch.cuda.FloatTensor'>\n",
      "(1604, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "Offest:1427\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x000001DBD3F5A940>\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x000001DB99AA31D0>\n",
      "  + Number of params: 19921\n",
      "DenseNet (\n",
      "  (conv1): Conv2d(2, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (dense1): Sequential (\n",
      "    (0): Bottleneck (\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv1): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv2): Conv2d(32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "    (1): Bottleneck (\n",
      "      (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv1): Conv2d(24, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv2): Conv2d(32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "  )\n",
      "  (trans1): Transition (\n",
      "    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (conv1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  )\n",
      "  (dense2): Sequential (\n",
      "    (0): Bottleneck (\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv1): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv2): Conv2d(32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "    (1): Bottleneck (\n",
      "      (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv1): Conv2d(24, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv2): Conv2d(32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "  )\n",
      "  (trans2): Transition (\n",
      "    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (conv1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  )\n",
      "  (dense3): Sequential (\n",
      "    (0): Bottleneck (\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv1): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv2): Conv2d(32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "    (1): Bottleneck (\n",
      "      (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv1): Conv2d(24, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv2): Conv2d(32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "  )\n",
      "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (fc): Linear (128 -> 1)\n",
      ")\n",
      "Epoch 1/25\n",
      "*****:\n",
      "[1/25] Loss: 0.670422\n",
      "[1/25] Loss: 0.658095\n",
      "Finish 1 epoch, Loss: 0.655028\n",
      "VALIDATION Loss: 0.661784\n",
      "\n",
      "Epoch 2/25\n",
      "*****:\n",
      "[2/25] Loss: 0.646918\n",
      "[2/25] Loss: 0.634161\n",
      "Finish 2 epoch, Loss: 0.630367\n",
      "VALIDATION Loss: 0.597930\n",
      "\n",
      "Epoch 3/25\n",
      "*****:\n",
      "[3/25] Loss: 0.590574\n",
      "[3/25] Loss: 0.579978\n",
      "Finish 3 epoch, Loss: 0.574232\n",
      "VALIDATION Loss: 0.548278\n",
      "\n",
      "Epoch 4/25\n",
      "*****:\n",
      "[4/25] Loss: 0.525625\n",
      "[4/25] Loss: 0.523652\n",
      "Finish 4 epoch, Loss: 0.521614\n",
      "VALIDATION Loss: 0.504097\n",
      "\n",
      "Epoch 5/25\n",
      "*****:\n",
      "[5/25] Loss: 0.477888\n",
      "[5/25] Loss: 0.476985\n",
      "Finish 5 epoch, Loss: 0.474818\n",
      "VALIDATION Loss: 0.462755\n",
      "\n",
      "Epoch 6/25\n",
      "*****:\n",
      "[6/25] Loss: 0.427622\n",
      "[6/25] Loss: 0.432572\n",
      "Finish 6 epoch, Loss: 0.430780\n",
      "VALIDATION Loss: 0.402771\n",
      "\n",
      "Epoch 7/25\n",
      "*****:\n",
      "[7/25] Loss: 0.380748\n",
      "[7/25] Loss: 0.385245\n",
      "Finish 7 epoch, Loss: 0.385010\n",
      "VALIDATION Loss: 0.370216\n",
      "\n",
      "Epoch 8/25\n",
      "*****:\n",
      "[8/25] Loss: 0.345152\n",
      "[8/25] Loss: 0.351355\n",
      "Finish 8 epoch, Loss: 0.353993\n",
      "VALIDATION Loss: 0.353582\n",
      "\n",
      "Epoch 9/25\n",
      "*****:\n",
      "[9/25] Loss: 0.323902\n",
      "[9/25] Loss: 0.335892\n",
      "Finish 9 epoch, Loss: 0.340270\n",
      "VALIDATION Loss: 0.334713\n",
      "\n",
      "Epoch 10/25\n",
      "*****:\n",
      "[10/25] Loss: 0.311342\n",
      "[10/25] Loss: 0.320478\n",
      "Finish 10 epoch, Loss: 0.325495\n",
      "VALIDATION Loss: 0.322846\n",
      "\n",
      "Epoch 11/25\n",
      "*****:\n",
      "[11/25] Loss: 0.293732\n",
      "[11/25] Loss: 0.305699\n",
      "Finish 11 epoch, Loss: 0.310120\n",
      "VALIDATION Loss: 0.314682\n",
      "\n",
      "Epoch 12/25\n",
      "*****:\n",
      "[12/25] Loss: 0.285394\n",
      "[12/25] Loss: 0.297421\n",
      "Finish 12 epoch, Loss: 0.301193\n",
      "VALIDATION Loss: 0.311347\n",
      "\n",
      "Epoch 13/25\n",
      "*****:\n",
      "[13/25] Loss: 0.278673\n",
      "[13/25] Loss: 0.291510\n",
      "Finish 13 epoch, Loss: 0.294955\n",
      "VALIDATION Loss: 0.307930\n",
      "\n",
      "Epoch 14/25\n",
      "*****:\n",
      "[14/25] Loss: 0.272979\n",
      "[14/25] Loss: 0.285952\n",
      "Finish 14 epoch, Loss: 0.288923\n",
      "VALIDATION Loss: 0.299985\n",
      "\n",
      "Epoch 15/25\n",
      "*****:\n",
      "[15/25] Loss: 0.269418\n",
      "[15/25] Loss: 0.280688\n",
      "Finish 15 epoch, Loss: 0.283122\n",
      "VALIDATION Loss: 0.291583\n",
      "\n",
      "Epoch 16/25\n",
      "*****:\n",
      "[16/25] Loss: 0.262224\n",
      "[16/25] Loss: 0.275108\n",
      "Finish 16 epoch, Loss: 0.277984\n",
      "VALIDATION Loss: 0.277617\n",
      "\n",
      "Epoch 17/25\n",
      "*****:\n",
      "[17/25] Loss: 0.261890\n",
      "[17/25] Loss: 0.271709\n",
      "Finish 17 epoch, Loss: 0.273414\n",
      "VALIDATION Loss: 0.279572\n",
      "\n",
      "Epoch 18/25\n",
      "*****:\n",
      "[18/25] Loss: 0.260437\n",
      "[18/25] Loss: 0.268902\n",
      "Finish 18 epoch, Loss: 0.269777\n",
      "VALIDATION Loss: 0.278944\n",
      "\n",
      "Epoch 19/25\n",
      "*****:\n",
      "[19/25] Loss: 0.253856\n",
      "[19/25] Loss: 0.264058\n",
      "Finish 19 epoch, Loss: 0.265491\n",
      "VALIDATION Loss: 0.278863\n",
      "\n",
      "Epoch 20/25\n",
      "*****:\n",
      "[20/25] Loss: 0.251824\n",
      "[20/25] Loss: 0.260461\n",
      "Finish 20 epoch, Loss: 0.260193\n",
      "VALIDATION Loss: 0.272947\n",
      "\n",
      "Epoch 21/25\n",
      "*****:\n",
      "[21/25] Loss: 0.247898\n",
      "[21/25] Loss: 0.255052\n",
      "Finish 21 epoch, Loss: 0.254542\n",
      "VALIDATION Loss: 0.270238\n",
      "\n",
      "Epoch 22/25\n",
      "*****:\n",
      "[22/25] Loss: 0.251457\n",
      "[22/25] Loss: 0.254821\n",
      "Finish 22 epoch, Loss: 0.255009\n",
      "VALIDATION Loss: 0.269290\n",
      "\n",
      "Epoch 23/25\n",
      "*****:\n",
      "[23/25] Loss: 0.249577\n",
      "[23/25] Loss: 0.250373\n",
      "Finish 23 epoch, Loss: 0.250691\n",
      "VALIDATION Loss: 0.274207\n",
      "\n",
      "Epoch 24/25\n",
      "*****:\n",
      "[24/25] Loss: 0.246873\n",
      "[24/25] Loss: 0.245294\n",
      "Finish 24 epoch, Loss: 0.245537\n",
      "VALIDATION Loss: 0.280187\n",
      "\n",
      "Epoch 25/25\n",
      "*****:\n",
      "[25/25] Loss: 0.239074\n",
      "[25/25] Loss: 0.238251\n",
      "Finish 25 epoch, Loss: 0.238730\n",
      "VALIDATION Loss: 0.286150\n",
      "\n",
      "(8424, 4)\n",
      "sample_submission.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "# Shlomo Kashani \n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "# from subprocess import check_output\n",
    "# print(check_output([\"ls\", \"../_RawData\"]).decode(\"utf8\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from sklearn import cross_validation\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score, log_loss, roc_auc_score, roc_curve, auc\n",
    "from sklearn.cross_validation import StratifiedKFold, ShuffleSplit, cross_val_score, train_test_split\n",
    "\n",
    "print('__Python VERSION:', sys.version)\n",
    "print('__pyTorch VERSION:', torch.__version__)\n",
    "\n",
    "import numpy\n",
    "import numpy as np\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "Tensor = FloatTensor\n",
    "\n",
    "import pandas\n",
    "import pandas as pd\n",
    "\n",
    "import logging\n",
    "handler=logging.basicConfig(level=logging.INFO)\n",
    "lgr = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# !pip install psutil\n",
    "import psutil\n",
    "import os\n",
    "def cpuStats():\n",
    "        print(sys.version)\n",
    "        print(psutil.cpu_percent())\n",
    "        print(psutil.virtual_memory())  # physical memory usage\n",
    "        pid = os.getpid()\n",
    "        py = psutil.Process(pid)\n",
    "        memoryUse = py.memory_info()[0] / 2. ** 30  # memory use in GB...I think\n",
    "        print('memory GB:', memoryUse)\n",
    "\n",
    "cpuStats()\n",
    "\n",
    "# use_cuda=False\n",
    "lgr.info(\"USE CUDA=\" + str (use_cuda))\n",
    "\n",
    "\n",
    "# #  Global params\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# fix seed\n",
    "seed=17*19\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "\n",
    "# #  View the Data\n",
    "# - Numerai provides a data set that is allready split into train, validation and test sets. \n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "# Data params\n",
    "TARGET_VAR= 'target'\n",
    "BASE_FOLDER = '../_RawData/'\n",
    "\n",
    "\n",
    "# #  Train / Validation / Test Split\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# data = pd.read_json(BASE_FOLDER + '/train.json')\n",
    "data = pd.read_json(\"../_RawData/train.json/data/processed/train.json\")\n",
    "\n",
    "print (data.shape)\n",
    "# data['precision_4'] = data['inc_angle'].apply(lambda x: len(str(x))) <= 7\n",
    "# data = data[data['precision_4'] == True]\n",
    "# print (data.shape)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import scipy.signal\n",
    "\n",
    "def cross_image(im1, im2):\n",
    "   # get rid of the color channels by performing a grayscale transform\n",
    "   # the type cast into 'float' is to avoid overflows\n",
    "   im1_gray = np.sum(im1.astype('float'), axis=2)\n",
    "   im2_gray = np.sum(im2.astype('float'), axis=2)\n",
    "\n",
    "   # get rid of the averages, otherwise the results are not good\n",
    "   im1_gray -= np.mean(im1_gray)\n",
    "   im2_gray -= np.mean(im2_gray)\n",
    "\n",
    "   # calculate the correlation image; note the flipping of onw of the images\n",
    "   return scipy.signal.fftconvolve(im1_gray, im2_gray[::-1,::-1], mode='same')\n",
    "   \n",
    "# Suffle\n",
    "import random\n",
    "from datetime import datetime\n",
    "from scipy import signal\n",
    "random.seed(datetime.now())\n",
    "# np.random.seed(datetime.now())\n",
    "from sklearn.utils import shuffle\n",
    "data = shuffle(data) # otherwise same validation set each time!\n",
    "data= data.reindex(np.random.permutation(data.index))\n",
    "\n",
    "data = shuffle(data) # otherwise same validation set each time!\n",
    "data= data.reindex(np.random.permutation(data.index))\n",
    "\n",
    "def Zpad(A, length):\n",
    "    arr = np.zeros(length)\n",
    "    arr[:len(A)] = A\n",
    "    return arr\n",
    "\n",
    "# data['band_1'] = data['band_1'].apply(lambda x: Zpad(x,6400))\n",
    "# data['band_2'] = data['band_1'].apply(lambda x: Zpad(x,6400))\n",
    "\n",
    "data['band_1'] = data['band_1'].apply(lambda x: np.array(x).reshape(75, 75))\n",
    "data['band_2'] = data['band_2'].apply(lambda x: np.array(x).reshape(75, 75))\n",
    "\n",
    "data['inc_angle'] = pd.to_numeric(data['inc_angle'], errors='coerce')\n",
    "\n",
    "import scipy\n",
    "band_1 = np.concatenate([im for im in data['band_1']]).reshape(-1, 75, 75)\n",
    "band_2 = np.concatenate([im for im in data['band_2']]).reshape(-1, 75, 75)\n",
    "# band_3=(band_1+band_2)/2\n",
    "# band_3=signal.fftconvolve(band_1, band_1, mode = 'same')\n",
    "\n",
    "full_img = np.stack([band_1, band_2], axis=1)\n",
    "\n",
    "# https://github.com/bermanmaxim/jaccardSegment/blob/master/compose.py\n",
    "\n",
    "# #  From Numpy to PyTorch GPU tensors\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "\n",
    "# Convert the np arrays into the correct dimention and type\n",
    "# Note that BCEloss requires Float in X as well as in y\n",
    "def XnumpyToTensor(x_data_np):\n",
    "    x_data_np = np.array(x_data_np, dtype=np.float32)        \n",
    "#     print(x_data_np.shape)\n",
    "    print(type(x_data_np))\n",
    "\n",
    "    if use_cuda:\n",
    "        lgr.info (\"Using the GPU\")    \n",
    "        X_tensor = (torch.from_numpy(x_data_np).cuda()) # Note the conversion for pytorch    \n",
    "    else:\n",
    "        lgr.info (\"Using the CPU\")\n",
    "        X_tensor = (torch.from_numpy(x_data_np)) # Note the conversion for pytorch\n",
    "        \n",
    "#     print((X_tensor.shape)) # torch.Size([108405, 29])\n",
    "    return X_tensor\n",
    "\n",
    "\n",
    "# Convert the np arrays into the correct dimention and type\n",
    "# Note that BCEloss requires Float in X as well as in y\n",
    "def YnumpyToTensor(y_data_np):    \n",
    "    y_data_np=y_data_np.reshape((y_data_np.shape[0],1)) # Must be reshaped for PyTorch!\n",
    "    print(y_data_np.shape)\n",
    "    print(type(y_data_np))\n",
    "\n",
    "    if use_cuda:\n",
    "        lgr.info (\"Using the GPU\")            \n",
    "    #     Y = Variable(torch.from_numpy(y_data_np).type(torch.LongTensor).cuda())\n",
    "        Y_tensor = (torch.from_numpy(y_data_np)).type(torch.FloatTensor).cuda()  # BCEloss requires Float        \n",
    "    else:\n",
    "        lgr.info (\"Using the CPU\")        \n",
    "    #     Y = Variable(torch.squeeze (torch.from_numpy(y_data_np).type(torch.LongTensor)))  #         \n",
    "        Y_tensor = (torch.from_numpy(y_data_np)).type(torch.FloatTensor)  # BCEloss requires Float        \n",
    "\n",
    "    print(type(Y_tensor)) # should be 'torch.cuda.FloatTensor'\n",
    "    print(y_data_np.shape)\n",
    "    print(type(y_data_np))    \n",
    "    return Y_tensor\n",
    "\n",
    "\n",
    "# #  Custom data loader\n",
    "\n",
    "# In[17]:\n",
    "\n",
    "# transformations = transforms.Compose([transforms.Scale(32),transforms.ToTensor()])\n",
    "# preprocess = transforms.Compose([\n",
    "#   transforms.Scale(75),\n",
    "#   transforms.CenterCrop(224),\n",
    "#   transforms.ToTensor(),\n",
    "#   normalize\n",
    "# ])\n",
    "\n",
    "class FullTrainningDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, full_ds, offset, length):\n",
    "        self.full_ds = full_ds\n",
    "        self.offset = offset\n",
    "        self.length = length\n",
    "        assert len(full_ds)>=offset+length, Exception(\"Parent Dataset not long enough\")\n",
    "        super(FullTrainningDataset, self).__init__()\n",
    "        \n",
    "    def __len__(self):        \n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        # label = torch.from_numpy(self.y_train[index])\n",
    "        return self.full_ds[i+self.offset]\n",
    "    \n",
    "validationRatio=0.11    \n",
    "\n",
    "def trainTestSplit(dataset, val_share=validationRatio):\n",
    "    val_offset = int(len(dataset)*(1-val_share))\n",
    "    print (\"Offest:\" + str(val_offset))\n",
    "    return FullTrainningDataset(dataset, 0, val_offset), FullTrainningDataset(dataset, \n",
    "                                                                              val_offset, len(dataset)-val_offset)\n",
    "\n",
    "\n",
    "# In[25]:\n",
    "\n",
    "\n",
    "batch_size=64\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# train_imgs = torch.from_numpy(full_img_tr).float()\n",
    "train_imgs=XnumpyToTensor (full_img)\n",
    "train_targets = YnumpyToTensor(data['is_iceberg'].values)\n",
    "dset_train = TensorDataset(train_imgs, train_targets)\n",
    "\n",
    "\n",
    "train_ds, val_ds = trainTestSplit(dset_train)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "val_loader = torch.utils.data.DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "print (train_loader)\n",
    "print (val_loader)\n",
    "\n",
    "num_epoches = 25\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "# import attr\n",
    "import torch\n",
    "import torch.cuda\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "n_channels = 2  # max 20\n",
    "total_classes = 1\n",
    "    \n",
    "\n",
    "# https://github.com/Lextal/pspnet-pytorch/blob/master/train.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Funct\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "# class SegNet(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(SegNet, self).__init__()\n",
    "\n",
    "#         self.encoder_1 = nn.Sequential(\n",
    "#             nn.Conv2d(2, 64, 7, padding=3),\n",
    "#             nn.BatchNorm2d(64),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d((2, 2), stride=(2, 2), return_indices=True)\n",
    "#         )  # first group\n",
    "\n",
    "#         self.encoder_2 = nn.Sequential(\n",
    "#             nn.Conv2d(64, 64, 7, padding=3),\n",
    "#             nn.BatchNorm2d(64),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d((2, 2), stride=(2, 2), return_indices=True)\n",
    "#         )  # second group\n",
    "\n",
    "#         self.encoder_3 = nn.Sequential(\n",
    "#             nn.Conv2d(64, 64, 7, padding=3),\n",
    "#             nn.BatchNorm2d(64),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d((2, 2), stride=(2, 2), return_indices=True)\n",
    "#         )  # third group\n",
    "\n",
    "#         self.encoder_4 = nn.Sequential(\n",
    "#             nn.Conv2d(64, 64, 7, padding=3),\n",
    "#             nn.BatchNorm2d(64),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d((2, 2), stride=(2, 2), return_indices=True)\n",
    "#         )  # fourth group\n",
    "\n",
    "#         self.unpool_1 = nn.MaxUnpool2d(2, stride=2)  # get masks\n",
    "#         self.unpool_2 = nn.MaxUnpool2d(2, stride=2)\n",
    "#         self.unpool_3 = nn.MaxUnpool2d(2, stride=2)\n",
    "#         self.unpool_4 = nn.MaxUnpool2d(2, stride=2)\n",
    "\n",
    "#         self.decoder_1 = nn.Sequential(\n",
    "#             nn.Conv2d(64, 64, 7, padding=3),\n",
    "#             nn.BatchNorm2d(64)\n",
    "#         )  # first group\n",
    "\n",
    "#         self.decoder_2 = nn.Sequential(\n",
    "#             nn.Conv2d(64, 64, 7, padding=3),\n",
    "#             nn.BatchNorm2d(64)\n",
    "#         )  # second group\n",
    "\n",
    "#         self.decoder_3 = nn.Sequential(\n",
    "#             nn.Conv2d(64, 64, 7, padding=3),\n",
    "#             nn.BatchNorm2d(64)\n",
    "#         )  # third group\n",
    "\n",
    "#         self.decoder_4 = nn.Sequential(\n",
    "#             nn.Conv2d(64, 3, 7, padding=3),\n",
    "#             nn.BatchNorm2d(3)\n",
    "#         )  # fourth group\n",
    "\n",
    "#         # self.conv_classifier = nn.Conv2d(128, 5, 1)\n",
    "        \n",
    "#         self.classifier = torch.nn.Sequential(\n",
    "#             nn.Linear(972, 1),             \n",
    "#         )\n",
    "        \n",
    "#         self.mp = nn.MaxPool2d(4, 4)\n",
    "        \n",
    "#         self.sig = nn.Sigmoid()   \n",
    "\n",
    "#     def forward(self, x):\n",
    "#         size_1 = x.size()\n",
    "#         x, indices_1 = self.encoder_1(x)\n",
    "\n",
    "#         size_2 = x.size()\n",
    "#         x, indices_2 = self.encoder_2(x)\n",
    "\n",
    "#         size_3 = x.size()\n",
    "#         x, indices_3 = self.encoder_3(x)\n",
    "\n",
    "#         size_4 = x.size()\n",
    "#         x, indices_4 = self.encoder_4(x)\n",
    "\n",
    "#         x = self.unpool_1(x, indices_4, output_size=size_4)\n",
    "#         x = self.decoder_1(x)\n",
    "\n",
    "#         x = self.unpool_2(x, indices_3, output_size=size_3)\n",
    "#         x = self.decoder_2(x)\n",
    "\n",
    "#         x = self.unpool_3(x, indices_2, output_size=size_2)\n",
    "#         x = self.decoder_3(x)\n",
    "\n",
    "#         x = self.unpool_4(x, indices_1, output_size=size_1)\n",
    "#         x = self.decoder_4(x)\n",
    "        \n",
    "#         x = self.mp(x)\n",
    "#         x = x.view(x.size(0), -1)    \n",
    "#         print(\"shape:\" + str(x.data.shape))\n",
    "#         x = self.classifier(x)\n",
    "#         # print(\"shape:\" + str(x.data.shape))\n",
    "\n",
    "#         x = self.sig(x)\n",
    "\n",
    "#         return x\n",
    "\n",
    "\n",
    "# model=SegNet()\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision.models as models\n",
    "\n",
    "import sys\n",
    "import math\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, nChannels, growthRate):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        interChannels = 4*growthRate\n",
    "        self.bn1 = nn.BatchNorm2d(nChannels)\n",
    "        self.conv1 = nn.Conv2d(nChannels, interChannels, kernel_size=1,\n",
    "                               bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(interChannels)\n",
    "        self.conv2 = nn.Conv2d(interChannels, growthRate, kernel_size=3,\n",
    "                               padding=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(F.relu(self.bn1(x)))\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        out = torch.cat((x, out), 1)\n",
    "        return out\n",
    "\n",
    "class SingleLayer(nn.Module):\n",
    "    def __init__(self, nChannels, growthRate):\n",
    "        super(SingleLayer, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(nChannels)\n",
    "        self.conv1 = nn.Conv2d(nChannels, growthRate, kernel_size=3,\n",
    "                               padding=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(F.relu(self.bn1(x)))\n",
    "        out = torch.cat((x, out), 1)\n",
    "        return out\n",
    "\n",
    "class Transition(nn.Module):\n",
    "    def __init__(self, nChannels, nOutChannels):\n",
    "        super(Transition, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(nChannels)\n",
    "        self.conv1 = nn.Conv2d(nChannels, nOutChannels, kernel_size=1,\n",
    "                               bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(F.relu(self.bn1(x)))\n",
    "        out = F.avg_pool2d(out, 2)\n",
    "        return out\n",
    "\n",
    "\n",
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, growthRate, depth, reduction, nClasses, bottleneck):\n",
    "        super(DenseNet, self).__init__()\n",
    "\n",
    "        nDenseBlocks = (depth-4) // 3\n",
    "        if bottleneck:\n",
    "            nDenseBlocks //= 2\n",
    "\n",
    "        nChannels = 2*growthRate\n",
    "        self.conv1 = nn.Conv2d(2, nChannels, kernel_size=3, padding=1,\n",
    "                               bias=False)\n",
    "        self.dense1 = self._make_dense(nChannels, growthRate, nDenseBlocks, bottleneck)\n",
    "        nChannels += nDenseBlocks*growthRate\n",
    "        nOutChannels = int(math.floor(nChannels*reduction))\n",
    "        self.trans1 = Transition(nChannels, nOutChannels)\n",
    "\n",
    "        nChannels = nOutChannels\n",
    "        self.dense2 = self._make_dense(nChannels, growthRate, nDenseBlocks, bottleneck)\n",
    "        nChannels += nDenseBlocks*growthRate\n",
    "        nOutChannels = int(math.floor(nChannels*reduction))\n",
    "        self.trans2 = Transition(nChannels, nOutChannels)\n",
    "\n",
    "        nChannels = nOutChannels\n",
    "        self.dense3 = self._make_dense(nChannels, growthRate, nDenseBlocks, bottleneck)\n",
    "        nChannels += nDenseBlocks*growthRate\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(nChannels)\n",
    "        self.fc = nn.Linear(128, nClasses)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_dense(self, nChannels, growthRate, nDenseBlocks, bottleneck):\n",
    "        layers = []\n",
    "        for i in range(int(nDenseBlocks)):\n",
    "            if bottleneck:\n",
    "                layers.append(Bottleneck(nChannels, growthRate))\n",
    "            else:\n",
    "                layers.append(SingleLayer(nChannels, growthRate))\n",
    "            nChannels += growthRate\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.trans1(self.dense1(out))\n",
    "        out = self.trans2(self.dense2(out))\n",
    "        out = self.dense3(out)\n",
    "        # print(out.data.shape)\n",
    "        out = F.avg_pool2d(F.relu(self.bn1(out)), 8)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        # print(out.data.shape)\n",
    "        out = F.sigmoid(self.fc(out))\n",
    "        return out\n",
    "\n",
    "model = DenseNet(growthRate=8, depth=20, reduction=0.5,\n",
    "                            bottleneck=True, nClasses=1)\n",
    "\n",
    "print('  + Number of params: {}'.format(sum([p.data.nelement() for p in model.parameters()])))\n",
    "        \n",
    "print(model)\n",
    "# https://github.com/ZijunDeng/pytorch-semantic-segmentation/tree/master/models\n",
    "# https://github.com/andreasveit/densenet-pytorch/blob/master/densenet.py\n",
    "# https://github.com/meliketoy/wide-resnet.pytorch/blob/master/networks/wide_resnet.py\n",
    "\n",
    "# # Loss and optimizer\n",
    "\n",
    "# In[28]:\n",
    "\n",
    "'''DenseNet in PyTorch.'''\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, in_planes, growth_rate):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv1 = nn.Conv2d(in_planes, 4 * growth_rate, kernel_size=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(4 * growth_rate)\n",
    "        self.conv2 = nn.Conv2d(4 * growth_rate, growth_rate, kernel_size=3, padding=1, bias=False)\n",
    "\n",
    "        self.mp = torch.nn.MaxPool2d(1, 1)\n",
    "        # self.avgpool = torch.nn.AvgPool2d(2,2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(F.relu(self.bn1(x)))\n",
    "\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        # out = self.mp(out)\n",
    "        # out = self.avgpool(out)\n",
    "\n",
    "        # print (x.data.shape)\n",
    "\n",
    "        out = torch.cat([out, x], 1)\n",
    "        out = self.mp(out)\n",
    "        # out = self.avgpool(out)\n",
    "        # print(out.data.shape)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Transition(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes):\n",
    "        super(Transition, self).__init__()\n",
    "        self.bn = nn.BatchNorm2d(in_planes)\n",
    "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(F.relu(self.bn(x)))\n",
    "        out = F.avg_pool2d(out, 2)\n",
    "        return out\n",
    "\n",
    "\n",
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, block, nblocks, growth_rate=12, reduction=0.5, num_classes=1):\n",
    "        super(DenseNet, self).__init__()\n",
    "        self.growth_rate = growth_rate\n",
    "\n",
    "        num_planes = 2 * growth_rate\n",
    "        self.conv1 = nn.Conv2d(3, num_planes, kernel_size=3, padding=1, bias=False)\n",
    "\n",
    "        self.dense1 = self._make_dense_layers(block, num_planes, nblocks[0])\n",
    "        num_planes += nblocks[0] * growth_rate\n",
    "        out_planes = int(math.floor(num_planes * reduction))\n",
    "        self.trans1 = Transition(num_planes, out_planes)\n",
    "        num_planes = out_planes\n",
    "\n",
    "        self.dense2 = self._make_dense_layers(block, num_planes, nblocks[1])\n",
    "        num_planes += nblocks[1] * growth_rate\n",
    "        out_planes = int(math.floor(num_planes * reduction))\n",
    "        self.trans2 = Transition(num_planes, out_planes)\n",
    "        num_planes = out_planes\n",
    "\n",
    "        self.dense3 = self._make_dense_layers(block, num_planes, nblocks[2])\n",
    "        num_planes += nblocks[2] * growth_rate\n",
    "        out_planes = int(math.floor(num_planes * reduction))\n",
    "        self.trans3 = Transition(num_planes, out_planes)\n",
    "        num_planes = out_planes\n",
    "\n",
    "        self.dense4 = self._make_dense_layers(block, num_planes, nblocks[3])\n",
    "        num_planes += nblocks[3] * growth_rate\n",
    "\n",
    "        self.bn = nn.BatchNorm2d(num_planes)\n",
    "\n",
    "        self.linear = nn.Linear(3328, num_classes)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def _make_dense_layers(self, block, in_planes, nblock):\n",
    "        layers = []\n",
    "        for i in range(nblock):\n",
    "            layers.append(block(in_planes, self.growth_rate))\n",
    "            in_planes += self.growth_rate\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.trans1(self.dense1(out))\n",
    "        out = self.trans2(self.dense2(out))\n",
    "        out = self.trans3(self.dense3(out))\n",
    "        out = self.dense4(out)\n",
    "        out = F.avg_pool2d(F.relu(self.bn(out)), 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        # print (out.data.shape)\n",
    "        out = self.linear(out)\n",
    "        out = self.sig(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "def DenseNet121():\n",
    "    return DenseNet(Bottleneck, [6, 12, 24, 16], growth_rate=12)\n",
    "\n",
    "\n",
    "def DenseNet169():\n",
    "    return DenseNet(Bottleneck, [6, 12, 32, 32], growth_rate=16)\n",
    "\n",
    "\n",
    "def DenseNet201():\n",
    "    return DenseNet(Bottleneck, [6, 12, 48, 32], growth_rate=32)\n",
    "\n",
    "\n",
    "def DenseNet161():\n",
    "    return DenseNet(Bottleneck, [6, 12, 36, 24], growth_rate=48)\n",
    "\n",
    "\n",
    "def densenet_cifar():\n",
    "    return DenseNet(Bottleneck, [6, 12, 24, 16], growth_rate=12)\n",
    "\n",
    "# test_densenet()\n",
    "\n",
    "\n",
    "\n",
    "loss_func=torch.nn.BCELoss() # Binary cross entropy: http://pytorch.org/docs/nn.html#bceloss\n",
    "\n",
    "# NN params\n",
    "LR = 0.0005\n",
    "MOMENTUM= 0.95\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR,weight_decay=5e-5) #  L2 regularization\n",
    "if use_cuda:\n",
    "    lgr.info (\"Using the GPU\")    \n",
    "    model.cuda()\n",
    "    loss_func.cuda()\n",
    "\n",
    "lgr.info (optimizer)\n",
    "lgr.info (loss_func)\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "criterion = loss_func\n",
    "all_losses = []\n",
    "val_losses = []\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    for epoch in range(num_epoches):\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epoches))\n",
    "        print('*' * 5 + ':')\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        for i, data in enumerate(train_loader, 1):\n",
    "    \n",
    "            img, label = data\n",
    "            if use_cuda:\n",
    "                img, label = Variable(img.cuda(async=True)), Variable(label.cuda(async=True))  # On GPU\n",
    "            else:\n",
    "                img, label = Variable(img), Variable(\n",
    "                    label)  # RuntimeError: expected CPU tensor (got CUDA tensor)\n",
    "    \n",
    "            out = model(img)\n",
    "            loss = criterion(out, label)\n",
    "            running_loss += loss.data[0] * label.size(0)\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            if i % 10 == 0:\n",
    "                all_losses.append(running_loss / (batch_size * i))\n",
    "                print('[{}/{}] Loss: {:.6f}'.format(\n",
    "                    epoch + 1, num_epoches, running_loss / (batch_size * i),\n",
    "                    running_acc / (batch_size * i)))\n",
    "    \n",
    "        print('Finish {} epoch, Loss: {:.6f}'.format(epoch + 1, running_loss / (len(train_ds))))\n",
    "    \n",
    "        model.eval()\n",
    "        eval_loss = 0\n",
    "        eval_acc = 0\n",
    "        for data in val_loader:\n",
    "            img, label = data\n",
    "    \n",
    "            if use_cuda:\n",
    "                img = Variable(img.cuda(async=True), volatile=True)\n",
    "                label = Variable(label.cuda(async=True), volatile=True)  # On GPU\n",
    "            else:\n",
    "                img = Variable(img, volatile=True)\n",
    "                label = Variable(label, volatile=True)\n",
    "    \n",
    "            out = model(img)\n",
    "            loss = criterion(out, label)\n",
    "            eval_loss += loss.data[0] * label.size(0)\n",
    "    \n",
    "        print('VALIDATION Loss: {:.6f}'.format(eval_loss / (len(val_ds))))\n",
    "        val_losses.append(eval_loss / (len(val_ds)))\n",
    "        print()\n",
    "    \n",
    "    torch.save(model.state_dict(), './cnn.pth')\n",
    "    \n",
    "    \n",
    "    df_test_set = pd.read_json('../_RawData/test.json/data/processed/test.json')\n",
    "    \n",
    "    df_test_set['band_1'] = df_test_set['band_1'].apply(lambda x: np.array(x).reshape(75, 75))\n",
    "    df_test_set['band_2'] = df_test_set['band_2'].apply(lambda x: np.array(x).reshape(75, 75))\n",
    "    df_test_set['inc_angle'] = pd.to_numeric(df_test_set['inc_angle'], errors='coerce')\n",
    "    \n",
    "    df_test_set.head(3)\n",
    "    \n",
    "    \n",
    "    print (df_test_set.shape)\n",
    "    columns = ['id', 'is_iceberg']\n",
    "    df_pred=pd.DataFrame(data=np.zeros((0,len(columns))), columns=columns)\n",
    "    # df_pred.id.astype(int)\n",
    "    \n",
    "    for index, row in df_test_set.iterrows():\n",
    "        rwo_no_id=row.drop('id')    \n",
    "        band_1_test = (rwo_no_id['band_1']).reshape(-1, 75, 75)\n",
    "        band_2_test = (rwo_no_id['band_2']).reshape(-1, 75, 75)\n",
    "        full_img_test = np.stack([band_1_test, band_2_test], axis=1)\n",
    "    \n",
    "        x_data_np = np.array(full_img_test, dtype=np.float32)        \n",
    "        if use_cuda:\n",
    "            X_tensor_test = Variable(torch.from_numpy(x_data_np).cuda()) # Note the conversion for pytorch    \n",
    "        else:\n",
    "            X_tensor_test = Variable(torch.from_numpy(x_data_np)) # Note the conversion for pytorch\n",
    "                        \n",
    "    #     X_tensor_test=X_tensor_test.view(1, trainX.shape[1]) # does not work with 1d tensors            \n",
    "        predicted_val = (model(X_tensor_test).data).float() # probabilities     \n",
    "        p_test =   predicted_val.cpu().numpy().item() # otherwise we get an array, we need a single float\n",
    "        \n",
    "        df_pred = df_pred.append({'id':row['id'], 'is_iceberg':p_test},ignore_index=True)\n",
    "    #     df_pred = df_pred.append({'id':row['id'].astype(int), 'probability':p_test},ignore_index=True)\n",
    "    \n",
    "    df_pred.head(5)\n",
    "    \n",
    "    \n",
    "    def savePred(df_pred):\n",
    "    #     csv_path = 'pred/p_{}_{}_{}.csv'.format(loss, name, (str(time.time())))\n",
    "    #     csv_path = 'pred_{}_{}.csv'.format(loss, (str(time.time())))\n",
    "        csv_path='sample_submission.csv'\n",
    "        df_pred.to_csv(csv_path, columns=('id', 'is_iceberg'), index=None)\n",
    "        print (csv_path)\n",
    "        \n",
    "    savePred (df_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
